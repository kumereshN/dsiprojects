{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs and Classification: Model Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=3 color='blue'> You don't have to import some of these libraries for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the graph style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>driven individual rushing towards dream ever s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reduce bounce rate webpage</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>made animated summary lean start eric ries hop...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skate ramp business</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help getting textile prototype created</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>trying learn various ing strategy came across ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>pretend know lot finance economics sold positi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>bill ackman bet market recovery despite covid ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>news covid vaccine drugmaker pfizer pfe partne...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>apparently buy people distressed mortgage note...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2166 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     driven individual rushing towards dream ever s...    1.0\n",
       "1                            reduce bounce rate webpage    1.0\n",
       "2     made animated summary lean start eric ries hop...    1.0\n",
       "3                                   skate ramp business    1.0\n",
       "4                help getting textile prototype created    1.0\n",
       "...                                                 ...    ...\n",
       "2161  trying learn various ing strategy came across ...    0.0\n",
       "2162  pretend know lot finance economics sold positi...    0.0\n",
       "2163  bill ackman bet market recovery despite covid ...    0.0\n",
       "2164  news covid vaccine drugmaker pfizer pfe partne...    0.0\n",
       "2165  apparently buy people distressed mortgage note...    0.0\n",
       "\n",
       "[2166 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_csv('./datasets/final_df.csv')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the model into their train and test set before transforming the text using the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df['text']\n",
    "y = final_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Reset the indexes\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1624,)\n",
      "(542,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the text using `countvectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the \"CountVectorizer\" object\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             lowercase= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the words to tokenize the words  \n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_test_vec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1624 rows Ã— 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aapl  aar  aaron  aaxn  aaz  ab  abandon  abbv  ...  zm  \\\n",
       "0      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "2      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "3      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "4      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "...   ..  ...   ...  ...    ...   ...  ...  ..      ...   ...  ...  ..   \n",
       "1619   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1620   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1621   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1622   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1623   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "\n",
       "      zoetis  zone  zoo  zookeeper  zoom  zts  zuck  zuckerberg  zweig  \n",
       "0          0     0    0          0     0    0     0           0      0  \n",
       "1          0     0    0          0     0    0     0           0      0  \n",
       "2          0     0    0          0     0    0     0           0      0  \n",
       "3          0     0    0          0     2    0     0           0      0  \n",
       "4          0     0    0          0     0    0     0           0      0  \n",
       "...      ...   ...  ...        ...   ...  ...   ...         ...    ...  \n",
       "1619       0     0    0          0     0    0     0           0      0  \n",
       "1620       0     0    0          0     0    0     0           0      0  \n",
       "1621       0     0    0          0     0    0     0           0      0  \n",
       "1622       0     0    0          0     0    0     0           0      0  \n",
       "1623       0     0    0          0     0    0     0           0      0  \n",
       "\n",
       "[1624 rows x 10551 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_vec.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows Ã— 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aapl  aar  aaron  aaxn  aaz  ab  abandon  abbv  ...  zm  zoetis  \\\n",
       "0     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "1     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "2     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "3     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "4     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "..   ..  ...   ...  ...    ...   ...  ...  ..      ...   ...  ...  ..     ...   \n",
       "537   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "538   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "539   0    0     0    0      0     0    0   0        0     1  ...   0       0   \n",
       "540   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "541   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "\n",
       "     zone  zoo  zookeeper  zoom  zts  zuck  zuckerberg  zweig  \n",
       "0       0    0          0     0    0     0           0      0  \n",
       "1       0    0          0     0    0     0           0      0  \n",
       "2       0    0          0     0    0     0           0      0  \n",
       "3       0    0          0     0    0     0           0      0  \n",
       "4       0    0          0     0    0     0           0      0  \n",
       "..    ...  ...        ...   ...  ...   ...         ...    ...  \n",
       "537     0    0          0     0    0     0           0      0  \n",
       "538     0    0          0     0    0     0           0      0  \n",
       "539     0    0          0     0    0     0           0      0  \n",
       "540     0    0          0     0    0     0           0      0  \n",
       "541     0    0          0     0    0     0           0      0  \n",
       "\n",
       "[542 rows x 10551 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert X_test into a DataFrame.\n",
    "X_test_df = pd.DataFrame(X_test_vec.toarray(),\n",
    "                         columns=cvec.get_feature_names())\n",
    "\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.538793\n",
       "0.0    0.461207\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy is 0.53 as the majority class fall under the entrepreneur subreddit which is class 1.\n",
    "\n",
    "The baseline accuracy is used to check if the created model performs better than the model without features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the logistics regression CV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For solver newton-cg, the test score is 0.9059040590405905\n",
      "For solver lbfgs, the test score is 0.9059040590405905\n",
      "For solver liblinear, the test score is 0.9040590405904059\n",
      "For solver sag, the test score is 0.9114391143911439\n",
      "For solver saga, the test score is 0.9132841328413284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Find the best solver\n",
    "solver_lst = ['newton-cg','lbfgs','liblinear','sag', 'saga']\n",
    "\n",
    "for s in solver_lst:\n",
    "    lrcv = LogisticRegressionCV(Cs=[0.00001,0.001, 0.01, 0.1,1], cv=5, n_jobs=-1, solver=s, max_iter=7000)\n",
    "    lrcv.fit(X_train_vec, y_train)\n",
    "    test_score = lrcv.score(X_test_vec, y_test)\n",
    "    print(f'For solver {s}, the test score is {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best solver wold be `saga` as it has the highest accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[1e-05, 0.001, 0.01, 0.1, 1], cv=5, max_iter=5000,\n",
       "                     n_jobs=-1, solver='saga')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV(Cs=[0.00001,0.001, 0.01, 0.1,1], cv=5, n_jobs=-1, solver='saga', max_iter=5000)\n",
    "lrcv.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Alpha value from Cross Validation\n",
    "lrcv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha value is the regularization parameter which is used to reduce a model's overfitting. The lower the value, the more regularization is used on the model.\n",
    "The Logistic Regression CV found 0.1 to be the best alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673645320197044"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for the training set\n",
    "lrcv.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132841328413284"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for the test set\n",
    "lrcv.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the model is overfitting comparing against the train and test set as the train set has a higher accuracy score compared to the test test.\n",
    "\n",
    "I can reduce the number of features in the model to reduce the variance which will decrease the overfitting and help improve the accuracy score.\n",
    "\n",
    "I can also increase regularization strength of the model to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 231\n",
      "False Positive: 19\n",
      "True Positive: 264\n",
      "False Negative: 28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lrcv.predict(X_test_vec)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[231,  19],\n",
       "       [ 28, 264]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specficity: 0.92\n",
      "Sensitivity: 0.9\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn+fp) # How accurately can the model predict for the negative class\n",
    "sensitivity = tp / (tp+fn) # How accurately can the model predict for the positive class\n",
    "accruacy = (tp+tn) / (tp+tn+fp+fn)\n",
    "\n",
    "print('Specficity:', round(specificity,2))\n",
    "print('Sensitivity:',round(sensitivity,2))\n",
    "print('Accuracy:',round(accruacy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sensitivity is slightly lower compared to the Specificity, which means the model is slightly more likely to accurately predict the negative class compared to the positive class. However, since I'm trying to predict whether the model is able to accurately predict the subreddit where the post belongs to, optimizing for Sensitivity and Specificity would not be a good measure. Accuracy would be a better performance metric.\n",
    "\n",
    "A total of 47 posts were misclassified which will be later on investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients for the Logisitics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>3.105827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idea</th>\n",
       "      <td>2.274191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>1.670755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>1.629490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>1.468443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1.467806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>website</th>\n",
       "      <td>1.443104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>1.420869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>1.378951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>1.359499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficients\n",
       "business       3.105827\n",
       "idea           2.274191\n",
       "product        1.670755\n",
       "startup        1.629490\n",
       "marketing      1.468443\n",
       "start          1.467806\n",
       "website        1.443104\n",
       "work           1.420869\n",
       "help           1.378951\n",
       "want           1.359499"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top coefficients for the positive class, the entrepreneur subreddit\n",
    "lrcv_coef = pd.DataFrame(np.exp(lrcv.coef_[0]),\n",
    "                          index=cvec.get_feature_names(),\n",
    "                          columns=['Coefficients']).sort_values('Coefficients',ascending=False)\n",
    "lrcv_coef.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the logistic regression coefficients represent the log odds that an observation is in target class, 1, given the values of it X variables, the log odd coefficients need to be converted to regular odds to make sense of them. This is done through exponentiating the log odds coefficients.\n",
    "\n",
    "For example:\n",
    "\n",
    "For every one-unit in `business`, the odds that the observation is in entrepreneur class is 3.1 times as large as the odds that the observation is not in the entrepreneur class provided all other variables are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.708195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>0.700323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portfolio</th>\n",
       "      <td>0.696829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fund</th>\n",
       "      <td>0.693976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palantir</th>\n",
       "      <td>0.693638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>0.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ment</th>\n",
       "      <td>0.637986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etf</th>\n",
       "      <td>0.617748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ing</th>\n",
       "      <td>0.423148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>0.272978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficients\n",
       "long           0.708195\n",
       "market         0.700323\n",
       "portfolio      0.696829\n",
       "fund           0.693976\n",
       "palantir       0.693638\n",
       "buy            0.639700\n",
       "ment           0.637986\n",
       "etf            0.617748\n",
       "ing            0.423148\n",
       "stock          0.272978"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top coefficients for the negative class, for the investing subreddit\n",
    "lrcv_coef.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coefficients are least likely to represent the entrepreneur subreddit class and more likely to represent the investing subreddit class given their low coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression: Identifying posts that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 12,  24,  38,  47,  90, 103, 145, 146, 152, 178, 185, 208, 211,\n",
       "            223, 232, 246, 290, 316, 385, 387, 420, 432, 437, 499, 500, 503,\n",
       "            522],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of Misclassified posts for the test set\n",
    "index_misclassified_post = y_test[y_pred != y_test].index\n",
    "index_misclassified_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>affordable preferably online magazine subscrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>weed business profitable worth california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>hi guy stock geared solely towards heavily gho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>started ing march adding k month account gradu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>researched much understand mortgage company of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>stm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>new targeted matching service grow network des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>hello year hard know live chile unemployed sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>st time importer home gym china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>freight company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>example mortgage cost landlord price rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>question open account interactive broker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>title say worth going effort condensing book n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>problem validation cash flow management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>group marijuana likely become accepted legaliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>freight transportation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>writing naked put option question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "12                digital advertisement market reach b\n",
       "24            competitor popping like crazy discussion\n",
       "38   affordable preferably online magazine subscrip...\n",
       "47           weed business profitable worth california\n",
       "90        want learn valuing business best place start\n",
       "103  suppose based vaccine anticipation news wonder...\n",
       "145  hi guy stock geared solely towards heavily gho...\n",
       "146  started ing march adding k month account gradu...\n",
       "152  researched much understand mortgage company of...\n",
       "178                                                stm\n",
       "185  new targeted matching service grow network des...\n",
       "208  hello year hard know live chile unemployed sin...\n",
       "211                    st time importer home gym china\n",
       "223  blog post based conversation reza sabahi reza ...\n",
       "232                                    freight company\n",
       "246  came across thought might interesting info pub...\n",
       "290          example mortgage cost landlord price rent\n",
       "316           question open account interactive broker\n",
       "385  wondering hear mainly public consultation ofte...\n",
       "387  title say worth going effort condensing book n...\n",
       "420            problem validation cash flow management\n",
       "432  group marijuana likely become accepted legaliz...\n",
       "437  penumbra tank hard xtra flex catheter meant he...\n",
       "499  hi like find community avenue helped learning ...\n",
       "500              finish reading book make note key bit\n",
       "503                     freight transportation company\n",
       "522                  writing naked put option question"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe for the misclassified posts\n",
    "log_reg_misclass_posts = pd.DataFrame(X_test[index_misclassified_post])\n",
    "log_reg_misclass_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      0.0\n",
       "2      1.0\n",
       "3      0.0\n",
       "4      1.0\n",
       "      ... \n",
       "537    0.0\n",
       "538    1.0\n",
       "539    0.0\n",
       "540    0.0\n",
       "541    1.0\n",
       "Length: 542, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the index values for y_pred\n",
    "y_pred_series = pd.Series(y_pred, index=X_test.index)\n",
    "y_pred_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the predicted and actual values for the labels\n",
    "log_reg_misclass_posts['y_true'] =  y_test[index_misclassified_post]\n",
    "log_reg_misclass_posts['y_pred'] = y_pred_series[index_misclassified_post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>affordable preferably online magazine subscrip...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>weed business profitable worth california</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>hi guy stock geared solely towards heavily gho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>started ing march adding k month account gradu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>researched much understand mortgage company of...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>stm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>new targeted matching service grow network des...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>hello year hard know live chile unemployed sin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>st time importer home gym china</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>freight company</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>example mortgage cost landlord price rent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>question open account interactive broker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>title say worth going effort condensing book n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>problem validation cash flow management</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>group marijuana likely become accepted legaliz...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>freight transportation company</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>writing naked put option question</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  y_true  y_pred\n",
       "12                digital advertisement market reach b     0.0     1.0\n",
       "24            competitor popping like crazy discussion     1.0     0.0\n",
       "38   affordable preferably online magazine subscrip...     1.0     0.0\n",
       "47           weed business profitable worth california     1.0     0.0\n",
       "90        want learn valuing business best place start     0.0     1.0\n",
       "103  suppose based vaccine anticipation news wonder...     0.0     1.0\n",
       "145  hi guy stock geared solely towards heavily gho...     0.0     1.0\n",
       "146  started ing march adding k month account gradu...     0.0     1.0\n",
       "152  researched much understand mortgage company of...     0.0     1.0\n",
       "178                                                stm     0.0     1.0\n",
       "185  new targeted matching service grow network des...     1.0     0.0\n",
       "208  hello year hard know live chile unemployed sin...     0.0     1.0\n",
       "211                    st time importer home gym china     1.0     0.0\n",
       "223  blog post based conversation reza sabahi reza ...     1.0     0.0\n",
       "232                                    freight company     1.0     0.0\n",
       "246  came across thought might interesting info pub...     0.0     1.0\n",
       "290          example mortgage cost landlord price rent     1.0     0.0\n",
       "316           question open account interactive broker     0.0     1.0\n",
       "385  wondering hear mainly public consultation ofte...     0.0     1.0\n",
       "387  title say worth going effort condensing book n...     0.0     1.0\n",
       "420            problem validation cash flow management     1.0     0.0\n",
       "432  group marijuana likely become accepted legaliz...     0.0     1.0\n",
       "437  penumbra tank hard xtra flex catheter meant he...     0.0     1.0\n",
       "499  hi like find community avenue helped learning ...     0.0     1.0\n",
       "500              finish reading book make note key bit     0.0     1.0\n",
       "503                     freight transportation company     1.0     0.0\n",
       "522                  writing naked put option question     0.0     1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Misclassified posts and their predicted and actual classification\n",
    "log_reg_misclass_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the misclassified posts and the probability of the posts being classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invest prob</th>\n",
       "      <th>Entrepreneur prob</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.546764</td>\n",
       "      <td>0.453236</td>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497689</td>\n",
       "      <td>0.502311</td>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.452619</td>\n",
       "      <td>0.547381</td>\n",
       "      <td>affordable preferably online magazine subscrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.251686</td>\n",
       "      <td>0.748314</td>\n",
       "      <td>weed business profitable worth california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086048</td>\n",
       "      <td>0.913952</td>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.377760</td>\n",
       "      <td>0.622240</td>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.875781</td>\n",
       "      <td>0.124219</td>\n",
       "      <td>hi guy stock geared solely towards heavily gho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.884543</td>\n",
       "      <td>0.115457</td>\n",
       "      <td>started ing march adding k month account gradu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.790496</td>\n",
       "      <td>0.209504</td>\n",
       "      <td>researched much understand mortgage company of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.490412</td>\n",
       "      <td>0.509588</td>\n",
       "      <td>stm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.463506</td>\n",
       "      <td>0.536494</td>\n",
       "      <td>new targeted matching service grow network des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.524301</td>\n",
       "      <td>0.475699</td>\n",
       "      <td>hello year hard know live chile unemployed sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.550736</td>\n",
       "      <td>0.449264</td>\n",
       "      <td>st time importer home gym china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.062611</td>\n",
       "      <td>0.937389</td>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.576233</td>\n",
       "      <td>0.423767</td>\n",
       "      <td>freight company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.351122</td>\n",
       "      <td>0.648878</td>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.545319</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>example mortgage cost landlord price rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.502760</td>\n",
       "      <td>0.497240</td>\n",
       "      <td>question open account interactive broker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.216203</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.729794</td>\n",
       "      <td>0.270206</td>\n",
       "      <td>title say worth going effort condensing book n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.461524</td>\n",
       "      <td>0.538476</td>\n",
       "      <td>problem validation cash flow management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.529385</td>\n",
       "      <td>0.470615</td>\n",
       "      <td>group marijuana likely become accepted legaliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.418686</td>\n",
       "      <td>0.581314</td>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.403165</td>\n",
       "      <td>0.596835</td>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.506901</td>\n",
       "      <td>0.493099</td>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.577364</td>\n",
       "      <td>0.422636</td>\n",
       "      <td>freight transportation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.517230</td>\n",
       "      <td>0.482770</td>\n",
       "      <td>writing naked put option question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Invest prob  Entrepreneur prob  \\\n",
       "0      0.546764           0.453236   \n",
       "1      0.497689           0.502311   \n",
       "2      0.452619           0.547381   \n",
       "3      0.251686           0.748314   \n",
       "4      0.086048           0.913952   \n",
       "5      0.377760           0.622240   \n",
       "6      0.875781           0.124219   \n",
       "7      0.884543           0.115457   \n",
       "8      0.790496           0.209504   \n",
       "9      0.490412           0.509588   \n",
       "10     0.463506           0.536494   \n",
       "11     0.524301           0.475699   \n",
       "12     0.550736           0.449264   \n",
       "13     0.062611           0.937389   \n",
       "14     0.576233           0.423767   \n",
       "15     0.351122           0.648878   \n",
       "16     0.545319           0.454681   \n",
       "17     0.502760           0.497240   \n",
       "18     0.216203           0.783797   \n",
       "19     0.729794           0.270206   \n",
       "20     0.461524           0.538476   \n",
       "21     0.529385           0.470615   \n",
       "22     0.418686           0.581314   \n",
       "23     0.403165           0.596835   \n",
       "24     0.506901           0.493099   \n",
       "25     0.577364           0.422636   \n",
       "26     0.517230           0.482770   \n",
       "\n",
       "                                                 text  \n",
       "0                digital advertisement market reach b  \n",
       "1            competitor popping like crazy discussion  \n",
       "2   affordable preferably online magazine subscrip...  \n",
       "3           weed business profitable worth california  \n",
       "4        want learn valuing business best place start  \n",
       "5   suppose based vaccine anticipation news wonder...  \n",
       "6   hi guy stock geared solely towards heavily gho...  \n",
       "7   started ing march adding k month account gradu...  \n",
       "8   researched much understand mortgage company of...  \n",
       "9                                                 stm  \n",
       "10  new targeted matching service grow network des...  \n",
       "11  hello year hard know live chile unemployed sin...  \n",
       "12                    st time importer home gym china  \n",
       "13  blog post based conversation reza sabahi reza ...  \n",
       "14                                    freight company  \n",
       "15  came across thought might interesting info pub...  \n",
       "16          example mortgage cost landlord price rent  \n",
       "17           question open account interactive broker  \n",
       "18  wondering hear mainly public consultation ofte...  \n",
       "19  title say worth going effort condensing book n...  \n",
       "20            problem validation cash flow management  \n",
       "21  group marijuana likely become accepted legaliz...  \n",
       "22  penumbra tank hard xtra flex catheter meant he...  \n",
       "23  hi like find community avenue helped learning ...  \n",
       "24              finish reading book make note key bit  \n",
       "25                     freight transportation company  \n",
       "26                  writing naked put option question  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The actual probability of the class being predicted by the model\n",
    "combined_mis = pd.DataFrame(lrcv.predict_proba(X_test_vec[index_misclassified_post]), columns=['Invest prob', 'Entrepreneur prob'])\n",
    "combined_mis = pd.concat([combined_mis, X_test[index_misclassified_post].reset_index(drop=True)], axis=1) # Combining the text posts and their probabilities\n",
    "combined_mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model misclassified some posts as some had only a few words and their probabilities are quite close which makes the classifier to difficult to classify the post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the frequency of the words that are misclassified as entrepreneur label\n",
    "misclass_words = combined_mis['text'].str.cat(sep=' ')\n",
    "\n",
    "misclass_words_dict = {}\n",
    "for word in misclass_words.split():\n",
    "    if word in misclass_words_dict:\n",
    "        misclass_words_dict[word] = misclass_words_dict[word] + 1\n",
    "    else:\n",
    "        misclass_words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reza</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financing</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Frequency\n",
       "company           29\n",
       "bank              23\n",
       "revenue           17\n",
       "reza              13\n",
       "going             13\n",
       "business          13\n",
       "need              12\n",
       "also              12\n",
       "startup           12\n",
       "financing         12"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclass_words_freq = pd.DataFrame(misclass_words_dict.values(), \n",
    "             index=misclass_words_dict.keys(),\n",
    "             columns=['Frequency']).sort_values('Frequency',ascending=False)\n",
    "\n",
    "# Top 10 words that were in misclassified posts\n",
    "misclass_words_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>3.105827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>1.670755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>1.629490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>1.468443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1.467806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fund</th>\n",
       "      <td>0.693976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>0.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ment</th>\n",
       "      <td>0.637986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ing</th>\n",
       "      <td>0.423148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>0.272978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>635 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficients\n",
       "business       3.105827\n",
       "product        1.670755\n",
       "startup        1.629490\n",
       "marketing      1.468443\n",
       "start          1.467806\n",
       "...                 ...\n",
       "fund           0.693976\n",
       "buy            0.639700\n",
       "ment           0.637986\n",
       "ing            0.423148\n",
       "stock          0.272978\n",
       "\n",
       "[635 rows x 1 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv_coef[lrcv_coef.index.isin(list(misclass_words_freq.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posts and titles that were misclassified as entrepreneur had words that have strong coefficients in them such as 'business'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using Multi-nominal Bayes as the X column is filled with the integer counts of the terms in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Multinominal Naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624384236453202"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instiate the model\n",
    "mnb = MultinomialNB()\n",
    "# Fit the training set\n",
    "mnb.fit(X_train_vec, y_train) \n",
    "\n",
    "# Accuracy score of the training set\n",
    "mnb.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428044280442804"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score of the test set\n",
    "mnb.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a slight overfitting of the model on the test set but Multi-nominal Naive Bayes seems to generalize better than Logistics Regression and scores better than the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the probabilities for the Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Negative Class Prob</th>\n",
       "      <th>Positive Class Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>-10.193635</td>\n",
       "      <td>-10.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa</td>\n",
       "      <td>-10.193635</td>\n",
       "      <td>-11.022148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aapl</td>\n",
       "      <td>-9.500488</td>\n",
       "      <td>-11.022148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aar</td>\n",
       "      <td>-10.193635</td>\n",
       "      <td>-11.022148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaron</td>\n",
       "      <td>-10.886783</td>\n",
       "      <td>-10.329000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Token  Negative Class Prob  Positive Class Prob\n",
       "0     aa           -10.193635           -10.329000\n",
       "1    aaa           -10.193635           -11.022148\n",
       "2   aapl            -9.500488           -11.022148\n",
       "3    aar           -10.193635           -11.022148\n",
       "4  aaron           -10.886783           -10.329000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_prob = pd.DataFrame({'Token':cvec.get_feature_names(),'Negative Class Prob':mnb.feature_log_prob_[0],\n",
    "              'Positive Class Prob':mnb.feature_log_prob_[1]})\n",
    "mnb_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Positive Class Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>business</td>\n",
       "      <td>-4.600525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6799</th>\n",
       "      <td>people</td>\n",
       "      <td>-5.066310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>would</td>\n",
       "      <td>-5.155679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>like</td>\n",
       "      <td>-5.161361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>get</td>\n",
       "      <td>-5.214005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>one</td>\n",
       "      <td>-5.311721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>idea</td>\n",
       "      <td>-5.321704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10410</th>\n",
       "      <td>work</td>\n",
       "      <td>-5.338568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>time</td>\n",
       "      <td>-5.345394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7223</th>\n",
       "      <td>product</td>\n",
       "      <td>-5.352267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token  Positive Class Prob\n",
       "1274   business            -4.600525\n",
       "6799     people            -5.066310\n",
       "10440     would            -5.155679\n",
       "5429       like            -5.161361\n",
       "4022        get            -5.214005\n",
       "6442        one            -5.311721\n",
       "4548       idea            -5.321704\n",
       "10410      work            -5.338568\n",
       "9461       time            -5.345394\n",
       "7223    product            -5.352267"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top log probabilities for the positive class, the entrepreneur subreddit\n",
    "\n",
    "mnb_prob.sort_values('Positive Class Prob', ascending=False)[['Token','Positive Class Prob']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Negative Class Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8906</th>\n",
       "      <td>stock</td>\n",
       "      <td>-4.625291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>company</td>\n",
       "      <td>-4.762099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>market</td>\n",
       "      <td>-4.865759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10490</th>\n",
       "      <td>year</td>\n",
       "      <td>-5.093769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>would</td>\n",
       "      <td>-5.199807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>share</td>\n",
       "      <td>-5.280980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>price</td>\n",
       "      <td>-5.397845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>like</td>\n",
       "      <td>-5.539675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>ing</td>\n",
       "      <td>-5.666427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>inc</td>\n",
       "      <td>-5.710633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Token  Negative Class Prob\n",
       "8906     stock            -4.625291\n",
       "1776   company            -4.762099\n",
       "5702    market            -4.865759\n",
       "10490     year            -5.093769\n",
       "10440    would            -5.199807\n",
       "8368     share            -5.280980\n",
       "7169     price            -5.397845\n",
       "5429      like            -5.539675\n",
       "4780       ing            -5.666427\n",
       "4678       inc            -5.710633"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top log probabilities for the negative class, the investing subreddit\n",
    "mnb_prob.sort_values('Negative Class Prob', ascending=False)[['Token','Negative Class Prob']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the probability means the features are more important for the positive class, which is the entrepreneur subreddit.\n",
    "\n",
    "The same can be said for the negative class which is the investing subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 234\n",
      "False Positive: 16\n",
      "True Positive: 277\n",
      "False Negative: 15\n"
     ]
    }
   ],
   "source": [
    "y_pred = mnb.predict(X_test_vec)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[234,  16],\n",
       "       [ 15, 277]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specficity: 0.94\n",
      "Sensitivity: 0.95\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn+fp) # How accurately can the model predict for the negative class\n",
    "sensitivity = tp / (tp+fn) # How accurately can the model predict for the positive class\n",
    "accruacy = (tp+tn) / (tp+tn+fp+fn)\n",
    "\n",
    "print('Specficity:', round(specificity,2))\n",
    "print('Sensitivity:',round(sensitivity,2))\n",
    "print('Accuracy:',round(accruacy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 31 posts were misclassified, 16 False Positives + 15 False Negatives, which is much better than 48 posts that were misclassified for the logistics regression. The scores are also slightly higher compared to the logistics regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassified posts for Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 44,  47,  61,  67,  90, 103, 178, 185, 208, 211, 221, 223, 232,\n",
       "            246, 251, 290, 295, 327, 385, 387, 393, 420, 421, 430, 432, 464,\n",
       "            470, 499, 500, 503, 522],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of Misclassified posts for the test set\n",
    "index_misclassified_post = y_test[y_pred != y_test].index\n",
    "index_misclassified_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>raising usd ask anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>weed business profitable worth california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>amazon start amazon pharmacy w free delivery p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>looking buying vending machine trying figure l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>stm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>new targeted matching service grow network des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>hello year hard know live chile unemployed sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>st time importer home gym china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>amp x b disclaimer christian username parody r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>freight company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>best place look potential partner co or partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>example mortgage cost landlord price rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>actually able get lawsuit purchased within dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>apology mod technically break rule seemed like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>title say worth going effort condensing book n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>looking buying vending machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>problem validation cash flow management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>anyone website stock footage image fat watermark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>wife started powersports rental company vehicl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>group marijuana likely become accepted legaliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>warrant conversion general sbews specifically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>could stop loss saved luckin coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>freight transportation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>writing naked put option question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "44                            raising usd ask anything\n",
       "47           weed business profitable worth california\n",
       "61   amazon start amazon pharmacy w free delivery p...\n",
       "67   looking buying vending machine trying figure l...\n",
       "90        want learn valuing business best place start\n",
       "103  suppose based vaccine anticipation news wonder...\n",
       "178                                                stm\n",
       "185  new targeted matching service grow network des...\n",
       "208  hello year hard know live chile unemployed sin...\n",
       "211                    st time importer home gym china\n",
       "221  amp x b disclaimer christian username parody r...\n",
       "223  blog post based conversation reza sabahi reza ...\n",
       "232                                    freight company\n",
       "246  came across thought might interesting info pub...\n",
       "251  best place look potential partner co or partne...\n",
       "290          example mortgage cost landlord price rent\n",
       "295  actually able get lawsuit purchased within dat...\n",
       "327  apology mod technically break rule seemed like...\n",
       "385  wondering hear mainly public consultation ofte...\n",
       "387  title say worth going effort condensing book n...\n",
       "393                     looking buying vending machine\n",
       "420            problem validation cash flow management\n",
       "421   anyone website stock footage image fat watermark\n",
       "430  wife started powersports rental company vehicl...\n",
       "432  group marijuana likely become accepted legaliz...\n",
       "464      warrant conversion general sbews specifically\n",
       "470                could stop loss saved luckin coffee\n",
       "499  hi like find community avenue helped learning ...\n",
       "500              finish reading book make note key bit\n",
       "503                     freight transportation company\n",
       "522                  writing naked put option question"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe for the misclassified posts\n",
    "mnb_misclass_posts = pd.DataFrame(X_test[index_misclassified_post])\n",
    "mnb_misclass_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61     amazon start amazon pharmacy w free delivery p...\n",
       "67     looking buying vending machine trying figure l...\n",
       "90          want learn valuing business best place start\n",
       "103    suppose based vaccine anticipation news wonder...\n",
       "178                                                  stm\n",
       "211                      st time importer home gym china\n",
       "232                                      freight company\n",
       "246    came across thought might interesting info pub...\n",
       "290            example mortgage cost landlord price rent\n",
       "295    actually able get lawsuit purchased within dat...\n",
       "385    wondering hear mainly public consultation ofte...\n",
       "421     anyone website stock footage image fat watermark\n",
       "464        warrant conversion general sbews specifically\n",
       "499    hi like find community avenue helped learning ...\n",
       "503                       freight transportation company\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common posts that were misclassified in both logistics regression and MNB \n",
    "log_reg_misclass_posts['text'][log_reg_misclass_posts['text'].isin(mnb_misclass_posts['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 16 common posts were misclassified in both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futher Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Logisitics Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipe to add the count vectorizer and logistic regression\n",
    "pipe_logreg = Pipeline([\n",
    "    ('countvec', CountVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('logreg', lrcv),\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_log_reg = {\n",
    "    'countvec__ngram_range': [(1,1),(1,2),(2,2)], # Testing using unigrams only, unigrams and bigrams, bigrams only\n",
    "    'countvec__max_features': [1000, 2000, 3000, 4000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'countvec__min_df': [1,2], # Minimum number of times token must occur in the document to include token\n",
    "    'countvec__max_df': [.8, .9], # Ignore words that are occuring more than 80% or 90% in the documents from the corpus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_log_reg = GridSearchCV(pipe_logreg,\n",
    "                 param_grid=params_log_reg,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('countvec',\n",
       "                                        CountVectorizer(lowercase=False)),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegressionCV(Cs=[1e-05, 0.001,\n",
       "                                                                 0.01, 0.1, 1],\n",
       "                                                             cv=5,\n",
       "                                                             max_iter=5000,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             solver='saga'))]),\n",
       "             param_grid={'countvec__max_df': [0.8, 0.9],\n",
       "                         'countvec__max_features': [1000, 2000, 3000, 4000],\n",
       "                         'countvec__min_df': [1, 2],\n",
       "                         'countvec__ngram_range': [(1, 1), (1, 2), (2, 2)]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "gs_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvec__max_df': 0.8,\n",
       " 'countvec__max_features': 4000,\n",
       " 'countvec__min_df': 1,\n",
       " 'countvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best parameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9002488129154796"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best score\n",
    "gs_log_reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958743842364532"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for the training set\n",
    "gs_log_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095940959409594"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score the test set\n",
    "gs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 230\n",
      "False Positive: 20\n",
      "True Positive: 263\n",
      "False Negative: 29\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_log_reg.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite searching for the best hyperparamter and removing the stopwords, there does not seem to be much change in the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Multi-nominal Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe to add the count vectorizer and Multi-nominal Bayes model\n",
    "pipe_mnb = Pipeline([\n",
    "    ('countvec', CountVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_mnb = {\n",
    "    'countvec__ngram_range': [(1,1),(2,2)], # Testing using unigrams bigrams\n",
    "    'countvec__max_features': [8000, 9000, 10000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'countvec__min_df': [1,2], # Minimum number of documents to include token\n",
    "    'countvec__max_df': [.9, .95], # Maximum number of documents to include token\n",
    "    'mnb__alpha': [0.1,0.2], # Testing different alpha values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_mnb = GridSearchCV(pipe_mnb,\n",
    "                 param_grid=params_mnb,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('countvec',\n",
       "                                        CountVectorizer(lowercase=False)),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             param_grid={'countvec__max_df': [0.9, 0.95],\n",
       "                         'countvec__max_features': [8000, 9000, 10000],\n",
       "                         'countvec__min_df': [1, 2],\n",
       "                         'countvec__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'mnb__alpha': [0.1, 0.2]})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "gs_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvec__max_df': 0.9,\n",
       " 'countvec__max_features': 9000,\n",
       " 'countvec__min_df': 1,\n",
       " 'countvec__ngram_range': (1, 1),\n",
       " 'mnb__alpha': 0.2}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters for the model\n",
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926727445394112"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model\n",
    "gs_mnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9655172413793104"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the training set\n",
    "gs_mnb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464944649446494"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the test set\n",
    "gs_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is slightly overfitting but it's doing better than the logistics regression model with a higher accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFTID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Logisitics Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe to add the TFIDF vectorizer and Multi-nominal Bayes model\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('logreg', lrcv)\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_log_reg = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(2,2)], # Testing using unigrams only, unigrams and bigrams, bigrams only\n",
    "    'tvec__max_features': [1000, 2000, 3000, 4000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'tvec__min_df': [2,3], # Minimum number of documents to include token\n",
    "    'tvec__max_df': [.8, .9], # Maximum number of documents to include token\n",
    "    #'logreg__solver': ['newton-cg', 'liblinear'], # Testing different algorithms\n",
    "    #'logreg__C': [0.001, 0.01, 0.8] # Different alpha values, which are regularization hyper parameters to reduce the model's overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_tvec_log_reg = GridSearchCV(pipe_tvec,\n",
    "                 param_grid=params_log_reg,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegressionCV(Cs=[1e-05, 0.001,\n",
       "                                                                 0.01, 0.1, 1],\n",
       "                                                             cv=5,\n",
       "                                                             max_iter=5000,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             solver='saga'))]),\n",
       "             param_grid={'tvec__max_df': [0.8, 0.9],\n",
       "                         'tvec__max_features': [1000, 2000, 3000, 4000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)]})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "gs_tvec_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.8,\n",
       " 'tvec__max_features': 4000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters for the model\n",
    "gs_tvec_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9144178537511871"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model score\n",
    "gs_tvec_log_reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802955665024631"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the training set\n",
    "gs_tvec_log_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9372693726937269"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the test set\n",
    "gs_tvec_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost the same accuracy score with the count vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Multi-nominal Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe to add the count vectorizer and Multi-nominal Bayes model\n",
    "pipe_mnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_mnb = {\n",
    "    'tvec__ngram_range': [(1,1),(2,2)], # Testing using unigrams bigrams\n",
    "    'tvec__max_features': [5000,8000, 9000, 10000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'tvec__min_df': [1,2], # Minimum number of documents to include token\n",
    "    'tvec__max_df': [.9, .95], # Maximum number of documents to include token\n",
    "    'mnb__alpha': [0.1,0.2], # Testing different alpha values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_tvec_mnb = GridSearchCV(pipe_mnb,\n",
    "                 param_grid=params_mnb,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             param_grid={'mnb__alpha': [0.1, 0.2], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [5000, 8000, 9000, 10000],\n",
       "                         'tvec__min_df': [1, 2],\n",
       "                         'tvec__ngram_range': [(1, 1), (2, 2)]})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "gs_tvec_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.1,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__max_features': 10000,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters for the model\n",
    "gs_tvec_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94397150997151"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model\n",
    "gs_tvec_mnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895320197044335"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the training set\n",
    "gs_tvec_mnb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9501845018450185"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against the test set\n",
    "gs_tvec_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is slightly overfitting but it's doing better than the logistics regression model with a higher accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary table of the accuracy scores for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Set | Type of vectorizer | Type of model | Accuracy Score |\n",
    "|-|-|-|-|\n",
    "| Training | Count Vectorizer | Logistic Regresion | 0.9581 |\n",
    "| Training | TFIDF Vectorizer | Logistic Regression | 0.9778 |\n",
    "| Test | Count Vectorizer | Logistic Regresion | 0.9096 |\n",
    "| Test | TFIDF Vectorizer | Logistic Regression | 0.9336 |\n",
    "| Train | Count Vectorizer | Multi-nominal Naive Bayes | 0.9667 |\n",
    "| Training | TFIDF Vectorizer | Multi-nominal Naive Bayes | 0.9864 |\n",
    "| Test | Count Vectorizer | Multi-nominal Naive Bayes | 0.9354 |\n",
    "| **Test** | **TFIDF Vectorizer** | **Multi-nominal Naive Bayes** | **0.9428**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I can see that Multi-nominal Navie Bayes generally performs slightly better than Logistics Regression and TFIDF vectorizer performs slightly better than Count Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=3 color='blue'>Instead of doing the analysis of word coefficients and misclassified posts on the model that was not tuned with gridsearch, it would be better to do it on the 'best model' you selected. You should also be tuning the entire pipeline, not each step separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, posts and their titles that were transformed with TFIDF Vectorizer were shown to perform slightly better than the posts transformed with the Count Vectorizer. A similar finding was seen for datasets trained on Multi-nominal Naive Bayes (MNNB) model performed slightly better than Logistic Regression. \n",
    "Even though both models have shown close performance, the Multi-nominal Naive Bayes model with TFIDF Vectorizer has shown to be the best model with a accuracy test score of 0.9428 while the logistic regression model with TFIDF Vectorizer coming close with an accuracy test score of 0.9336. Both models also beats the baseline accuracy score of 0.5387.\n",
    "\n",
    "I would recommend MNNB model to be deployed in the entrepreneur subreddit as it's show to have a very high accuracy in filtering out investing subreddit posts from the entrepreneur subreddit and this would help to save time and energy instead of manually looking through the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=3 color='blue'>Do infer the differences between the subreddits and possible use cases for the model / insights beyond just classification."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
