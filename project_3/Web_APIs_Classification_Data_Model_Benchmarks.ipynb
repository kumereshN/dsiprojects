{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs and Classification: Model Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the graph style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>driven individual rushing towards dream ever s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reduce bounce rate webpage</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>made animated summary lean start eric ries hop...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skate ramp business</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help getting textile prototype created</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>trying learn various ing strategy came across ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>pretend know lot finance economics sold positi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>bill ackman bet market recovery despite covid ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>news covid vaccine drugmaker pfizer pfe partne...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>apparently buy people distressed mortgage note...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     driven individual rushing towards dream ever s...    1.0\n",
       "1                            reduce bounce rate webpage    1.0\n",
       "2     made animated summary lean start eric ries hop...    1.0\n",
       "3                                   skate ramp business    1.0\n",
       "4                help getting textile prototype created    1.0\n",
       "...                                                 ...    ...\n",
       "2161  trying learn various ing strategy came across ...    0.0\n",
       "2162  pretend know lot finance economics sold positi...    0.0\n",
       "2163  bill ackman bet market recovery despite covid ...    0.0\n",
       "2164  news covid vaccine drugmaker pfizer pfe partne...    0.0\n",
       "2165  apparently buy people distressed mortgage note...    0.0\n",
       "\n",
       "[2166 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_csv('./datasets/final_df.csv')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the model into their train and test set before transforming the text using the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df['text']\n",
    "y = final_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Reset the indexes\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1624,)\n",
      "(542,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the text using `countvectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the \"CountVectorizer\" object\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the words to tokenize the words  \n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_test_vec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1624 rows × 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aapl  aar  aaron  aaxn  aaz  ab  abandon  abbv  ...  zm  \\\n",
       "0      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "2      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "3      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "4      0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "...   ..  ...   ...  ...    ...   ...  ...  ..      ...   ...  ...  ..   \n",
       "1619   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1620   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1621   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1622   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "1623   0    0     0    0      0     0    0   0        0     0  ...   0   \n",
       "\n",
       "      zoetis  zone  zoo  zookeeper  zoom  zts  zuck  zuckerberg  zweig  \n",
       "0          0     0    0          0     0    0     0           0      0  \n",
       "1          0     0    0          0     0    0     0           0      0  \n",
       "2          0     0    0          0     0    0     0           0      0  \n",
       "3          0     0    0          0     2    0     0           0      0  \n",
       "4          0     0    0          0     0    0     0           0      0  \n",
       "...      ...   ...  ...        ...   ...  ...   ...         ...    ...  \n",
       "1619       0     0    0          0     0    0     0           0      0  \n",
       "1620       0     0    0          0     0    0     0           0      0  \n",
       "1621       0     0    0          0     0    0     0           0      0  \n",
       "1622       0     0    0          0     0    0     0           0      0  \n",
       "1623       0     0    0          0     0    0     0           0      0  \n",
       "\n",
       "[1624 rows x 10551 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_vec.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows × 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aapl  aar  aaron  aaxn  aaz  ab  abandon  abbv  ...  zm  zoetis  \\\n",
       "0     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "1     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "2     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "3     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "4     0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "..   ..  ...   ...  ...    ...   ...  ...  ..      ...   ...  ...  ..     ...   \n",
       "537   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "538   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "539   0    0     0    0      0     0    0   0        0     1  ...   0       0   \n",
       "540   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "541   0    0     0    0      0     0    0   0        0     0  ...   0       0   \n",
       "\n",
       "     zone  zoo  zookeeper  zoom  zts  zuck  zuckerberg  zweig  \n",
       "0       0    0          0     0    0     0           0      0  \n",
       "1       0    0          0     0    0     0           0      0  \n",
       "2       0    0          0     0    0     0           0      0  \n",
       "3       0    0          0     0    0     0           0      0  \n",
       "4       0    0          0     0    0     0           0      0  \n",
       "..    ...  ...        ...   ...  ...   ...         ...    ...  \n",
       "537     0    0          0     0    0     0           0      0  \n",
       "538     0    0          0     0    0     0           0      0  \n",
       "539     0    0          0     0    0     0           0      0  \n",
       "540     0    0          0     0    0     0           0      0  \n",
       "541     0    0          0     0    0     0           0      0  \n",
       "\n",
       "[542 rows x 10551 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert X_test into a DataFrame.\n",
    "X_test_df = pd.DataFrame(X_test_vec.toarray(),\n",
    "                         columns=cvec.get_feature_names())\n",
    "\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.538745\n",
       "0.0    0.461255\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy is required to check if the model performs better than the default model.\n",
    "\n",
    "The baseline accuracy is the majority class fall which is the entrepreneur subreddit which is the class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the logistics regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956896551724138"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Logistic Regression model, setting max_iter to a higher value to prevent convergence warning.\n",
    "lr = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "lr.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9059040590405905"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "lr.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the model is overfitting comparing against the train and test set as the train set has a higher accuracy score compared to the test test.\n",
    "\n",
    "I can reduce the number of features in the model to reduce the variance which will decrease the overfitting and help improve the accuracy score.\n",
    "\n",
    "I can also increase regularization strength of the model to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 219\n",
      "False Positive: 31\n",
      "True Positive: 272\n",
      "False Negative: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr.predict(X_test_vec)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[219,  31],\n",
       "       [ 20, 272]], dtype=int64)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specficity: 0.88\n",
      "Sensitivity: 0.93\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn+fp) # How accurately can the model predict for the negative class\n",
    "sensitivity = tp / (tp+fn) # How accurately can the model predict for the positive class\n",
    "\n",
    "print('Specficity:', round(specificity,2))\n",
    "print('Sensitivity:',round(sensitivity,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sensitivity is slightly higher compared to the Specificity, which means the model is slightly more likely to accurately predict the positive class compared to the negative class. However, since I'm trying to predict whether the model is able to accurately predict the subreddit where the post belongs to, optimizing for Sensitivity and Specificity would not be a good measure. Accuracy would be a better performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression: Identifying posts that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  1,   2,   5,   8,  12,  24,  33,  43,  56,  61,  66,  67,  79,\n",
       "             83,  90, 103, 108, 138, 151, 153, 178, 211, 223, 232, 246, 247,\n",
       "            266, 289, 295, 316, 319, 326, 337, 343, 359, 370, 384, 385, 405,\n",
       "            418, 421, 437, 445, 464, 465, 478, 487, 499, 500, 503, 505],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of Misclassified posts for the test set\n",
    "index_misclassified_post = y_test[y_pred != y_test].index\n",
    "index_misclassified_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid evaluation matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good career path mechanical engineer data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know wrong world constant money like end day m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sien sientra going tit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>worthwhile learning ment outside taking action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ace v icln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>amazon start amazon pharmacy w free delivery p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>aston martin lagonda aml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>looking buying vending machine trying figure l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>cost efficient source branded eco friendly cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>company formation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>interviewed contributed hour workweek early se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>short penumbra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>list question vcs or ask startup founder googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>scam academy private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>stm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>st time importer home gym china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>freight company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>feel free share tool amp methodology use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>reason nkla rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>news aggregator apps pay license fee display p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>actually able get lawsuit purchased within dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>question open account interactive broker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>everyone pro dayparting wondering anyone share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>crsr point would say time sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>recently pondering question hopefully find ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>looking amzn alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>issue heavy reliance complex model ebit ebitda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>nndm arkq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>need help paper corporate bond looking insight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>apply increase conversion rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>hpq short analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>anyone website stock footage image fat watermark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>bj wholesale trade much lower multiple costco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>warrant conversion general sbews specifically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>anyone opinion real estate wholesale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>ed gamestop towards beginning year made gain k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>black rock monopoly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>freight transportation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>feel like paralyasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "1                              covid evaluation matrix\n",
       "2    good career path mechanical engineer data scie...\n",
       "5                                                covid\n",
       "8    know wrong world constant money like end day m...\n",
       "12                digital advertisement market reach b\n",
       "24            competitor popping like crazy discussion\n",
       "33                              sien sientra going tit\n",
       "43      worthwhile learning ment outside taking action\n",
       "56                                          ace v icln\n",
       "61   amazon start amazon pharmacy w free delivery p...\n",
       "66                            aston martin lagonda aml\n",
       "67   looking buying vending machine trying figure l...\n",
       "79      cost efficient source branded eco friendly cup\n",
       "83                                   company formation\n",
       "90        want learn valuing business best place start\n",
       "103  suppose based vaccine anticipation news wonder...\n",
       "108  interviewed contributed hour workweek early se...\n",
       "138                                     short penumbra\n",
       "151  list question vcs or ask startup founder googl...\n",
       "153                               scam academy private\n",
       "178                                                stm\n",
       "211                    st time importer home gym china\n",
       "223  blog post based conversation reza sabahi reza ...\n",
       "232                                    freight company\n",
       "246  came across thought might interesting info pub...\n",
       "247           feel free share tool amp methodology use\n",
       "266                                 reason nkla rising\n",
       "289  news aggregator apps pay license fee display p...\n",
       "295  actually able get lawsuit purchased within dat...\n",
       "316           question open account interactive broker\n",
       "319  everyone pro dayparting wondering anyone share...\n",
       "326                     crsr point would say time sell\n",
       "337  recently pondering question hopefully find ans...\n",
       "343                           looking amzn alternative\n",
       "359     issue heavy reliance complex model ebit ebitda\n",
       "370                                          nndm arkq\n",
       "384     need help paper corporate bond looking insight\n",
       "385  wondering hear mainly public consultation ofte...\n",
       "405                     apply increase conversion rate\n",
       "418                                 hpq short analysis\n",
       "421   anyone website stock footage image fat watermark\n",
       "437  penumbra tank hard xtra flex catheter meant he...\n",
       "445      bj wholesale trade much lower multiple costco\n",
       "464      warrant conversion general sbews specifically\n",
       "465               anyone opinion real estate wholesale\n",
       "478  ed gamestop towards beginning year made gain k...\n",
       "487                                black rock monopoly\n",
       "499  hi like find community avenue helped learning ...\n",
       "500              finish reading book make note key bit\n",
       "503                     freight transportation company\n",
       "505                               feel like paralyasis"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe for the misclassified posts\n",
    "misclass_posts = pd.DataFrame(X_test[index_misclassified_post])\n",
    "misclass_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      1.0\n",
       "      ... \n",
       "537    0.0\n",
       "538    1.0\n",
       "539    0.0\n",
       "540    0.0\n",
       "541    1.0\n",
       "Length: 542, dtype: float64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the index values for y_pred\n",
    "y_pred_series = pd.Series(y_pred, index=X_test.index)\n",
    "y_pred_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the predicted and actual values for the labels\n",
    "misclass_posts['y_true'] =  y_test[index_misclassified_post]\n",
    "misclass_posts['y_pred'] = y_pred_series[index_misclassified_post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>length_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid evaluation matrix</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good career path mechanical engineer data scie...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know wrong world constant money like end day m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  y_true  y_pred  \\\n",
       "1                             covid evaluation matrix     0.0     1.0   \n",
       "2   good career path mechanical engineer data scie...     1.0     0.0   \n",
       "5                                               covid     0.0     1.0   \n",
       "8   know wrong world constant money like end day m...     1.0     0.0   \n",
       "12               digital advertisement market reach b     0.0     1.0   \n",
       "\n",
       "    length_of_text  \n",
       "1                3  \n",
       "2                7  \n",
       "5                1  \n",
       "8               71  \n",
       "12               5  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the text\n",
    "misclass_posts['length_of_text'] = misclass_posts['text'].map(lambda x:len(x.split()))\n",
    "misclass_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>length_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>stm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>nndm arkq</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>short penumbra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid evaluation matrix</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>black rock monopoly</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ace v icln</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>hpq short analysis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>scam academy private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>looking amzn alternative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>reason nkla rising</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sien sientra going tit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>aston martin lagonda aml</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>warrant conversion general sbews specifically</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>question open account interactive broker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>crsr point would say time sell</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>issue heavy reliance complex model ebit ebitda</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>need help paper corporate bond looking insight</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>bj wholesale trade much lower multiple costco</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>amazon start amazon pharmacy w free delivery p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>ed gamestop towards beginning year made gain k...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>actually able get lawsuit purchased within dat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>recently pondering question hopefully find ans...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  y_true  y_pred  \\\n",
       "5                                                covid     0.0     1.0   \n",
       "178                                                stm     0.0     1.0   \n",
       "370                                          nndm arkq     0.0     1.0   \n",
       "138                                     short penumbra     0.0     1.0   \n",
       "1                              covid evaluation matrix     0.0     1.0   \n",
       "487                                black rock monopoly     0.0     1.0   \n",
       "56                                          ace v icln     0.0     1.0   \n",
       "418                                 hpq short analysis     0.0     1.0   \n",
       "153                               scam academy private     0.0     1.0   \n",
       "343                           looking amzn alternative     0.0     1.0   \n",
       "266                                 reason nkla rising     0.0     1.0   \n",
       "33                              sien sientra going tit     0.0     1.0   \n",
       "66                            aston martin lagonda aml     0.0     1.0   \n",
       "464      warrant conversion general sbews specifically     0.0     1.0   \n",
       "316           question open account interactive broker     0.0     1.0   \n",
       "12                digital advertisement market reach b     0.0     1.0   \n",
       "326                     crsr point would say time sell     0.0     1.0   \n",
       "359     issue heavy reliance complex model ebit ebitda     0.0     1.0   \n",
       "384     need help paper corporate bond looking insight     0.0     1.0   \n",
       "90        want learn valuing business best place start     0.0     1.0   \n",
       "445      bj wholesale trade much lower multiple costco     0.0     1.0   \n",
       "500              finish reading book make note key bit     0.0     1.0   \n",
       "61   amazon start amazon pharmacy w free delivery p...     0.0     1.0   \n",
       "246  came across thought might interesting info pub...     0.0     1.0   \n",
       "385  wondering hear mainly public consultation ofte...     0.0     1.0   \n",
       "478  ed gamestop towards beginning year made gain k...     0.0     1.0   \n",
       "295  actually able get lawsuit purchased within dat...     0.0     1.0   \n",
       "499  hi like find community avenue helped learning ...     0.0     1.0   \n",
       "437  penumbra tank hard xtra flex catheter meant he...     0.0     1.0   \n",
       "337  recently pondering question hopefully find ans...     0.0     1.0   \n",
       "103  suppose based vaccine anticipation news wonder...     0.0     1.0   \n",
       "\n",
       "     length_of_text  \n",
       "5                 1  \n",
       "178               1  \n",
       "370               2  \n",
       "138               2  \n",
       "1                 3  \n",
       "487               3  \n",
       "56                3  \n",
       "418               3  \n",
       "153               3  \n",
       "343               3  \n",
       "266               3  \n",
       "33                4  \n",
       "66                4  \n",
       "464               5  \n",
       "316               5  \n",
       "12                5  \n",
       "326               6  \n",
       "359               7  \n",
       "384               7  \n",
       "90                7  \n",
       "445               7  \n",
       "500               7  \n",
       "61                9  \n",
       "246              18  \n",
       "385              20  \n",
       "478              27  \n",
       "295              41  \n",
       "499              51  \n",
       "437              62  \n",
       "337              73  \n",
       "103             124  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posts that were misclassified as entrepreneur subreddit\n",
    "misclass_entre = misclass_posts[misclass_posts['y_pred'] == 1].sort_values('length_of_text')\n",
    "misclass_entre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.19354838709677"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority of the misclassified class for entreg is 74% \n",
    "(misclass_entre['length_of_text'].map(lambda x:x<10).sum() / len(misclass_entre)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seems the majority of the posts that were misclassified as investing were have a length of less than 10 which is about 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the frequency of the words that are misclassified as entrepreneur label\n",
    "misclass_entre_words = \" \".join(misclass_entre['text'].values)\n",
    "\n",
    "misclass_entre_words_dict = {}\n",
    "for word in misclass_entre_words.split():\n",
    "    misclass_entre_words_dict[word] = misclass_entre_words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ing</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ed</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Frequency\n",
       "e          377\n",
       "b           53\n",
       "w           47\n",
       "v           35\n",
       "ing         34\n",
       "ed          26\n",
       "go          12\n",
       "day          9\n",
       "men          8\n",
       "hi           8"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(misclass_entre_words_dict.values(), \n",
    "             index=misclass_entre_words_dict.keys(),\n",
    "             columns=['Frequency']).sort_values('Frequency',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>length_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>freight company</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>company formation</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>feel like paralyasis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>freight transportation company</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>apply increase conversion rate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>anyone opinion real estate wholesale</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>st time importer home gym china</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>worthwhile learning ment outside taking action</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>cost efficient source branded eco friendly cup</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>feel free share tool amp methodology use</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>anyone website stock footage image fat watermark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>interviewed contributed hour workweek early se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good career path mechanical engineer data scie...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>list question vcs or ask startup founder googl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>news aggregator apps pay license fee display p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>everyone pro dayparting wondering anyone share...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>looking buying vending machine trying figure l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know wrong world constant money like end day m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  y_true  y_pred  \\\n",
       "232                                    freight company     1.0     0.0   \n",
       "83                                   company formation     1.0     0.0   \n",
       "505                               feel like paralyasis     1.0     0.0   \n",
       "503                     freight transportation company     1.0     0.0   \n",
       "405                     apply increase conversion rate     1.0     0.0   \n",
       "465               anyone opinion real estate wholesale     1.0     0.0   \n",
       "24            competitor popping like crazy discussion     1.0     0.0   \n",
       "211                    st time importer home gym china     1.0     0.0   \n",
       "43      worthwhile learning ment outside taking action     1.0     0.0   \n",
       "79      cost efficient source branded eco friendly cup     1.0     0.0   \n",
       "247           feel free share tool amp methodology use     1.0     0.0   \n",
       "421   anyone website stock footage image fat watermark     1.0     0.0   \n",
       "108  interviewed contributed hour workweek early se...     1.0     0.0   \n",
       "2    good career path mechanical engineer data scie...     1.0     0.0   \n",
       "151  list question vcs or ask startup founder googl...     1.0     0.0   \n",
       "289  news aggregator apps pay license fee display p...     1.0     0.0   \n",
       "319  everyone pro dayparting wondering anyone share...     1.0     0.0   \n",
       "67   looking buying vending machine trying figure l...     1.0     0.0   \n",
       "8    know wrong world constant money like end day m...     1.0     0.0   \n",
       "223  blog post based conversation reza sabahi reza ...     1.0     0.0   \n",
       "\n",
       "     length_of_text  \n",
       "232               2  \n",
       "83                2  \n",
       "505               3  \n",
       "503               3  \n",
       "405               4  \n",
       "465               5  \n",
       "24                5  \n",
       "211               6  \n",
       "43                6  \n",
       "79                7  \n",
       "247               7  \n",
       "421               7  \n",
       "108               7  \n",
       "2                 7  \n",
       "151               9  \n",
       "289               9  \n",
       "319              14  \n",
       "67               27  \n",
       "8                71  \n",
       "223             544  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posts that were misclassified as investing subreddit\n",
    "misclass_investing = misclass_posts[misclass_posts['y_pred'] == 0].sort_values('length_of_text')\n",
    "misclass_investing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority of the misclassified class for investing is 74% \n",
    "(misclass_investing['length_of_text'].map(lambda x:x<10).sum() / len(misclass_investing)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seems the majority of the posts that were misclassified as investing were have a length of less than 10 which is about 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the misclassified posts and obtaining the probability of the posts being classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invest prob</th>\n",
       "      <th>Entrepreneur prob</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.481165</td>\n",
       "      <td>0.518835</td>\n",
       "      <td>covid evaluation matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.520048</td>\n",
       "      <td>0.479952</td>\n",
       "      <td>good career path mechanical engineer data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.480389</td>\n",
       "      <td>0.519611</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.749980</td>\n",
       "      <td>0.250020</td>\n",
       "      <td>know wrong world constant money like end day m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.448877</td>\n",
       "      <td>0.551123</td>\n",
       "      <td>digital advertisement market reach b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.562297</td>\n",
       "      <td>0.437703</td>\n",
       "      <td>competitor popping like crazy discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.490710</td>\n",
       "      <td>0.509290</td>\n",
       "      <td>sien sientra going tit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.769746</td>\n",
       "      <td>0.230254</td>\n",
       "      <td>worthwhile learning ment outside taking action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.482642</td>\n",
       "      <td>0.517358</td>\n",
       "      <td>ace v icln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.219636</td>\n",
       "      <td>0.780364</td>\n",
       "      <td>amazon start amazon pharmacy w free delivery p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.411237</td>\n",
       "      <td>0.588763</td>\n",
       "      <td>aston martin lagonda aml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.927816</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>looking buying vending machine trying figure l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.517821</td>\n",
       "      <td>0.482179</td>\n",
       "      <td>cost efficient source branded eco friendly cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.553967</td>\n",
       "      <td>0.446033</td>\n",
       "      <td>company formation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.993499</td>\n",
       "      <td>want learn valuing business best place start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.260639</td>\n",
       "      <td>0.739361</td>\n",
       "      <td>suppose based vaccine anticipation news wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.626828</td>\n",
       "      <td>0.373172</td>\n",
       "      <td>interviewed contributed hour workweek early se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.399914</td>\n",
       "      <td>0.600086</td>\n",
       "      <td>short penumbra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.504372</td>\n",
       "      <td>0.495628</td>\n",
       "      <td>list question vcs or ask startup founder googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.470487</td>\n",
       "      <td>0.529513</td>\n",
       "      <td>scam academy private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.411190</td>\n",
       "      <td>0.588810</td>\n",
       "      <td>stm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.610883</td>\n",
       "      <td>0.389117</td>\n",
       "      <td>st time importer home gym china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>blog post based conversation reza sabahi reza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.556867</td>\n",
       "      <td>0.443133</td>\n",
       "      <td>freight company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.159053</td>\n",
       "      <td>0.840947</td>\n",
       "      <td>came across thought might interesting info pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.614338</td>\n",
       "      <td>0.385662</td>\n",
       "      <td>feel free share tool amp methodology use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.468087</td>\n",
       "      <td>0.531913</td>\n",
       "      <td>reason nkla rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.526977</td>\n",
       "      <td>0.473023</td>\n",
       "      <td>news aggregator apps pay license fee display p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.095575</td>\n",
       "      <td>0.904425</td>\n",
       "      <td>actually able get lawsuit purchased within dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.452709</td>\n",
       "      <td>0.547291</td>\n",
       "      <td>question open account interactive broker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.497382</td>\n",
       "      <td>everyone pro dayparting wondering anyone share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.349813</td>\n",
       "      <td>0.650187</td>\n",
       "      <td>crsr point would say time sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.261616</td>\n",
       "      <td>0.738384</td>\n",
       "      <td>recently pondering question hopefully find ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.362284</td>\n",
       "      <td>0.637716</td>\n",
       "      <td>looking amzn alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.467821</td>\n",
       "      <td>0.532179</td>\n",
       "      <td>issue heavy reliance complex model ebit ebitda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.411252</td>\n",
       "      <td>0.588748</td>\n",
       "      <td>nndm arkq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.392327</td>\n",
       "      <td>0.607673</td>\n",
       "      <td>need help paper corporate bond looking insight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.122569</td>\n",
       "      <td>0.877431</td>\n",
       "      <td>wondering hear mainly public consultation ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.658881</td>\n",
       "      <td>0.341119</td>\n",
       "      <td>apply increase conversion rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.474087</td>\n",
       "      <td>0.525913</td>\n",
       "      <td>hpq short analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.832038</td>\n",
       "      <td>0.167962</td>\n",
       "      <td>anyone website stock footage image fat watermark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.053774</td>\n",
       "      <td>0.946226</td>\n",
       "      <td>penumbra tank hard xtra flex catheter meant he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.403533</td>\n",
       "      <td>0.596467</td>\n",
       "      <td>bj wholesale trade much lower multiple costco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.384633</td>\n",
       "      <td>0.615367</td>\n",
       "      <td>warrant conversion general sbews specifically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.535570</td>\n",
       "      <td>0.464430</td>\n",
       "      <td>anyone opinion real estate wholesale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.363498</td>\n",
       "      <td>0.636502</td>\n",
       "      <td>ed gamestop towards beginning year made gain k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.484538</td>\n",
       "      <td>0.515462</td>\n",
       "      <td>black rock monopoly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.274666</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>hi like find community avenue helped learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.369732</td>\n",
       "      <td>0.630268</td>\n",
       "      <td>finish reading book make note key bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.557427</td>\n",
       "      <td>0.442573</td>\n",
       "      <td>freight transportation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.534202</td>\n",
       "      <td>0.465798</td>\n",
       "      <td>feel like paralyasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Invest prob  Entrepreneur prob  \\\n",
       "0      0.481165           0.518835   \n",
       "1      0.520048           0.479952   \n",
       "2      0.480389           0.519611   \n",
       "3      0.749980           0.250020   \n",
       "4      0.448877           0.551123   \n",
       "5      0.562297           0.437703   \n",
       "6      0.490710           0.509290   \n",
       "7      0.769746           0.230254   \n",
       "8      0.482642           0.517358   \n",
       "9      0.219636           0.780364   \n",
       "10     0.411237           0.588763   \n",
       "11     0.927816           0.072184   \n",
       "12     0.517821           0.482179   \n",
       "13     0.553967           0.446033   \n",
       "14     0.006501           0.993499   \n",
       "15     0.260639           0.739361   \n",
       "16     0.626828           0.373172   \n",
       "17     0.399914           0.600086   \n",
       "18     0.504372           0.495628   \n",
       "19     0.470487           0.529513   \n",
       "20     0.411190           0.588810   \n",
       "21     0.610883           0.389117   \n",
       "22     0.999897           0.000103   \n",
       "23     0.556867           0.443133   \n",
       "24     0.159053           0.840947   \n",
       "25     0.614338           0.385662   \n",
       "26     0.468087           0.531913   \n",
       "27     0.526977           0.473023   \n",
       "28     0.095575           0.904425   \n",
       "29     0.452709           0.547291   \n",
       "30     0.502618           0.497382   \n",
       "31     0.349813           0.650187   \n",
       "32     0.261616           0.738384   \n",
       "33     0.362284           0.637716   \n",
       "34     0.467821           0.532179   \n",
       "35     0.411252           0.588748   \n",
       "36     0.392327           0.607673   \n",
       "37     0.122569           0.877431   \n",
       "38     0.658881           0.341119   \n",
       "39     0.474087           0.525913   \n",
       "40     0.832038           0.167962   \n",
       "41     0.053774           0.946226   \n",
       "42     0.403533           0.596467   \n",
       "43     0.384633           0.615367   \n",
       "44     0.535570           0.464430   \n",
       "45     0.363498           0.636502   \n",
       "46     0.484538           0.515462   \n",
       "47     0.274666           0.725334   \n",
       "48     0.369732           0.630268   \n",
       "49     0.557427           0.442573   \n",
       "50     0.534202           0.465798   \n",
       "\n",
       "                                                 text  \n",
       "0                             covid evaluation matrix  \n",
       "1   good career path mechanical engineer data scie...  \n",
       "2                                               covid  \n",
       "3   know wrong world constant money like end day m...  \n",
       "4                digital advertisement market reach b  \n",
       "5            competitor popping like crazy discussion  \n",
       "6                              sien sientra going tit  \n",
       "7      worthwhile learning ment outside taking action  \n",
       "8                                          ace v icln  \n",
       "9   amazon start amazon pharmacy w free delivery p...  \n",
       "10                           aston martin lagonda aml  \n",
       "11  looking buying vending machine trying figure l...  \n",
       "12     cost efficient source branded eco friendly cup  \n",
       "13                                  company formation  \n",
       "14       want learn valuing business best place start  \n",
       "15  suppose based vaccine anticipation news wonder...  \n",
       "16  interviewed contributed hour workweek early se...  \n",
       "17                                     short penumbra  \n",
       "18  list question vcs or ask startup founder googl...  \n",
       "19                               scam academy private  \n",
       "20                                                stm  \n",
       "21                    st time importer home gym china  \n",
       "22  blog post based conversation reza sabahi reza ...  \n",
       "23                                    freight company  \n",
       "24  came across thought might interesting info pub...  \n",
       "25           feel free share tool amp methodology use  \n",
       "26                                 reason nkla rising  \n",
       "27  news aggregator apps pay license fee display p...  \n",
       "28  actually able get lawsuit purchased within dat...  \n",
       "29           question open account interactive broker  \n",
       "30  everyone pro dayparting wondering anyone share...  \n",
       "31                     crsr point would say time sell  \n",
       "32  recently pondering question hopefully find ans...  \n",
       "33                           looking amzn alternative  \n",
       "34     issue heavy reliance complex model ebit ebitda  \n",
       "35                                          nndm arkq  \n",
       "36     need help paper corporate bond looking insight  \n",
       "37  wondering hear mainly public consultation ofte...  \n",
       "38                     apply increase conversion rate  \n",
       "39                                 hpq short analysis  \n",
       "40   anyone website stock footage image fat watermark  \n",
       "41  penumbra tank hard xtra flex catheter meant he...  \n",
       "42      bj wholesale trade much lower multiple costco  \n",
       "43      warrant conversion general sbews specifically  \n",
       "44               anyone opinion real estate wholesale  \n",
       "45  ed gamestop towards beginning year made gain k...  \n",
       "46                                black rock monopoly  \n",
       "47  hi like find community avenue helped learning ...  \n",
       "48              finish reading book make note key bit  \n",
       "49                     freight transportation company  \n",
       "50                               feel like paralyasis  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The actual probability of the class being predicted by the model\n",
    "combined_mis = pd.DataFrame(lr.predict_proba(X_test_vec[index_misclassified_post]), columns=['Invest prob', 'Entrepreneur prob'])\n",
    "combined_mis = pd.concat([combined_mis, X_test[index_misclassified_post].reset_index(drop=True)], axis=1)\n",
    "combined_mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model misclassified some posts as most of the words fall under the opposite class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients for the Logisitics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>7.405037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idea</th>\n",
       "      <td>5.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>3.912808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>3.892313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>3.593916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>2.623289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>2.408979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>2.398420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa</th>\n",
       "      <td>2.264786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>2.196020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficients\n",
       "business       7.405037\n",
       "idea           5.770500\n",
       "product        3.912808\n",
       "startup        3.892313\n",
       "marketing      3.593916\n",
       "name           2.623289\n",
       "com            2.408979\n",
       "free           2.398420\n",
       "usa            2.264786\n",
       "start          2.196020"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top coefficients for the positive class, the entrepreneur subreddit\n",
    "lr_coef = pd.DataFrame(np.exp(lr.coef_[0]),\n",
    "                          index=vectorizer.get_feature_names(),\n",
    "                          columns=['Coefficients']).sort_values('Coefficients',ascending=False)\n",
    "lr_coef.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the logistic regression coefficients represent the log odds that an observation is in target class, 1, given the values of it X variables, the log odd coefficients need to be converted to regular odds to make sense of them. This is done through exponentiating the log odds coefficients.\n",
    "\n",
    "For example:\n",
    "\n",
    "For every one-unit in `business`, the odds that the observation is in entrepreneur class is 7.4 times as large as the odds that the observation is not in the entrepreneur class provided all other variables are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>growth</th>\n",
       "      <td>0.328880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tesla</th>\n",
       "      <td>0.327780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roku</th>\n",
       "      <td>0.309921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portfolio</th>\n",
       "      <td>0.300534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought</th>\n",
       "      <td>0.292947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palantir</th>\n",
       "      <td>0.263532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ment</th>\n",
       "      <td>0.236508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etf</th>\n",
       "      <td>0.214843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ing</th>\n",
       "      <td>0.101773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>0.059345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coefficients\n",
       "growth         0.328880\n",
       "tesla          0.327780\n",
       "roku           0.309921\n",
       "portfolio      0.300534\n",
       "thought        0.292947\n",
       "palantir       0.263532\n",
       "ment           0.236508\n",
       "etf            0.214843\n",
       "ing            0.101773\n",
       "stock          0.059345"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top bottom coefficients for the negative class, for the investing subreddit\n",
    "lr_coef.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coefficients are least likely to represent the entrepreneur subreddit class and more likely to represent the investing subreddit class given their low coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using Multi-nominal Bayes as the X column is filled with the integer counts of the terms in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Multinominal Naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624384236453202"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instiate the model\n",
    "mnb = MultinomialNB()\n",
    "# Fit the training set\n",
    "mnb.fit(X_train_vec, y_train) \n",
    "\n",
    "# Accuracy score of the training set\n",
    "mnb.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428044280442804"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score of the test set\n",
    "mnb.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a slight overfitting of the model on the test set but Multi-nominal Naive Bayes seems to generalize better than Logistics Regression and scores better than the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients for the Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob for positive class\n",
    "pos_class_prob_sorted = mnb.feature_log_prob_[1, :].argsort()\n",
    "#prob for negative class\n",
    "neg_class_prob_sorted = mnb.feature_log_prob_[0, :].argsort()\n",
    "#getting the top features \n",
    "neg_top_features = np.take(cvec.get_feature_names(), neg_class_prob_sorted)\n",
    "pos_top_features = np.take(cvec.get_feature_names(), pos_class_prob_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.32900036, -11.02214754, -11.02214754, ...,  -9.63585318,\n",
       "         -9.63585318, -11.02214754]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([10550,  7388,  7386,  7385,  3343,\\n            ...\\n             4022,  5429, 10440,  6799,  1274],\\n           dtype='int64', length=10009). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-452d2d1ec5e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_class_prob_sorted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[1;31m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlbook\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_seq_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"display.width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                     raise KeyError(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                         \u001b[1;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                         \u001b[1;34m\"is no longer supported. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([10550,  7388,  7386,  7385,  3343,\\n            ...\\n             4022,  5429, 10440,  6799,  1274],\\n           dtype='int64', length=10009). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\""
     ]
    }
   ],
   "source": [
    "pos_class_prob_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Logisitics Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearch CV on the CountVectorizer, Logreg, mnb.\n",
    "\n",
    "Using GridSearchCV on the TFIFD, Logreg, mnb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipe to add the count vectorizer and logistic regression\n",
    "pipe_logreg = Pipeline([\n",
    "    ('countvec', CountVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('logreg', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_log_reg = {\n",
    "    'countvec__ngram_range': [(1,1),(2,2)], # Testing using unigrams and unigrams and bigrams\n",
    "    'countvec__max_features': [5000, 6000, 7000, 8000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'countvec__min_df': [2,3], # Minimum number of documents to include token\n",
    "    'countvec__max_df': [.8, .9], # Maximum number of documents to include token\n",
    "    'logreg__solver': ['newton-cg', 'liblinear'], # Testing different algorithms\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_log_reg = GridSearchCV(pipe_logreg,\n",
    "                 param_grid=params_log_reg,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('countvec',\n",
       "                                        CountVectorizer(lowercase=False)),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=500))]),\n",
       "             param_grid={'countvec__max_df': [0.8, 0.9],\n",
       "                         'countvec__max_features': [5000, 6000, 7000, 8000],\n",
       "                         'countvec__min_df': [2, 3],\n",
       "                         'countvec__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'logreg__solver': ['newton-cg', 'liblinear']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvec__max_df': 0.8,\n",
       " 'countvec__max_features': 5000,\n",
       " 'countvec__min_df': 2,\n",
       " 'countvec__ngram_range': (1, 1),\n",
       " 'logreg__solver': 'liblinear'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9014852801519467"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926108374384236"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8985239852398524"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 218\n",
      "False Positive: 32\n",
      "True Positive: 269\n",
      "False Negative: 23\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_log_reg.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[218,  32],\n",
       "       [ 23, 269]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specficity: 0.87\n",
      "Sensitivity: 0.92\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn+fp) # How accurately can the model predict for the negative class\n",
    "sensitivity = tp / (tp+fn) # How accurately can the model predict for the positive class\n",
    "\n",
    "print('Specficity:', round(specificity,2))\n",
    "print('Sensitivity:',round(sensitivity,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Multi-nominal Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe to add the count vectorizer and Multi-nominal Bayes model\n",
    "pipe_mnb = Pipeline([\n",
    "    ('countvec', CountVectorizer(lowercase=False)), # Already coverted to lowercase\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Parameters to test the different hyper parameters\n",
    "params_mnb = {\n",
    "    'countvec__ngram_range': [(1,1),(2,2)], # Testing using unigrams bigrams\n",
    "    'countvec__max_features': [8000, 9000, 10000], # Since features are about 10,551, I'll try to use lower features\n",
    "    'countvec__min_df': [1,2], # Minimum number of documents to include token\n",
    "    'countvec__max_df': [.9, .95], # Maximum number of documents to include token\n",
    "    'mnb__alpha': [0.1,0.2], # Testing different alpha values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV\n",
    "\n",
    "gs_mnb = GridSearchCV(pipe_mnb,\n",
    "                 param_grid=params_mnb,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('countvec',\n",
       "                                        CountVectorizer(lowercase=False)),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             param_grid={'countvec__max_df': [0.9, 0.95],\n",
       "                         'countvec__max_features': [8000, 9000, 10000],\n",
       "                         'countvec__min_df': [1, 2],\n",
       "                         'countvec__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'mnb__alpha': [0.1, 0.2]})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvec__max_df': 0.9,\n",
       " 'countvec__max_features': 9000,\n",
       " 'countvec__min_df': 1,\n",
       " 'countvec__ngram_range': (1, 1),\n",
       " 'mnb__alpha': 0.2}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926727445394112"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9655172413793104"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464944649446494"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFTID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tvec.fit_transform(X_train)\n",
    "X_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train and X_test into their DataFrames.\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=tvec.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aapl  aar  aaron  aaxn  aaz   ab  abandon  abbv  ...   zm  \\\n",
       "0  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "1  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "2  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "3  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "4  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "\n",
       "   zoetis  zone  zoo  zookeeper      zoom  zts  zuck  zuckerberg  zweig  \n",
       "0     0.0   0.0  0.0        0.0  0.000000  0.0   0.0         0.0    0.0  \n",
       "1     0.0   0.0  0.0        0.0  0.000000  0.0   0.0         0.0    0.0  \n",
       "2     0.0   0.0  0.0        0.0  0.000000  0.0   0.0         0.0    0.0  \n",
       "3     0.0   0.0  0.0        0.0  0.080284  0.0   0.0         0.0    0.0  \n",
       "4     0.0   0.0  0.0        0.0  0.000000  0.0   0.0         0.0    0.0  \n",
       "\n",
       "[5 rows x 10551 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaxn</th>\n",
       "      <th>aaz</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbv</th>\n",
       "      <th>...</th>\n",
       "      <th>zm</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zts</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aapl  aar  aaron  aaxn  aaz   ab  abandon  abbv  ...   zm  \\\n",
       "0  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "1  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "2  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "3  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "4  0.0  0.0   0.0  0.0    0.0   0.0  0.0  0.0      0.0   0.0  ...  0.0   \n",
       "\n",
       "   zoetis  zone  zoo  zookeeper  zoom  zts  zuck  zuckerberg  zweig  \n",
       "0     0.0   0.0  0.0        0.0   0.0  0.0   0.0         0.0    0.0  \n",
       "1     0.0   0.0  0.0        0.0   0.0  0.0   0.0         0.0    0.0  \n",
       "2     0.0   0.0  0.0        0.0   0.0  0.0   0.0         0.0    0.0  \n",
       "3     0.0   0.0  0.0        0.0   0.0  0.0   0.0         0.0    0.0  \n",
       "4     0.0   0.0  0.0        0.0   0.0  0.0   0.0         0.0    0.0  \n",
       "\n",
       "[5 rows x 10551 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the logistics regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858374384236454"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Logistic Regression model, setting max_iter to a higher value to prevent convergence warning.\n",
    "lr = LogisticRegression(solver='newton-cg',max_iter=500)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940959409594096"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to check\n",
    "It seems the model is overfitting comparing against the train and test set as the train set has a higher accuracy score compared to the test test.\n",
    "\n",
    "I can reduce the number of features in the model to reduce the variance which will decrease the overfitting and help improve the accuracy score.\n",
    "\n",
    "I can also increase regularization strength of the model to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 233\n",
      "False Positive: 17\n",
      "True Positive: 277\n",
      "False Negative: 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"True Positive:\", tp)\n",
    "print(\"False Negative:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[233,  17],\n",
       "       [ 15, 277]], dtype=int64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specficity: 0.93\n",
      "Sensitivity: 0.95\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn+fp) # How accurately can the model predict for the negative class\n",
    "sensitivity = tp / (tp+fn) # How accurately can the model predict for the positive class\n",
    "\n",
    "print('Specficity:', round(specificity,2))\n",
    "print('Sensitivity:',round(sensitivity,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to check\n",
    "\n",
    "The Sensitivity is slightly higher compared to the Specificity, which means the model is slightly more likely to accurately predict the positive class compared to the negative class. However, since I'm trying to predict whether the model is able to accurately predict the subreddit where the post belongs to, optimizing for Sensitivity and Specificity would not be a good measure. Accuracy would be a better performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Multi-nominal Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using Multi-nominal Bayes as the X column is filled with the integer counts of the terms in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Multinominal Naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741379310344828"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instiate the model\n",
    "mnb = MultinomialNB()\n",
    "# Fit the training set\n",
    "mnb.fit(X_train, y_train) \n",
    "\n",
    "# Accuracy score of the training set\n",
    "mnb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940959409594096"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score of the test set\n",
    "mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a slight overfitting of the model on the test set but Multi-nominal Naive Bayes seems to generalize better than Logistics Regression and scores better than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
