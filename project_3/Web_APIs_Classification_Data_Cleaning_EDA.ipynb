{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs and Classification: Data cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "To assist one of our colleagues, who is a moderator of the entrepreneur subreddit, with filtering out the investing posts in the entrepreneur subreddit.\n",
    "The entrepreneur subreddit is mostly filled with irrelevant posts from the investing subreddit, which causes annoyance to the business owners who share their business ideas. By constructing a classifier model, we can use it to seperate business ideas and business investment strategies for new business owners who desire to start their side hustle. The two classifier models that will be constructed will be the **Logistics Regression Classifier and the Multi-nominal Bayes Model**. The performance metric to be used for measuring against the models will be **Accuracy** as the model needs to classify the post according to their respective subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The project aims to create a classification model to distinguish posts from the entrepreneur subreddit and the investing subreddit. The posts and title were scrapped from their respective subreddits using reddit's API and the requests library. The posts were \"cleaned\" to remove unwanted characters and a brief exploratory data analysis was done on the dataset. \n",
    "\n",
    "From the analysis, Term Frequency Inverse Document Frequency (TFIDF) Vectorizer was found to perform slightly better than the Count Vectorizer. Also, Multi-nominal Naive Bayes Model performed slightly better than the Logistics Regression model even though their performance was comparable. Both models also have a higher accuracy score compared to Baseline accuracy.\n",
    "\n",
    "\n",
    "### Contents:\n",
    "- [Data Import & Cleaning for Entrepreneur subreddit](#Obtaining-the-data-from-entrepreneur-sub-reddit)\n",
    "- [Data Import & Cleaning for Investing subreddit](#Obtaining-the-data-from-investing-sub-reddit)\n",
    "- [Exploratory Data Analysis for Entrepreneur Subreddit](#EDA-on-Entrepreneurship-sub)\n",
    "- [Exploratory Data Analysis for Investing Subreddit](#EDA-on-Investing-subreddit)\n",
    "- [Merge the dataframes](#Merge-both-subreddit-dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the graph style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the data from entrepreneur sub reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_post(url, no_loop):\n",
    "    \"\"\"\n",
    "    This function obtains the posts from the chosen subreddit from the url variable.\n",
    "    no_loop is the number of loops \n",
    "    name is the name of the csv file to save\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    after = None\n",
    "    new_df = 'No dataframe as it\\'s one loop'\n",
    "    name = re.search('r\\/(.+).json',url).group(1) # search for the name of the sub reddit\n",
    "\n",
    "    for loop in range(no_loop): # Number of loops\n",
    "        if after == None: # If there's no next post\n",
    "            current_url = url # Make use of the current url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after # Current url becomes the next post\n",
    "        print(f'Current url is: {current_url}') # Prints the current url\n",
    "        res = requests.get(current_url, headers={'User-agent': 'Entre 2.0'}) # Create the request, USER AGENT can be changed\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code) # If error, then break\n",
    "            break\n",
    "\n",
    "        current_dict = res.json() # Parse into JSON\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']] # Gets the current posts\n",
    "        posts.extend(current_posts) # Store it in a list named posts\n",
    "        after = current_dict['data']['after'] # Get the next url\n",
    "\n",
    "        if loop > 0: # Saving the progress\n",
    "            prev_posts = pd.read_csv('./datasets/' + name + '.csv') # Save the posts\n",
    "            current_df = pd.DataFrame(current_posts) # current posts in a new dataframe\n",
    "            new_df = pd.concat([prev_posts, current_df]) # Once it breaks out of for loop, new_df is gone.\n",
    "            new_df.to_csv('./datasets/' + name + '.csv', index=False)\n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv('./datasets/' + name + '.csv', index = False)\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print(f'Sleep for {sleep_duration} seconds') # Sleep duration in seconds\n",
    "        time.sleep(sleep_duration)\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap the entrepreneur subreddit posts, about 1,000 posts scrapped\n",
    "get_reddit_post('https://www.reddit.com/r/entrepreneur.json',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entrepreneur csv file\n",
    "entre_df = pd.read_csv('./datasets/entrepreneur.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               Thank you Thursday! - (November 26, 2020)\n",
       "selftext    Your opportunity to thank the /r/Entrepreneur ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entre_df[['title','selftext']].loc[0] # Gets the title and text of the first post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data in entrepreneur sub reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop the first row as it's not relevant, it's ask questions monday\n",
    "entre_df.drop(labels=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>Happy Thanksgiving everyone!\\n\\nToday probably...</td>\n",
       "      <td>t2_10hwkz</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Being driven individuals, we are all rushing t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>846881</td>\n",
       "      <td>1.606411e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>Hi, recently we had a client who was strugglin...</td>\n",
       "      <td>t2_5mi25ldv</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>How you can reduce bounce rate from your webpage?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>846881</td>\n",
       "      <td>1.606449e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>Link to video: https://youtu.be/j6QPZp--lJE\\n ...</td>\n",
       "      <td>t2_17jopcwt</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>I made an animated summary of \"The lean Start ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>846881</td>\n",
       "      <td>1.606453e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>Hi all I'm a 16 year old from Adelaide Austral...</td>\n",
       "      <td>t2_1bftus7y</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Skate ramp business</td>\n",
       "      <td>[{'e': 'text', 't': 'Recommendations?'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>846881</td>\n",
       "      <td>1.606390e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>b5eccc92-6452-11e6-93ad-0ecc2c508ed9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>Hey entrepreneurs - I recently came up with an...</td>\n",
       "      <td>t2_rxefv</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Help with getting a textile prototype created.</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>846881</td>\n",
       "      <td>1.606457e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   approved_at_utc     subreddit  \\\n",
       "1              NaN  Entrepreneur   \n",
       "2              NaN  Entrepreneur   \n",
       "3              NaN  Entrepreneur   \n",
       "4              NaN  Entrepreneur   \n",
       "5              NaN  Entrepreneur   \n",
       "\n",
       "                                            selftext author_fullname  saved  \\\n",
       "1  Happy Thanksgiving everyone!\\n\\nToday probably...       t2_10hwkz  False   \n",
       "2  Hi, recently we had a client who was strugglin...     t2_5mi25ldv  False   \n",
       "3  Link to video: https://youtu.be/j6QPZp--lJE\\n ...     t2_17jopcwt  False   \n",
       "4  Hi all I'm a 16 year old from Adelaide Austral...     t2_1bftus7y  False   \n",
       "5  Hey entrepreneurs - I recently came up with an...        t2_rxefv  False   \n",
       "\n",
       "   mod_reason_title  gilded  clicked  \\\n",
       "1               NaN       0    False   \n",
       "2               NaN       0    False   \n",
       "3               NaN       0    False   \n",
       "4               NaN       0    False   \n",
       "5               NaN       0    False   \n",
       "\n",
       "                                               title  \\\n",
       "1  Being driven individuals, we are all rushing t...   \n",
       "2  How you can reduce bounce rate from your webpage?   \n",
       "3  I made an animated summary of \"The lean Start ...   \n",
       "4                                Skate ramp business   \n",
       "5     Help with getting a textile prototype created.   \n",
       "\n",
       "                        link_flair_richtext  ... parent_whitelist_status  \\\n",
       "1                                        []  ...                 all_ads   \n",
       "2                                        []  ...                 all_ads   \n",
       "3                                        []  ...                 all_ads   \n",
       "4  [{'e': 'text', 't': 'Recommendations?'}]  ...                 all_ads   \n",
       "5                                        []  ...                 all_ads   \n",
       "\n",
       "   stickied                                                url  \\\n",
       "1     False  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "2     False  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "3     False  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "4     False  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "5     False  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "\n",
       "  subreddit_subscribers   created_utc  num_crossposts  media is_video  \\\n",
       "1                846881  1.606411e+09               1    NaN    False   \n",
       "2                846881  1.606449e+09               0    NaN    False   \n",
       "3                846881  1.606453e+09               0    NaN    False   \n",
       "4                846881  1.606390e+09               0    NaN    False   \n",
       "5                846881  1.606457e+09               0    NaN    False   \n",
       "\n",
       "                 link_flair_template_id author_cakeday  \n",
       "1                                   NaN            NaN  \n",
       "2                                   NaN            NaN  \n",
       "3                                   NaN            NaN  \n",
       "4  b5eccc92-6452-11e6-93ad-0ecc2c508ed9            NaN  \n",
       "5                                   NaN            NaN  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Being driven individuals, we are all rushing t...</td>\n",
       "      <td>Happy Thanksgiving everyone!\\n\\nToday probably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How you can reduce bounce rate from your webpage?</td>\n",
       "      <td>Hi, recently we had a client who was strugglin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made an animated summary of \"The lean Start ...</td>\n",
       "      <td>Link to video: https://youtu.be/j6QPZp--lJE\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skate ramp business</td>\n",
       "      <td>Hi all I'm a 16 year old from Adelaide Austral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Help with getting a textile prototype created.</td>\n",
       "      <td>Hey entrepreneurs - I recently came up with an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Me again with another side hustle vid for feed...</td>\n",
       "      <td>G'day guys,  \\nBack again with the second part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>I made an animated summary of \"The Magic of Th...</td>\n",
       "      <td>Link to video: https://youtu.be/wdQRQ82AED8\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tik tok marketing?</td>\n",
       "      <td>Have been seeing a lot lately from startups tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>How to make passive income</td>\n",
       "      <td>This is the best way to earn money from HOme\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Leave my “safe” job to be self employed? Need ...</td>\n",
       "      <td>I currently work in law enforcement and am pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "1    Being driven individuals, we are all rushing t...   \n",
       "2    How you can reduce bounce rate from your webpage?   \n",
       "3    I made an animated summary of \"The lean Start ...   \n",
       "4                                  Skate ramp business   \n",
       "5       Help with getting a textile prototype created.   \n",
       "..                                                 ...   \n",
       "993  Me again with another side hustle vid for feed...   \n",
       "994  I made an animated summary of \"The Magic of Th...   \n",
       "995                                 Tik tok marketing?   \n",
       "996                         How to make passive income   \n",
       "997  Leave my “safe” job to be self employed? Need ...   \n",
       "\n",
       "                                              selftext  \n",
       "1    Happy Thanksgiving everyone!\\n\\nToday probably...  \n",
       "2    Hi, recently we had a client who was strugglin...  \n",
       "3    Link to video: https://youtu.be/j6QPZp--lJE\\n ...  \n",
       "4    Hi all I'm a 16 year old from Adelaide Austral...  \n",
       "5    Hey entrepreneurs - I recently came up with an...  \n",
       "..                                                 ...  \n",
       "993  G'day guys,  \\nBack again with the second part...  \n",
       "994  Link to video: https://youtu.be/wdQRQ82AED8\\n ...  \n",
       "995  Have been seeing a lot lately from startups tr...  \n",
       "996  This is the best way to earn money from HOme\\n...  \n",
       "997  I currently work in law enforcement and am pre...  \n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title and Posts\n",
    "entre_df[['title', 'selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the title and post together using pd.concat\n",
    "entre_df_merged = pd.concat([entre_df['title'], entre_df['selftext']], axis=0)\n",
    "entre_df_merged = pd.DataFrame(entre_df_merged, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Being driven individuals, we are all rushing t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How you can reduce bounce rate from your webpage?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made an animated summary of \"The lean Start ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skate ramp business</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Help with getting a textile prototype created.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>G'day guys,  \\nBack again with the second part...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Link to video: https://youtu.be/wdQRQ82AED8\\n ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Have been seeing a lot lately from startups tr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>This is the best way to earn money from HOme\\n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I currently work in law enforcement and am pre...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "1    Being driven individuals, we are all rushing t...    1.0\n",
       "2    How you can reduce bounce rate from your webpage?    1.0\n",
       "3    I made an animated summary of \"The lean Start ...    1.0\n",
       "4                                  Skate ramp business    1.0\n",
       "5       Help with getting a textile prototype created.    1.0\n",
       "..                                                 ...    ...\n",
       "993  G'day guys,  \\nBack again with the second part...    1.0\n",
       "994  Link to video: https://youtu.be/wdQRQ82AED8\\n ...    1.0\n",
       "995  Have been seeing a lot lately from startups tr...    1.0\n",
       "996  This is the best way to earn money from HOme\\n...    1.0\n",
       "997  I currently work in law enforcement and am pre...    1.0\n",
       "\n",
       "[1994 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the labels to the dataframe, filled with ones\n",
    "entre_df_merged['label'] = np.ones(len(entre_df_merged))\n",
    "entre_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to clean the title and the post\n",
    "def clean_post(df):\n",
    "    \"\"\"\n",
    "    This function removes the unnecessary characters, punctuations, removes stop words and lemmantizes the words\n",
    "    from the posts and titles. Lemmantization is used as I want to preserve the meaning of the words in which it'll compare the words against a dictionary.\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    \n",
    "    # Stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for post in df:\n",
    "        # Lowercase the text\n",
    "        post = post.lower()\n",
    "\n",
    "        # Find the https websites and removes them\n",
    "        post = re.sub('\\(https:.*?\\)','',post)\n",
    "\n",
    "        # Removes youtube links\n",
    "        post = re.sub('https:.*?\\\\n','',post)\n",
    "\n",
    "        # Removes uncaptured url links at the bottom of the text\n",
    "        post = re.sub('https.*?[\\\\n|\"]','',post)\n",
    "\n",
    "        # Removes characters: \\n\\n&amp;#x200B;\n",
    "        post = re.sub('\\\\n\\\\n&amp;#x200b;\\\\n\\\\n','',post)\n",
    "\n",
    "        # Removing the special characters, like punctuation marks, periods\n",
    "        # post = re.sub(r'[^\\w]',' ',post)\n",
    "        \n",
    "        # Removes digits and keeps the letters\n",
    "        post = re.sub(r'[^a-zA-Z]', ' ', post)\n",
    "\n",
    "        # Removes underscores\n",
    "        post = re.sub(' _', ' ',post)\n",
    "\n",
    "        # Removes addtional white spaces\n",
    "        post = re.sub(' +', ' ',post)\n",
    "        \n",
    "        # Removes words that have entrepreneur variations\n",
    "        post = post.replace('entrepreneur','').replace('Entrepreneur','')\n",
    "        \n",
    "        # Removes words that have invest variations\n",
    "        post = post.replace('invest','').replace('investor','')\n",
    "        \n",
    "        # Stores the words in a list \n",
    "        lst = [] \n",
    "        \n",
    "        # If the word is not in the stop words then, lemmantize the words\n",
    "        for word in post.split():\n",
    "            if not word in stops:\n",
    "                lst.append(lemmatizer.lemmatize(word))\n",
    "            \n",
    "        new_lst.append(\" \".join(lst))\n",
    "        \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>driven individual rushing towards dream ever s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reduce bounce rate webpage</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>made animated summary lean start eric ries hop...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skate ramp business</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>help getting textile prototype created</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>g day guy back second part disjoined series th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>link video release new video often interested ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>seeing lot lately startup trying figure tik to...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>best way earn money home check video http www ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>currently work law enforcement pretty miserabl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "1    driven individual rushing towards dream ever s...    1.0\n",
       "2                           reduce bounce rate webpage    1.0\n",
       "3    made animated summary lean start eric ries hop...    1.0\n",
       "4                                  skate ramp business    1.0\n",
       "5               help getting textile prototype created    1.0\n",
       "..                                                 ...    ...\n",
       "993  g day guy back second part disjoined series th...    1.0\n",
       "994  link video release new video often interested ...    1.0\n",
       "995  seeing lot lately startup trying figure tik to...    1.0\n",
       "996  best way earn money home check video http www ...    1.0\n",
       "997  currently work law enforcement pretty miserabl...    1.0\n",
       "\n",
       "[1994 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entre_df_merged['text'] = clean_post(entre_df_merged['text'])\n",
    "entre_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>driven individual rushing towards dream ever s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reduce bounce rate webpage</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>made animated summary lean start eric ries hop...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skate ramp business</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>help getting textile prototype created</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>like start product photography service stick l...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>im reading around sub around reddit building b...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>fluffy content goal build arr business right p...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>hi merchant holiday season coming real soon be...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>market</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "1    driven individual rushing towards dream ever s...    1.0\n",
       "2                           reduce bounce rate webpage    1.0\n",
       "3    made animated summary lean start eric ries hop...    1.0\n",
       "4                                  skate ramp business    1.0\n",
       "5               help getting textile prototype created    1.0\n",
       "..                                                 ...    ...\n",
       "592  like start product photography service stick l...    1.0\n",
       "593  im reading around sub around reddit building b...    1.0\n",
       "594  fluffy content goal build arr business right p...    1.0\n",
       "595  hi merchant holiday season coming real soon be...    1.0\n",
       "596                                             market    1.0\n",
       "\n",
       "[1167 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate posts\n",
    "entre_df_merged = entre_df_merged.drop_duplicates(subset=['text'])\n",
    "entre_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "entre_df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for duplicates\n",
    "entre_df_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to cleaned_entre_df to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entre_df_merged.to_csv('./datasets/cleaned_entre_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data from investing sub reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reddit_post(\"https://www.reddit.com/r/investing.json\",40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_df = pd.read_csv('./datasets/investing.csv')\n",
    "invest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data in investing sub reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "invest_df.duplicated(subset='selftext').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "invest_df = invest_df.drop_duplicates(subset=['selftext'])\n",
    "invest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop the first row as it's not relevant, it's ask questions monday\n",
    "invest_df.drop(labels=[0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the title and post together using pd.concat\n",
    "invest_df_merged = pd.concat([invest_df['title'], invest_df['selftext']], axis=0)\n",
    "invest_df_merged = pd.DataFrame(invest_df_merged, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels to the dataframe\n",
    "invest_df_merged['label'] = np.zeros(len(invest_df_merged))\n",
    "invest_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean the post with clean_post function\n",
    "invest_df_merged.text = clean_post(invest_df_merged.text)\n",
    "invest_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "invest_df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "invest_df_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to cleaned_invest_df to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_df_merged.to_csv('./datasets/cleaned_invest_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Entrepreneurship sub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = \" \".join(entre_df_merged.text).split()\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of words\n",
    "word_dict = {}\n",
    "for word in list_of_words:\n",
    "    word_dict[word] = list_of_words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring word\n",
    "max(word_dict, key=word_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the words frequency in descending order\n",
    "sort_words_freq = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_words_freq[:10]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding it into a dataframe\n",
    "entre_freq_df = pd.DataFrame(sort_words_freq, columns=['word','frequency'])\n",
    "entre_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.barplot(x='word', y='frequency', data=entre_freq_df[:10], palette='coolwarm')\n",
    "plt.title('Entrepreneur subreddit: Top 10 most frequently occuring words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that entrepreneur sub reddit is where business minded people share their business ideas, it's not surprising to see the word 'business' coming in top with a count of 800, followed by the word 'people'. This shows that businesses primarily involves social interactions for them to function. There's seems to be a large drop after the word 'business', with the other words having a range of 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of length of text for titles and posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "entre_df_merged = entre_df_merged.reset_index(drop=True)\n",
    "entre_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the titles\n",
    "entre_df_merged_title = pd.DataFrame(entre_df_merged.text.loc[:596])\n",
    "entre_df_merged_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words into a list and then count the number of words\n",
    "entre_df_merged_title['length'] = entre_df_merged_title.text.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entre_df_merged_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.histplot(data=entre_df_merged_title.length, bins=500)\n",
    "\n",
    "plt.title('Distribution of length of title for Entrepreneur subreddit')\n",
    "plt.xlabel('Length of title')\n",
    "plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution shows a heavy right-skewed graph, with most of the titles having a length of 5 and most of the titles fall in the range between 2 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the posts\n",
    "entre_df_merged_post = pd.DataFrame(entre_df_merged.text.loc[597:])\n",
    "entre_df_merged_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words into a list and then count the number of words\n",
    "entre_df_merged_post['length'] = entre_df_merged_post.text.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entre_df_merged_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.histplot(data=entre_df_merged_post.length, bins=500)\n",
    "\n",
    "plt.title('Distribution of length of post for Entrepreneur subreddit')\n",
    "plt.xlabel('Length of post')\n",
    "plt.xlim(0,250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution for the length of the posts for the Entrepreneur subreddit shows a right-tailed skewed graph. Most of the posts have an average of 25 words in their posts and the range is between 25 to 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with time zone\n",
    "entre_df.columns[entre_df.columns.str.contains('utc|time|created')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "entre_df_time = entre_df[['title','selftext','created_utc']].drop_duplicates(subset=['selftext']).drop(['title','selftext'], axis=1)\n",
    "entre_df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to datatime\n",
    "entre_df_time = pd.to_datetime(entre_df_time['created_utc'], unit='s')\n",
    "entre_df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the hours\n",
    "entre_df_time.dt.hour.value_counts().plot(kind='bar', title='Hour of the day posted on entrepreneur subreddit', figsize=(11,7))\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the most popular time to post is 19:00, 15:00, 21:00 18:00. It seems like entrepreneurs like to post in the later afternoon and evening. I'll group up the hourly timings to get a overall picture of the posting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the posters' hours, 0-6, 6-12, 12-18, 18-23.\n",
    "entre_hour = entre_df_time.dt.hour\n",
    "entre_hour_grpby = pd.cut(entre_hour, bins=[-1,6,12,18,23], labels=['Midnight','Morning','Afternoon','Evening'])\n",
    "entre_hour_grpby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the graph of the time of the post\n",
    "entre_hour_grpby.value_counts().plot(kind='bar', title='Time of posting', figsize=(11,7))\n",
    "\n",
    "plt.xlabel('Time of the day')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems most users post during the afternoon and evening and they're least likely to post in the morning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Investing subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = \" \".join(invest_df_merged.text).split()\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of words\n",
    "word_dict = {}\n",
    "for word in list_of_words:\n",
    "    word_dict[word] = list_of_words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring word\n",
    "max(word_dict, key=word_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the words frequency\n",
    "sort_words_freq = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_words_freq[:10]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the frequency of words dictionary into a dataframe\n",
    "invest_freq_df = pd.DataFrame(sort_words_freq, columns=['word','frequency'])\n",
    "invest_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,10))\n",
    "sns.barplot(x='word', y='frequency', data=invest_freq_df[:10], palette='coolwarm')\n",
    "plt.title('Investing subreddit: Top 20 most frequently occuring words')\n",
    "plt.xticks(fontsize=11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequently occuring word for the investing subreddit is 'stock'. Given that this is a subreddit where users share their investing ideas, it's not surprising to see 'stock' coming at the top. The next top word is 'company', in which users most likely ask what type of companies to invest. 'Market' and 'stock' goes together since users are sharing their trading ideas.\n",
    "\n",
    "The 'u' could most likely mean the United States stocks market as it's most likely lemmantized. It's seems the US stocks market is popular on this subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top occuring words for the entrepreneur subreddit\n",
    "entre_freq_df['word'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top occuring words for the investing subreddit\n",
    "invest_freq_df['word'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the top occuring words from their respective subreddits are unique, I can safely assume that the model will not have issues distinguishing the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of length of text for titles and posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "invest_df_merged = invest_df_merged.reset_index(drop=True)\n",
    "invest_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the titles\n",
    "invest_df_merged_title = pd.DataFrame(invest_df_merged.text.loc[:500])\n",
    "invest_df_merged_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words into a list and then count the number of words\n",
    "invest_df_merged_title['length'] = invest_df_merged_title.text.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_df_merged_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.histplot(data=invest_df_merged_title.length, bins=500)\n",
    "\n",
    "plt.title('Distribution of length of title for Investing subreddit')\n",
    "plt.xlabel('Length of title')\n",
    "plt.xlim(0,30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution shows a heavy right-skewed graph, with most of the titles having a length of 5 and most of the titles falling in between the range of 2 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the posts\n",
    "invest_df_merged_post = pd.DataFrame(invest_df_merged.text.loc[500:])\n",
    "invest_df_merged_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words into a list and then count the number of words\n",
    "invest_df_merged_post['length'] = invest_df_merged_post.text.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_df_merged_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.histplot(data=invest_df_merged_post.length, bins=500)\n",
    "\n",
    "plt.title('Distribution of length of post for investing subreddit')\n",
    "plt.xlabel('Length of post')\n",
    "plt.xlim(0,250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution for the length of the posts for the investing subreddit shows a right-tailed skewed graph. Most of the posts have an average of 25 words in their posts and most of the posts fall in the range between 25 to 60 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with time zone\n",
    "invest_df.columns[invest_df.columns.str.contains('utc|time|created')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "invest_df_time = invest_df[['title','selftext','created_utc']].drop_duplicates(subset=['selftext']).drop(['title','selftext'], axis=1)\n",
    "invest_df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to datatime\n",
    "invest_df_time = pd.to_datetime(invest_df_time['created_utc'], unit='s')\n",
    "invest_df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the hours\n",
    "invest_df_time.dt.hour.value_counts().plot(kind='bar', title='Hour of the day posted on investing subreddit', figsize=(11,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the most popular time to post is 18:00, 20:00, 17:00, 19:00. It seems like these traders like to post in the evening. I'll group up the hourly timings to get a overall picture of the posting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the posters' hours, 0-6, 6-12, 12-18, 18-23.\n",
    "invest_hour = invest_df_time.dt.hour\n",
    "invest_hour_grpby = pd.cut(invest_hour, bins=[-1,6,12,18,23], labels=['Midnight','Morning','Afternoon','Evening'])\n",
    "invest_hour_grpby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the graph of the time of the post\n",
    "invest_hour_grpby.value_counts().plot(kind='bar', title='Time of posting for investing subreddit', figsize=(11,7))\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems most users post during the afternoon and evening and they're least likely to post in the morning, similar to the Entrepreneur subbredit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the functions to clean the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes unwanted links and lower case the words\n",
    "test = clean_post(entre_df_merged.text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removes stop words\n",
    "test = remove_stop_words(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemmitizes words\n",
    "test = lemmitizer(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both subreddit dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_invest_df = pd.read_csv('./datasets/cleaned_invest_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_entre_df = pd.read_csv('./datasets/cleaned_entre_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([clean_entre_df,clean_invest_df],axis=0)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A null value found in the dataframe\n",
    "final_df[final_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the null value\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicates found\n",
    "final_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./datasets/final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
