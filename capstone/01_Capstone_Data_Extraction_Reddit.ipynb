{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Topic Modelling on AMD vs Nvidia GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Data Extraction\n",
    "- Data Cleaning\n",
    "- [EDA](#EDA)\n",
    "- [Prepare data for LDA Analysis](#Prepare-data-for-LDA-Analysis)\n",
    "- [LDA Model Training](#LDA-Model-Training)\n",
    "- Model creation\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import re\n",
    "# NLTK Library\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import PRAW package\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Gensim library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Detect non-english words\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Detect non-english words using spacy\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max rows and columns for Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style use\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Data Extraction from reddit using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain from learnpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued from code above\n",
    "\n",
    "for submission in reddit.subreddit(\"learnpython\").hot(limit=30):\n",
    "    print(submission.title)\n",
    "\n",
    "# Output: 10 submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorized Reddit instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit.read_only)  # Output: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_sub = reddit.subreddit(\"cryptocurrency\")\n",
    "\n",
    "print(crypto_sub.display_name)  # output: redditdev\n",
    "print(crypto_sub.title)         # output: reddit development\n",
    "print(crypto_sub.description)   # output: a subreddit for discussion of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain `Submission` Instances from a subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in crypto_sub.hot(limit=10):\n",
    "    print(submission.title)\n",
    "    print(submission.score)\n",
    "    print(submission.id)\n",
    "    print(submission.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(id=\"ktzv3a\")\n",
    "print(submission.title)  # Output: reddit will soon only be available ...\n",
    "\n",
    "# or\n",
    "# submission = reddit.submission(url='https://www.reddit.com/...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain `Comment` Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "top_level_comments = list(submission.comments)\n",
    "all_comments = submission.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(id=\"ktzv3a\")\n",
    "submission.comment_sort = \"new\"\n",
    "top_level_comments = list(submission.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(id=\"39zje0\")\n",
    "# print(submission.title) # to make it non-lazy\n",
    "pprint.pprint(vars(submission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting comments with PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id=\"3g1jfi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "lst = []\n",
    "for top_level_comment in submission.comments:\n",
    "    lst.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the replies of the top comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for top_level_comment in submission.comments:\n",
    "    for second_level_comment in top_level_comment.replies:\n",
    "        print(second_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the second level comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "comment_queue = submission.comments[:]  # Seed with top-level\n",
    "while comment_queue:\n",
    "    comment = comment_queue.pop(0)\n",
    "    print(comment.body)\n",
    "    comment_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the comments from RTX 3080 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the reddit instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper_32\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)  # Output: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission object for rtx 3080 megathread\n",
    "submission = reddit.submission(id=\"itw87x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level comments only extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx_3080_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    rtx_3080_lst.append(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting it to a Series\n",
    "rtx_3080_df = pd.Series(rtx_3080_lst)\n",
    "rtx_3080_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx_3080_df.loc[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rtx 3080 comments to csv file\n",
    "rtx_3080_df.to_csv('./reddit dataset/rtx_3080.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and Second level comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for top_level_comment in submission.comments:\n",
    "    for second_level_comment in top_level_comment.replies:\n",
    "        print(second_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper function to scrap the dataset in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_sub_dict = {'rtx_3060ti': 'k4mctp', 'rtx_3070':'jj8k0l', 'rtx_3080': 'itw87x', 'rtx_3090': 'iyy5sx', 'rtx_3000': 'iko4ir'}\n",
    "gpu_sub_dict_test = {'rtx_3060ti': 'k4mctp'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper_32\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeGPUComment(gpu_dict):\n",
    "    for key, value in gpu_sub_dict.items():\n",
    "        gpu_lst = []\n",
    "        # Creating the submission object for rtx megathreads\n",
    "        submission = reddit.submission(id=value)\n",
    "        \n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            gpu_lst.append(comment.body)\n",
    "        # Converted to Series format\n",
    "        rtx_df = pd.DataFrame({'Reddit comments':gpu_lst})\n",
    "        rtx_df['tag'] = key\n",
    "        rtx_df.to_csv('./reddit dataset/' + key + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeGPUComment(gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Having just weathered the 6800/XT launch, it f...</td>\n",
       "      <td>rtx_3060ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EVGA queue\\n\\nhttps://www.evga.com/products/pr...</td>\n",
       "      <td>rtx_3060ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow guru3d actually commented on the pricing s...</td>\n",
       "      <td>rtx_3060ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like a lot of people here, I have a 500W PSU a...</td>\n",
       "      <td>rtx_3060ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seems like a great card and it seems like the ...</td>\n",
       "      <td>rtx_3060ti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Reddit comments         tag\n",
       "0  Having just weathered the 6800/XT launch, it f...  rtx_3060ti\n",
       "1  EVGA queue\\n\\nhttps://www.evga.com/products/pr...  rtx_3060ti\n",
       "2  Wow guru3d actually commented on the pricing s...  rtx_3060ti\n",
       "3  Like a lot of people here, I have a 500W PSU a...  rtx_3060ti\n",
       "4  Seems like a great card and it seems like the ...  rtx_3060ti"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_df = pd.read_csv('./reddit dataset/rtx_3060ti.csv')\n",
    "gpu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-ab01192883ad>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-ab01192883ad>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    gpu_df.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gpu_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing csv file\n",
    "GPU_df = pd.read_csv('./amazon dataset/gpu_df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the data\n",
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "GPU_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the customer review title and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to clean the title and the post\n",
    "def clean_post(df):\n",
    "    \"\"\"\n",
    "    This function removes the unnecessary characters, punctuations, removes stop words and lemmantizes the words\n",
    "    from the posts and titles. Lemmantization is used as I want to preserve the meaning of the words in which it'll compare the words against a dictionary.\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    \n",
    "    # Stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for post in df:\n",
    "        # Lowercase the text\n",
    "        post = post.lower()\n",
    "\n",
    "        # Find the https websites and removes them\n",
    "        post = re.sub(r'\\(https:.*?\\)','',post)\n",
    "\n",
    "        # Removes youtube links\n",
    "        post = re.sub('https:.*?\\\\n','',post)\n",
    "\n",
    "        # Removes uncaptured url links at the bottom of the text\n",
    "        post = re.sub('https.*?[\\\\n|\"]','',post)\n",
    "\n",
    "        # Removes characters: \\n\\n&amp;#x200B;\n",
    "        post = re.sub('\\\\n\\\\n&amp;#x200b;\\\\n\\\\n','',post)\n",
    "\n",
    "        # Removing the special characters, like punctuation marks, periods\n",
    "        post = re.sub(r'[^\\w]',' ',post)\n",
    "        \n",
    "        # Removes digits and keeps the letters\n",
    "        # post = re.sub(r'[^a-zA-Z]', ' ', post)\n",
    "\n",
    "        # Removes underscores\n",
    "        post = re.sub(' _', ' ',post)\n",
    "\n",
    "        # Removes addtional white spaces\n",
    "        post = re.sub(' +', ' ',post)\n",
    "        \n",
    "        # Stores the words in a list \n",
    "        lst = [] \n",
    "        \n",
    "        # If the word is not in the stop words then, lemmantize the words\n",
    "        for word in post.split():\n",
    "            if not word in stops:\n",
    "                lst.append(lemmatizer.lemmatize(word))\n",
    "            \n",
    "        new_lst.append(\" \".join(lst))\n",
    "        \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df = pd.read_csv('./reddit dataset/rtx_3080.csv')\n",
    "GPU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the Customer Review column\n",
    "GPU_df['Customer Review'] = clean_post(GPU_df['0'])\n",
    "GPU_df['Customer Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df.drop('0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly going through the rows to check if it's cleaned properly \n",
    "GPU_df['Customer Review'].loc[np.random.randint(1707)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the Customer Review Title column\n",
    "GPU_df['Customer Review Title'] = clean_post(GPU_df['Customer Review Title'])\n",
    "GPU_df['Customer Review Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly going through the rows to check if it's cleaned properly \n",
    "GPU_df['Customer Review Title'].loc[np.random.randint(2048)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english words in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non english by creating a helper function\n",
    "from langdetect import detect\n",
    "def isenglish(text):\n",
    "    try:\n",
    "        if nlp(text)._.language.get('language') == 'en':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GPU_df['isenglish'] = GPU_df['Customer Review'].apply(isenglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows with the string deleted in them\n",
    "GPU_df['Customer Review'].map(lambda x: x.count('deleted')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 130 rows were non-english reviews. These have to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df.drop(GPU_df[GPU_df['isenglish'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[['Customer Review Title', 'Customer Review']].loc[GPU_df['Customer Review'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[['Customer Review Title', 'Customer Review']].loc[GPU_df[['Customer Review Title']].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df['Customer Review Title'] == 'far good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicate values doesn't be seem to be actually duplicates, just a few words that were written by the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "GPU_df.to_csv('./amazon dataset/cleaned_gpu_df_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing csv file\n",
    "GPU_df = pd.read_csv('./amazon dataset/cleaned_gpu_df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "GPU_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with null values\n",
    "GPU_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Review title and reviews\n",
    "GPU_df_no_reviews = GPU_df.drop(['Customer Review Title', 'Customer Review'], axis=1)\n",
    "GPU_df_no_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the ids match\n",
    "list(GPU_df_no_reviews.drop_duplicates(['id'])['id'].unique()) == list(GPU_df['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicate values in the GPU_df_no_reviews\n",
    "GPU_df_no_reviews = GPU_df_no_reviews.drop_duplicates(['id'])\n",
    "GPU_df_no_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the id as the index and reset the index\n",
    "GPU_df_no_reviews = GPU_df_no_reviews.set_index('id').reset_index(drop=True)\n",
    "GPU_df_no_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(99,99))\n",
    "sns.displot(GPU_df_no_reviews['Price'], bins=12, aspect=1.5, height=6, color='green')\n",
    "plt.axvline(GPU_df_no_reviews['Price'].mean(),color='red')\n",
    "plt.axvline(GPU_df_no_reviews['Price'].median(),color='yellow')\n",
    "\n",
    "plt.title('Distribution of sale price of GPUs', size=13)\n",
    "plt.legend(['Mean sale price','Median sale price']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution shows a right skewed graph with most of the GPUs falling below the 100 dollars range. The mean and the median prices are far part showing that they are some outliers in the price distribution as seen in the price range of 800 and 1000 dollars range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of AMD and Nvidia Chipsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews['Chipset Brand'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most of the GPUs are under Nvidia with a proportion of 70% while Amd has a proportion of 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular brands by their rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews['Manufacturer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As NVIDIA, NVIDIA Corporation and Althon Micro Inc. have only 1 GPUs, I'll not include them in the popular brand investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer_list = ['AMD','ASRock','Aiposen','SAPPHIRE', 'Althon Micro Inc.', 'NVIDIA']\n",
    "GPU_df_no_reviews.groupby('Manufacturer').mean().drop(manufacturer_list)['Overall Customer Rating'].sort_index(ascending=False).plot(kind='barh', \n",
    "                                                                                                                                            title='Most popular brand by rating', \n",
    "                                                                                                                                            figsize=(11,7), \n",
    "                                                                                                                                            color='green')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Brand', rotation=360);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean().drop(manufacturer_list)['Overall Customer Rating'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without including Nvidia and Althon Micro Inc as they have only 1 type of GPU, Asus, EVGA and SAPPHIRE are the most popular brands given their high ratings.\n",
    "\n",
    "The reason behind this is that consumers usually prefer 3rd party coolers fitted into the GPUs compared to the Nvidia's coolers as they're much more effective in controlling the airflow and decreasing the GPU temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Chipset Brand has a higher customer rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Chipset Brand').mean()['Overall Customer Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia is slightly ahead of AMD in terms of the Overall Customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Manufacturer produces GPUs with higher Memory Speed and Size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean()['Memory Speed(MHz)'].sort_values().plot(kind='barh', figsize=(11,7))\n",
    "\n",
    "plt.title('Memory speed of the GPUs produced by individual manufacturers')\n",
    "plt.xlabel('Memory speed (MHz)')\n",
    "plt.ylabel('Manufacturer',rotation=360);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean()['Memory Speed(MHz)'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASRock, Gigabyte, XFX and EVGA manufacturers produces GPUs with high amount of memory speed which shows that they're premium brands that produce 'Enthusiast Grade' types of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title = \" \".join(GPU_df['Customer Review Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review_title)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the word cloud, it seems that consumers are mostly satisfied with their GPU purchase with 'good', 'great' and 'best' words coming out at the top. The consumers are mostly gamers and most of them play in 1080p resolution and they seem to be price sensitive with the words such as 'bang buck' and 'great value' having a bigger size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title_list = customer_review_title.split()\n",
    "customer_review_title_dict = {}\n",
    "\n",
    "for word in customer_review_title_list:\n",
    "    if word not in customer_review_title_dict.keys():\n",
    "        customer_review_title_dict[word] = customer_review_title_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_title_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'words': customer_review_title_dict.keys(), 'freq': customer_review_title_dict.values()}\n",
    "customer_review_title_df = pd.DataFrame(df)\n",
    "customer_review_title_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_review_title_df['freq'].hist(bins=150)\n",
    "# plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \" \".join(GPU_df['Customer Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the customer review title word cloud, consumers who purchase GPUs tend to be gamers and they play on 1080p resolution. GPU fans are an important factor when making a GPU purchase as the word 'fan' size is rather big. The word 'issue' and 'problem' shows up big which suggests that consumers may have encountered issues with the GPUs they have purchased. The two brands 'amd' and 'nvidia' shows that these 2 are the major players in the GPU market. GPU drivers seem to play an important role in making sure that the GPU is functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_list = customer_review.split()\n",
    "customer_review_dict = {}\n",
    "\n",
    "for word in customer_review_list:\n",
    "    if word not in customer_review_dict.keys():\n",
    "        customer_review_dict[word] = customer_review_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = {'words': customer_review_dict.keys(), 'freq': customer_review_dict.values()}\n",
    "customer_review_df = pd.DataFrame(review_df)\n",
    "customer_review_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LDA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using only Customer Review to conduct the LDA Analysis as it makes up the bulk of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to the customer reviews from series to a list.\n",
    "data = GPU_df['Customer Review'].values.tolist()\n",
    "data[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the first document with up to 30 words in them\n",
    "print(texts[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                      passes=20, random_state=42)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join(os.getcwd()+'\\\\visualization\\\\'+'ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if False:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, os.getcwd()+ '\\\\visualization\\\\' + 'ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
