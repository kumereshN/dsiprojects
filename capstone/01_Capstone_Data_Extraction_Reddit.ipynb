{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Topic Modelling on AMD vs Nvidia GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Data Extraction\n",
    "- Data Cleaning\n",
    "- [EDA](#EDA)\n",
    "- [Prepare data for LDA Analysis](#Prepare-data-for-LDA-Analysis)\n",
    "- [LDA Model Training](#LDA-Model-Training)\n",
    "- Model creation\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import re\n",
    "# NLTK Library\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import PRAW package\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Gensim library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Detect non-english words\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Detect non-english words using spacy\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max rows and columns for Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style use\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper function to scrap the dataset in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with keys as the text and the value being reddit submission id \n",
    "nvidia_gpu_sub_dict = {'rtx_3060ti': 'k4mctp', 'rtx_3070':'jj8k0l', \n",
    "                       'rtx_3080': 'itw87x', 'rtx_3090': 'iyy5sx', 'rtx_3000': 'iko4ir'}\n",
    "amd_gpu_sub_dict = {'amd_gpu': 'iknr7g', 'rx_6000_rdna2':'jjq6v1', 'rx_6000_nov_18':'jvxm8z', 'radeon_rx_6000':'jwesyt'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional nvidia comments scrapped\n",
    "add_nvidia_gpu_sub_dict = {'rtx_3090vs3080vs3070': 'ioyjk6', 'scalper_warning': 'ivi4qm', 'rtx_3090_memory': 'igw0bg',\n",
    "                          'rtx_3080ti_priced': 'jrmtmx', 'rtx_3080_3090_leak': 'ii6179'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper_32\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeGPUComment(gpu_dict):\n",
    "    for key, value in gpu_dict.items():\n",
    "        # Creates the GPU list to hold the reddit comments\n",
    "        gpu_lst = []\n",
    "        # Creating the submission object for rtx megathreads\n",
    "        submission = reddit.submission(id=value)\n",
    "        \n",
    "        # Extract all the commments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            gpu_lst.append(comment.body)\n",
    "        # Converted to Dataframe format\n",
    "        rtx_df = pd.DataFrame({'Reddit comments':gpu_lst})\n",
    "        # Creating the column for tags, the keys of the dictionary will be stored here\n",
    "        rtx_df['tag'] = key\n",
    "        # Export the individual submissions to csv file\n",
    "        rtx_df.to_csv('./reddit dataset/' + key + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of nvidia commments\n",
    "# scrapeGPUComment(nvidia_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of amd commments\n",
    "# scrapeGPUComment(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of additional nvidia commments\n",
    "# scrapeGPUComment(add_nvidia_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframes\n",
    "\n",
    "# Nvidia's comments\n",
    "rtx_3000 = pd.read_csv('./reddit dataset/rtx_3000.csv')\n",
    "rtx_3060ti = pd.read_csv('./reddit dataset/rtx_3060ti.csv')\n",
    "rtx_3070 = pd.read_csv('./reddit dataset/rtx_3070.csv')\n",
    "rtx_3080 = pd.read_csv('./reddit dataset/rtx_3080.csv')\n",
    "rtx_3090 = pd.read_csv('./reddit dataset/rtx_3090.csv')\n",
    "\n",
    "# Amd's comments\n",
    "amd_gpu = pd.read_csv('./reddit dataset/amd_gpu.csv')\n",
    "rx_6000 = pd.read_csv('./reddit dataset/radeon_rx_6000.csv')\n",
    "rx_6000_nov_18 = pd.read_csv('./reddit dataset/rx_6000_nov_18.csv')\n",
    "rx_6000_rdna2 = pd.read_csv('./reddit dataset/rx_6000_rdna2.csv')\n",
    "\n",
    "# Additional Nvidia's comments scrapped\n",
    "rtx_3090vs3080vs3070 = pd.read_csv('./reddit dataset/rtx_3090vs3080vs3070.csv')\n",
    "scalper_warning = pd.read_csv('./reddit dataset/scalper_warning.csv')\n",
    "rtx_3090_memory = pd.read_csv('./reddit dataset/rtx_3090_memory.csv')\n",
    "rtx_3080ti_priced = pd.read_csv('./reddit dataset/rtx_3080ti_priced.csv')\n",
    "rtx_3080_3090_leak = pd.read_csv('./reddit dataset/rtx_3080_3090_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dataframes by their rows\n",
    "combined_df = pd.concat([rtx_3000, rtx_3060ti, rtx_3070, rtx_3080,\n",
    "                        rtx_3090, amd_gpu, rx_6000, rx_6000_nov_18, rx_6000_rdna2,\n",
    "                        rtx_3090vs3080vs3070, scalper_warning, rtx_3090_memory, rtx_3080ti_priced, rtx_3080_3090_leak],\n",
    "                       axis=0, ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34534, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the data\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    3\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "combined_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    0\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments with `[deleted]` and `[removed]` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34531, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  46,   56,   57,   82,   83,  118,  150,  157,  160,  244,\n",
       "            ...\n",
       "            2240, 2321, 2343, 2348, 2350, 2364, 2407, 2422, 2425, 2436],\n",
       "           dtype='int64', length=778)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_deleted_comments_idx = combined_df[(combined_df['Reddit comments'] == '[removed]') | (combined_df['Reddit comments'] == '[deleted]')].index\n",
    "removed_deleted_comments_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 778 comments will be dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the deleted and removed comments\n",
    "combined_df.drop(removed_deleted_comments_idx, axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28809, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to clean the title and the post\n",
    "def clean_post(df):\n",
    "    \"\"\"\n",
    "    This function removes the unnecessary characters, punctuations, removes stop words and lemmantizes the words\n",
    "    from the posts and titles. Lemmantization is used as I want to preserve the meaning of the words in which it'll compare the words against a dictionary.\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    \n",
    "    # Stop words\n",
    "    stops = stopwords.words('english')\n",
    "    # Adding additional stop words\n",
    "    stops.extend(['http', 'www'])\n",
    "    # stops.extend(['nvidia', 'amd', 'card', 'gpu', 'http', 'www'])\n",
    "    stops = set(stops)\n",
    "    \n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for post in df:\n",
    "        # Lowercase the text\n",
    "        post = post.lower()\n",
    "\n",
    "        # Find the https websites and removes them\n",
    "        post = re.sub(r'\\(https:.*?\\)','',post)\n",
    "\n",
    "        # Removes youtube links\n",
    "        post = re.sub('https:.*?\\\\n','',post)\n",
    "\n",
    "        # Removes uncaptured url links at the bottom of the text\n",
    "        post = re.sub('https.*?[\\\\n|\"]','',post)\n",
    "\n",
    "        # Removes characters: \\n\\n&amp;#x200B;\n",
    "        post = re.sub('\\\\n\\\\n&amp;#x200b;\\\\n\\\\n','',post)\n",
    "\n",
    "        # Removing the special characters, like punctuation marks, periods\n",
    "        post = re.sub(r'[^\\w]',' ',post)\n",
    "        \n",
    "        # Removes digits and keeps the letters\n",
    "        # post = re.sub(r'[^a-zA-Z]', ' ', post)\n",
    "\n",
    "        # Removes underscores\n",
    "        post = re.sub(' _', ' ',post)\n",
    "\n",
    "        # Removes addtional white spaces\n",
    "        post = re.sub(' +', ' ',post)\n",
    "        \n",
    "        # Stores the words in a list \n",
    "        lst = [lemmatizer.lemmatize(word) for word in post.split() if word not in stops]\n",
    "            \n",
    "        new_lst.append(\" \".join(lst))\n",
    "        \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     pre order time releasing 17th seems\n",
       "2                          going hard grab card 3080 17th\n",
       "4       uk price 3090 1399 3080 649 3070 469 scan aib ...\n",
       "5       badly want 3080 especially price still concern...\n",
       "6       nobody talking many spatula jensen whole pot full\n",
       "                              ...                        \n",
       "2433             even motherboard say pcie 3 0 compatible\n",
       "2434    thanks understanding know course none 3000s dv...\n",
       "2435                                         buy 1400 gpu\n",
       "2437                          lol know ill wait 3070 3060\n",
       "2438    generational compatibility either also compati...\n",
       "Name: Reddit comments, Length: 28809, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleans the Reddit comments column\n",
    "combined_df['Reddit comments'] = clean_post(combined_df['Reddit comments'])\n",
    "combined_df['Reddit comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments that are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28809</td>\n",
       "      <td>28809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>27985</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>150</td>\n",
       "      <td>9869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reddit comments            tag\n",
       "count            28809          28809\n",
       "unique           27985             14\n",
       "top                     rx_6000_rdna2\n",
       "freq               150           9869"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 150 empty comments that have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_comments_idx = combined_df[combined_df['Reddit comments'] == ''].index\n",
    "combined_df.drop(empty_comments_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty comments have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tags with either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rtx_3000', 'rtx_3060ti', 'rtx_3070', 'rtx_3080', 'rtx_3090',\n",
       "       'amd_gpu', 'radeon_rx_6000', 'rx_6000_nov_18', 'rx_6000_rdna2',\n",
       "       'rtx_3090vs3080vs3070', 'scalper_warning', 'rtx_3090_memory',\n",
       "       'rtx_3080ti_priced', 'rtx_3080_3090_leak'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags need to be renamed to either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_tag_lst = list(nvidia_gpu_sub_dict) + list(add_nvidia_gpu_sub_dict)\n",
    "amd_tag_lst = list(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rtx_3060ti', 'rtx_3070', 'rtx_3080', 'rtx_3090', 'rtx_3000', 'rtx_3090vs3080vs3070', 'scalper_warning', 'rtx_3090_memory', 'rtx_3080ti_priced', 'rtx_3080_3090_leak']\n",
      "['amd_gpu', 'rx_6000_rdna2', 'rx_6000_nov_18', 'radeon_rx_6000']\n"
     ]
    }
   ],
   "source": [
    "print(nvidia_tag_lst)\n",
    "print(amd_tag_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['tag'].replace(nvidia_tag_lst, ['nvidia' for _ in range(len(nvidia_tag_lst))], inplace = True)\n",
    "combined_df['tag'].replace(amd_tag_lst, ['amd' for _ in range(len(amd_tag_lst))], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amd       15837\n",
       "nvidia    12085\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>pre order</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>pre order</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>http evga com article 01434 evga geforce rtx 3...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>scared new mining performance card new memory ...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>3060</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>case</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>one</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>think</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>good bot</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>good bot</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>919 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Reddit comments     tag\n",
       "58                                            pre order  nvidia\n",
       "130                                           pre order  nvidia\n",
       "159   http evga com article 01434 evga geforce rtx 3...  nvidia\n",
       "408   scared new mining performance card new memory ...  nvidia\n",
       "450                                                3060  nvidia\n",
       "...                                                 ...     ...\n",
       "2131                                               case  nvidia\n",
       "2143                                                one  nvidia\n",
       "2261                                              think  nvidia\n",
       "2405                                           good bot  nvidia\n",
       "2406                                           good bot  nvidia\n",
       "\n",
       "[919 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.duplicated(subset='Reddit comments', keep = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 941 comments that are duplicated and have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27922, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(subset='Reddit comments', keep = False, ignore_index= True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27003, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicated comments have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rip rdna 2'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly going through the rows to check if it's cleaned properly \n",
    "combined_df['Reddit comments'].loc[np.random.randint(1707)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english words in the reviews (Possible to ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non english by creating a helper function\n",
    "from langdetect import detect\n",
    "def isenglish(text):\n",
    "    try:\n",
    "        if nlp(text)._.language.get('language') == 'en':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GPU_df['isenglish'] = GPU_df['Customer Review'].apply(isenglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows with the string deleted in them\n",
    "GPU_df['Customer Review'].map(lambda x: x.count('deleted')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 130 rows were non-english reviews. These have to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the cleaned csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "combined_df.to_csv('./reddit dataset/cleaned_combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Amd comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Nvidia comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre order time releasing 17th seems</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>going hard grab card 3080 17th</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk price 3090 1399 3080 649 3070 469 scan aib ...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>badly want 3080 especially price still concern...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nobody talking many spatula jensen whole pot full</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>even motherboard say pcie 3 0 compatible</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>thanks understanding know course none 3000s dv...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>buy 1400 gpu</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27001</th>\n",
       "      <td>lol know ill wait 3070 3060</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27002</th>\n",
       "      <td>generational compatibility either also compati...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27003 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Reddit comments     tag\n",
       "0                    pre order time releasing 17th seems  nvidia\n",
       "1                         going hard grab card 3080 17th  nvidia\n",
       "2      uk price 3090 1399 3080 649 3070 469 scan aib ...  nvidia\n",
       "3      badly want 3080 especially price still concern...  nvidia\n",
       "4      nobody talking many spatula jensen whole pot full  nvidia\n",
       "...                                                  ...     ...\n",
       "26998           even motherboard say pcie 3 0 compatible  nvidia\n",
       "26999  thanks understanding know course none 3000s dv...  nvidia\n",
       "27000                                       buy 1400 gpu  nvidia\n",
       "27001                        lol know ill wait 3070 3060  nvidia\n",
       "27002  generational compatibility either also compati...  nvidia\n",
       "\n",
       "[27003 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the existing csv file\n",
    "GPU_df = pd.read_csv('./reddit dataset/cleaned_combined_df.csv')\n",
    "GPU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27003</td>\n",
       "      <td>27003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>27003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>lowering 3080</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reddit comments    tag\n",
       "count            27003  27003\n",
       "unique           27003      2\n",
       "top      lowering 3080    amd\n",
       "freq                 1  15188"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "GPU_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicated comments\n",
    "GPU_df.duplicated(subset='Reddit comments', keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of AMD and Nvidia comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df['tag'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(GPU_df['tag'].value_counts(normalize=True) * 100).plot(kind='bar', \n",
    "                                                        title='Distribution of comments',\n",
    "                                                        figsize=(11,7),\n",
    "                                                        xlabel = 'Manufacturer',\n",
    "                                                       ylabel='Frequency',\n",
    "                                                       color = ['red','green'])\n",
    "\n",
    "plt.xticks(np.arange(2), ['AMD', 'Nvidia'],fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, around 53% of the comments were from AMD and the Hardware subreddit and 43% of the comments were from Nvidia's subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title = \" \".join(GPU_df['Customer Review Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review_title)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the word cloud, it seems that consumers are mostly satisfied with their GPU purchase with 'good', 'great' and 'best' words coming out at the top. The consumers are mostly gamers and most of them play in 1080p resolution and they seem to be price sensitive with the words such as 'bang buck' and 'great value' having a bigger size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title_list = customer_review_title.split()\n",
    "customer_review_title_dict = {}\n",
    "\n",
    "for word in customer_review_title_list:\n",
    "    if word not in customer_review_title_dict.keys():\n",
    "        customer_review_title_dict[word] = customer_review_title_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_title_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'words': customer_review_title_dict.keys(), 'freq': customer_review_title_dict.values()}\n",
    "customer_review_title_df = pd.DataFrame(df)\n",
    "customer_review_title_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_review_title_df['freq'].hist(bins=150)\n",
    "# plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \" \".join(GPU_df['Customer Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the customer review title word cloud, consumers who purchase GPUs tend to be gamers and they play on 1080p resolution. GPU fans are an important factor when making a GPU purchase as the word 'fan' size is rather big. The word 'issue' and 'problem' shows up big which suggests that consumers may have encountered issues with the GPUs they have purchased. The two brands 'amd' and 'nvidia' shows that these 2 are the major players in the GPU market. GPU drivers seem to play an important role in making sure that the GPU is functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_list = customer_review.split()\n",
    "customer_review_dict = {}\n",
    "\n",
    "for word in customer_review_list:\n",
    "    if word not in customer_review_dict.keys():\n",
    "        customer_review_dict[word] = customer_review_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = {'words': customer_review_dict.keys(), 'freq': customer_review_dict.values()}\n",
    "customer_review_df = pd.DataFrame(review_df)\n",
    "customer_review_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LDA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using only Customer Review to conduct the LDA Analysis as it makes up the bulk of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to the customer reviews from series to a list.\n",
    "data = GPU_df['Customer Review'].values.tolist()\n",
    "data[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the first document with up to 30 words in them\n",
    "print(texts[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                      passes=20, random_state=42)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join(os.getcwd()+'\\\\visualization\\\\'+'ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if False:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, os.getcwd()+ '\\\\visualization\\\\' + 'ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
