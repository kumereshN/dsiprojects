{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Topic Modelling on AMD vs Nvidia GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Data Extraction\n",
    "- Data Cleaning\n",
    "- [EDA](#EDA)\n",
    "- [Prepare data for LDA Analysis](#Prepare-data-for-LDA-Analysis)\n",
    "- [LDA Model Training](#LDA-Model-Training)\n",
    "- Model creation\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import re\n",
    "# NLTK Library\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import PRAW package\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Gensim library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Detect non-english words\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Detect non-english words using spacy\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max rows and columns for Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style use\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper function to scrap the dataset in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_gpu_sub_dict = {'rtx_3060ti': 'k4mctp', 'rtx_3070':'jj8k0l', 'rtx_3080': 'itw87x', 'rtx_3090': 'iyy5sx', 'rtx_3000': 'iko4ir'}\n",
    "amd_gpu_sub_dict = {'amd_gpu': 'iknr7g', 'rx_6000_rdna2':'jjq6v1', 'rx_6000_nov_18':'jvxm8z', 'radeon_rx_6000':'jwesyt'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper_32\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeGPUComment(gpu_dict):\n",
    "    for key, value in gpu_dict.items():\n",
    "        # Creates the GPU list to hold the reddit comments\n",
    "        gpu_lst = []\n",
    "        # Creating the submission object for rtx megathreads\n",
    "        submission = reddit.submission(id=value)\n",
    "        \n",
    "        # Extract all the commments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            gpu_lst.append(comment.body)\n",
    "        # Converted to Dataframe format\n",
    "        rtx_df = pd.DataFrame({'Reddit comments':gpu_lst})\n",
    "        rtx_df['tag'] = key\n",
    "        rtx_df.to_csv('./reddit dataset/' + key + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of nvidia commments\n",
    "# scrapeGPUComment(nvidia_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of amd commments\n",
    "# scrapeGPUComment(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframes\n",
    "\n",
    "# Nvidia's comments\n",
    "rtx_3000 = pd.read_csv('./reddit dataset/rtx_3000.csv')\n",
    "rtx_3060ti = pd.read_csv('./reddit dataset/rtx_3060ti.csv')\n",
    "rtx_3070 = pd.read_csv('./reddit dataset/rtx_3070.csv')\n",
    "rtx_3080 = pd.read_csv('./reddit dataset/rtx_3080.csv')\n",
    "rtx_3090 = pd.read_csv('./reddit dataset/rtx_3090.csv')\n",
    "\n",
    "# Amd's comments\n",
    "amd_gpu = pd.read_csv('./reddit dataset/amd_gpu.csv')\n",
    "rx_6000 = pd.read_csv('./reddit dataset/radeon_rx_6000.csv')\n",
    "rx_6000_nov_18 = pd.read_csv('./reddit dataset/rx_6000_nov_18.csv')\n",
    "rx_6000_rdna2 = pd.read_csv('./reddit dataset/rx_6000_rdna2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dataframes by their rows\n",
    "combined_df = pd.concat([rtx_3000, rtx_3060ti, rtx_3070, rtx_3080,\n",
    "                        rtx_3090, amd_gpu, rx_6000, rx_6000_nov_18, rx_6000_rdna2],\n",
    "                       axis=0, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25093, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25093, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the data\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    3\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "combined_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    0\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments with `[deleted]` and `[removed]` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25090, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   46,    56,    57,    82,    83,   118,   150,   157,   160,\n",
       "              244,\n",
       "            ...\n",
       "            10580, 10582, 10584, 10586, 10587, 10588, 10590, 10591, 10592,\n",
       "            10594],\n",
       "           dtype='int64', length=530)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_deleted_comments_idx = combined_df[(combined_df['Reddit comments'] == '[removed]') | (combined_df['Reddit comments'] == '[deleted]')].index\n",
    "removed_deleted_comments_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 530 comments will be dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the deleted and removed comments\n",
    "combined_df.drop(removed_deleted_comments_idx, axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22770, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to clean the title and the post\n",
    "def clean_post(df):\n",
    "    \"\"\"\n",
    "    This function removes the unnecessary characters, punctuations, removes stop words and lemmantizes the words\n",
    "    from the posts and titles. Lemmantization is used as I want to preserve the meaning of the words in which it'll compare the words against a dictionary.\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    \n",
    "    # Stop words\n",
    "    stops = stopwords.words('english')\n",
    "    # Addin additional stop words\n",
    "    stops.extend(['nvidia', 'amd', 'card', 'gpu', 'http', 'www'])\n",
    "    stops = set(stops)\n",
    "    \n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for post in df:\n",
    "        # Lowercase the text\n",
    "        post = post.lower()\n",
    "\n",
    "        # Find the https websites and removes them\n",
    "        post = re.sub(r'\\(https:.*?\\)','',post)\n",
    "\n",
    "        # Removes youtube links\n",
    "        post = re.sub('https:.*?\\\\n','',post)\n",
    "\n",
    "        # Removes uncaptured url links at the bottom of the text\n",
    "        post = re.sub('https.*?[\\\\n|\"]','',post)\n",
    "\n",
    "        # Removes characters: \\n\\n&amp;#x200B;\n",
    "        post = re.sub('\\\\n\\\\n&amp;#x200b;\\\\n\\\\n','',post)\n",
    "\n",
    "        # Removing the special characters, like punctuation marks, periods\n",
    "        post = re.sub(r'[^\\w]',' ',post)\n",
    "        \n",
    "        # Removes digits and keeps the letters\n",
    "        # post = re.sub(r'[^a-zA-Z]', ' ', post)\n",
    "\n",
    "        # Removes underscores\n",
    "        post = re.sub(' _', ' ',post)\n",
    "\n",
    "        # Removes addtional white spaces\n",
    "        post = re.sub(' +', ' ',post)\n",
    "        \n",
    "        # Stores the words in a list \n",
    "        lst = [] \n",
    "        \n",
    "        # If the word is not in the stop words then, lemmantize the words\n",
    "        #for word in post.split():\n",
    "        #    if not word in stops:\n",
    "        #        lst.append(lemmatizer.lemmatize(word))\n",
    "        lst = [lemmatizer.lemmatize(word) for word in post.split() if word not in stops]\n",
    "            \n",
    "        new_lst.append(\" \".join(lst))\n",
    "        \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      pre order time releasing 17th seems\n",
       "1        price revelation insane expect magic rdna 2 wa...\n",
       "2                                going hard grab 3080 17th\n",
       "3        3080 seems like best high end right 1440 144hz...\n",
       "4        uk price 3090 1399 3080 649 3070 469 scan aib ...\n",
       "                               ...                        \n",
       "10583    probably score higher english proficiency test...\n",
       "10585    prove know concept cost opportunity efficiency...\n",
       "10589    cost opportunity perfectly ok way say many nam...\n",
       "10593                                       take care babe\n",
       "10595                                       take care babe\n",
       "Name: Reddit comments, Length: 22770, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleans the Reddit comments column\n",
    "combined_df['Reddit comments'] = clean_post(combined_df['Reddit comments'])\n",
    "combined_df['Reddit comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments that are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22770</td>\n",
       "      <td>22770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>22057</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>125</td>\n",
       "      <td>10081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reddit comments            tag\n",
       "count            22770          22770\n",
       "unique           22057              9\n",
       "top                     rx_6000_rdna2\n",
       "freq               125          10081"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 125 empty comments that have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_comments_idx = combined_df[combined_df['Reddit comments'] == ''].index\n",
    "combined_df.drop(empty_comments_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty comments have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 822 comments that are duplicated and have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>preorder</td>\n",
       "      <td>rtx_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>pre order</td>\n",
       "      <td>rtx_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>pre order</td>\n",
       "      <td>rtx_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>http evga com article 01434 evga geforce rtx 3...</td>\n",
       "      <td>rtx_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>scared new mining performance card new memory ...</td>\n",
       "      <td>rtx_3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10506</th>\n",
       "      <td>see</td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10554</th>\n",
       "      <td>thanks</td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>problem</td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10593</th>\n",
       "      <td>take care babe</td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10595</th>\n",
       "      <td>take care babe</td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Reddit comments            tag\n",
       "26                                              preorder       rtx_3000\n",
       "58                                             pre order       rtx_3000\n",
       "130                                            pre order       rtx_3000\n",
       "159    http evga com article 01434 evga geforce rtx 3...       rtx_3000\n",
       "408    scared new mining performance card new memory ...       rtx_3000\n",
       "...                                                  ...            ...\n",
       "10506                                                see  rx_6000_rdna2\n",
       "10554                                             thanks  rx_6000_rdna2\n",
       "10556                                            problem  rx_6000_rdna2\n",
       "10593                                     take care babe  rx_6000_rdna2\n",
       "10595                                     take care babe  rx_6000_rdna2\n",
       "\n",
       "[822 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.duplicated(subset='Reddit comments', keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22353, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(subset='Reddit comments', keep = False, ignore_index= True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21531, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicated comments have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tags with either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rtx_3000', 'rtx_3060ti', 'rtx_3070', 'rtx_3080', 'rtx_3090',\n",
       "       'amd_gpu', 'radeon_rx_6000', 'rx_6000_nov_18', 'rx_6000_rdna2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags need to be renamed to either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_tag_lst = list(nvidia_gpu_sub_dict)\n",
    "amd_tag_lst = list(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['tag'].replace(nvidia_tag_lst, ['nvidia' for _ in range(len(nvidia_tag_lst))], inplace = True)\n",
    "combined_df['tag'].replace(amd_tag_lst, ['amd' for _ in range(len(amd_tag_lst))], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amd       16016\n",
       "nvidia     5515\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'confused successor 2080 ti 3090 insanely powerful seems like titan people equipment take advantage full capability also much expensive 2080 ti launch jensen called 3080 flagship typically best best flagship'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly going through the rows to check if it's cleaned properly \n",
    "combined_df['Reddit comments'].loc[np.random.randint(1707)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english words in the reviews (Possible to ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non english by creating a helper function\n",
    "from langdetect import detect\n",
    "def isenglish(text):\n",
    "    try:\n",
    "        if nlp(text)._.language.get('language') == 'en':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GPU_df['isenglish'] = GPU_df['Customer Review'].apply(isenglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows with the string deleted in them\n",
    "GPU_df['Customer Review'].map(lambda x: x.count('deleted')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 130 rows were non-english reviews. These have to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the cleaned csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "combined_df.to_csv('./reddit dataset/cleaned_combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre order time releasing 17th seems</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price revelation insane expect magic rdna 2 wa...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>going hard grab 3080 17th</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3080 seems like best high end right 1440 144hz...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uk price 3090 1399 3080 649 3070 469 scan aib ...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21526</th>\n",
       "      <td>dude really understand example confused get sa...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21527</th>\n",
       "      <td>still know example like still thought looked</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21528</th>\n",
       "      <td>probably score higher english proficiency test...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21529</th>\n",
       "      <td>prove know concept cost opportunity efficiency...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21530</th>\n",
       "      <td>cost opportunity perfectly ok way say many nam...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21531 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Reddit comments     tag\n",
       "0                    pre order time releasing 17th seems  nvidia\n",
       "1      price revelation insane expect magic rdna 2 wa...  nvidia\n",
       "2                              going hard grab 3080 17th  nvidia\n",
       "3      3080 seems like best high end right 1440 144hz...  nvidia\n",
       "4      uk price 3090 1399 3080 649 3070 469 scan aib ...  nvidia\n",
       "...                                                  ...     ...\n",
       "21526  dude really understand example confused get sa...     amd\n",
       "21527       still know example like still thought looked     amd\n",
       "21528  probably score higher english proficiency test...     amd\n",
       "21529  prove know concept cost opportunity efficiency...     amd\n",
       "21530  cost opportunity perfectly ok way say many nam...     amd\n",
       "\n",
       "[21531 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the existing csv file\n",
    "GPU_df = pd.read_csv('./reddit dataset/cleaned_combined_df.csv')\n",
    "GPU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21531</td>\n",
       "      <td>21531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>21531</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>okay finally able get store phone 0 fulfilled ...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>16016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Reddit comments    tag\n",
       "count                                               21531  21531\n",
       "unique                                              21531      2\n",
       "top     okay finally able get store phone 0 fulfilled ...    amd\n",
       "freq                                                    1  16016"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21531, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    0\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "GPU_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for duplicated comments\n",
    "GPU_df.duplicated(subset='Reddit comments', keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(99,99))\n",
    "sns.displot(GPU_df_no_reviews['Price'], bins=12, aspect=1.5, height=6, color='green')\n",
    "plt.axvline(GPU_df_no_reviews['Price'].mean(),color='red')\n",
    "plt.axvline(GPU_df_no_reviews['Price'].median(),color='yellow')\n",
    "\n",
    "plt.title('Distribution of sale price of GPUs', size=13)\n",
    "plt.legend(['Mean sale price','Median sale price']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution shows a right skewed graph with most of the GPUs falling below the 100 dollars range. The mean and the median prices are far part showing that they are some outliers in the price distribution as seen in the price range of 800 and 1000 dollars range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of AMD and Nvidia Chipsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews['Chipset Brand'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most of the GPUs are under Nvidia with a proportion of 70% while Amd has a proportion of 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular brands by their rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews['Manufacturer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As NVIDIA, NVIDIA Corporation and Althon Micro Inc. have only 1 GPUs, I'll not include them in the popular brand investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer_list = ['AMD','ASRock','Aiposen','SAPPHIRE', 'Althon Micro Inc.', 'NVIDIA']\n",
    "GPU_df_no_reviews.groupby('Manufacturer').mean().drop(manufacturer_list)['Overall Customer Rating'].sort_index(ascending=False).plot(kind='barh', \n",
    "                                                                                                                                            title='Most popular brand by rating', \n",
    "                                                                                                                                            figsize=(11,7), \n",
    "                                                                                                                                            color='green')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Brand', rotation=360);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean().drop(manufacturer_list)['Overall Customer Rating'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without including Nvidia and Althon Micro Inc as they have only 1 type of GPU, Asus, EVGA and SAPPHIRE are the most popular brands given their high ratings.\n",
    "\n",
    "The reason behind this is that consumers usually prefer 3rd party coolers fitted into the GPUs compared to the Nvidia's coolers as they're much more effective in controlling the airflow and decreasing the GPU temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Chipset Brand has a higher customer rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Chipset Brand').mean()['Overall Customer Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia is slightly ahead of AMD in terms of the Overall Customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Manufacturer produces GPUs with higher Memory Speed and Size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean()['Memory Speed(MHz)'].sort_values().plot(kind='barh', figsize=(11,7))\n",
    "\n",
    "plt.title('Memory speed of the GPUs produced by individual manufacturers')\n",
    "plt.xlabel('Memory speed (MHz)')\n",
    "plt.ylabel('Manufacturer',rotation=360);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df_no_reviews.groupby('Manufacturer').mean()['Memory Speed(MHz)'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASRock, Gigabyte, XFX and EVGA manufacturers produces GPUs with high amount of memory speed which shows that they're premium brands that produce 'Enthusiast Grade' types of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title = \" \".join(GPU_df['Customer Review Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review_title)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the word cloud, it seems that consumers are mostly satisfied with their GPU purchase with 'good', 'great' and 'best' words coming out at the top. The consumers are mostly gamers and most of them play in 1080p resolution and they seem to be price sensitive with the words such as 'bang buck' and 'great value' having a bigger size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title_list = customer_review_title.split()\n",
    "customer_review_title_dict = {}\n",
    "\n",
    "for word in customer_review_title_list:\n",
    "    if word not in customer_review_title_dict.keys():\n",
    "        customer_review_title_dict[word] = customer_review_title_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_title_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'words': customer_review_title_dict.keys(), 'freq': customer_review_title_dict.values()}\n",
    "customer_review_title_df = pd.DataFrame(df)\n",
    "customer_review_title_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_review_title_df['freq'].hist(bins=150)\n",
    "# plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \" \".join(GPU_df['Customer Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the customer review title word cloud, consumers who purchase GPUs tend to be gamers and they play on 1080p resolution. GPU fans are an important factor when making a GPU purchase as the word 'fan' size is rather big. The word 'issue' and 'problem' shows up big which suggests that consumers may have encountered issues with the GPUs they have purchased. The two brands 'amd' and 'nvidia' shows that these 2 are the major players in the GPU market. GPU drivers seem to play an important role in making sure that the GPU is functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_list = customer_review.split()\n",
    "customer_review_dict = {}\n",
    "\n",
    "for word in customer_review_list:\n",
    "    if word not in customer_review_dict.keys():\n",
    "        customer_review_dict[word] = customer_review_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = {'words': customer_review_dict.keys(), 'freq': customer_review_dict.values()}\n",
    "customer_review_df = pd.DataFrame(review_df)\n",
    "customer_review_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LDA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using only Customer Review to conduct the LDA Analysis as it makes up the bulk of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to the customer reviews from series to a list.\n",
    "data = GPU_df['Customer Review'].values.tolist()\n",
    "data[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the first document with up to 30 words in them\n",
    "print(texts[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                      passes=20, random_state=42)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join(os.getcwd()+'\\\\visualization\\\\'+'ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if False:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, os.getcwd()+ '\\\\visualization\\\\' + 'ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
