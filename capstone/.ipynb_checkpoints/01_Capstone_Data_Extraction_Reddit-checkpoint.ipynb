{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Topic Modelling on AMD vs Nvidia GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Data Extraction\n",
    "- Data Cleaning\n",
    "- [EDA](#EDA)\n",
    "- [Prepare data for LDA Analysis](#Prepare-data-for-LDA-Analysis)\n",
    "- [LDA Model Training](#LDA-Model-Training)\n",
    "- Model creation\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import re\n",
    "# NLTK Library\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import PRAW package\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Gensim library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Detect non-english words\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Detect non-english words using spacy\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max rows and columns for Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style use\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper function to scrap the dataset in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with keys as the text and the value being reddit submission id \n",
    "nvidia_gpu_sub_dict = {'rtx_3060ti': 'k4mctp', 'rtx_3070':'jj8k0l', \n",
    "                       'rtx_3080': 'itw87x', 'rtx_3090': 'iyy5sx', 'rtx_3000': 'iko4ir'}\n",
    "amd_gpu_sub_dict = {'amd_gpu': 'iknr7g', 'rx_6000_rdna2':'jjq6v1', 'rx_6000_nov_18':'jvxm8z', 'radeon_rx_6000':'jwesyt'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional nvidia comments scrapped\n",
    "add_nvidia_gpu_sub_dict = {'rtx_3090vs3080vs3070': 'ioyjk6', 'scalper_warning': 'ivi4qm', 'rtx_3090_memory': 'igw0bg',\n",
    "                          'rtx_3080ti_priced': 'jrmtmx', 'rtx_3080_3090_leak': 'ii6179'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit = praw.Reddit(\n",
    "     client_id=\"IR7Y4cUBrVAbGg\",\n",
    "     client_secret=\"podr43kzztn_CoVgtNQiNpDfjI5mjg\",\n",
    "     user_agent=\"gpu_scrapper_32\",\n",
    "     username=\"leader2345\",\n",
    "     password=\"rPLHgrS8\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeGPUComment(gpu_dict):\n",
    "    for key, value in gpu_dict.items():\n",
    "        # Creates the GPU list to hold the reddit comments\n",
    "        gpu_lst = []\n",
    "        # Creating the submission object for rtx megathreads\n",
    "        submission = reddit.submission(id=value)\n",
    "        \n",
    "        # Extract all the commments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            gpu_lst.append(comment.body)\n",
    "        # Converted to Dataframe format\n",
    "        rtx_df = pd.DataFrame({'Reddit comments':gpu_lst})\n",
    "        rtx_df['tag'] = key\n",
    "        rtx_df.to_csv('./reddit dataset/' + key + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of nvidia commments\n",
    "# scrapeGPUComment(nvidia_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of amd commments\n",
    "# scrapeGPUComment(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping of additional nvidia commments\n",
    "# scrapeGPUComment(add_nvidia_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframes\n",
    "\n",
    "# Nvidia's comments\n",
    "rtx_3000 = pd.read_csv('./reddit dataset/rtx_3000.csv')\n",
    "rtx_3060ti = pd.read_csv('./reddit dataset/rtx_3060ti.csv')\n",
    "rtx_3070 = pd.read_csv('./reddit dataset/rtx_3070.csv')\n",
    "rtx_3080 = pd.read_csv('./reddit dataset/rtx_3080.csv')\n",
    "rtx_3090 = pd.read_csv('./reddit dataset/rtx_3090.csv')\n",
    "\n",
    "# Amd's comments\n",
    "amd_gpu = pd.read_csv('./reddit dataset/amd_gpu.csv')\n",
    "rx_6000 = pd.read_csv('./reddit dataset/radeon_rx_6000.csv')\n",
    "rx_6000_nov_18 = pd.read_csv('./reddit dataset/rx_6000_nov_18.csv')\n",
    "rx_6000_rdna2 = pd.read_csv('./reddit dataset/rx_6000_rdna2.csv')\n",
    "\n",
    "# Additional Nvidia's comments scrapped\n",
    "rtx_3090vs3080vs3070 = pd.read_csv('./reddit dataset/rtx_3090vs3080vs3070.csv')\n",
    "scalper_warning = pd.read_csv('./reddit dataset/scalper_warning.csv')\n",
    "rtx_3090_memory = pd.read_csv('./reddit dataset/rtx_3090_memory.csv')\n",
    "rtx_3080ti_priced = pd.read_csv('./reddit dataset/rtx_3080ti_priced.csv')\n",
    "rtx_3080_3090_leak = pd.read_csv('./reddit dataset/rtx_3080_3090_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dataframes by their rows\n",
    "combined_df = pd.concat([rtx_3000, rtx_3060ti, rtx_3070, rtx_3080,\n",
    "                        rtx_3090, amd_gpu, rx_6000, rx_6000_nov_18, rx_6000_rdna2,\n",
    "                        rtx_3090vs3080vs3070, scalper_warning, rtx_3090_memory, rtx_3080ti_priced, rtx_3080_3090_leak],\n",
    "                       axis=0, ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34534, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the data\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    3\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "combined_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    0\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments with `[deleted]` and `[removed]` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34531, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  46,   56,   57,   82,   83,  118,  150,  157,  160,  244,\n",
       "            ...\n",
       "            2240, 2321, 2343, 2348, 2350, 2364, 2407, 2422, 2425, 2436],\n",
       "           dtype='int64', length=778)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_deleted_comments_idx = combined_df[(combined_df['Reddit comments'] == '[removed]') | (combined_df['Reddit comments'] == '[deleted]')].index\n",
    "removed_deleted_comments_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 778 comments will be dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the deleted and removed comments\n",
    "combined_df.drop(removed_deleted_comments_idx, axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28809, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to clean the title and the post\n",
    "def clean_post(df):\n",
    "    \"\"\"\n",
    "    This function removes the unnecessary characters, punctuations, removes stop words and lemmantizes the words\n",
    "    from the posts and titles. Lemmantization is used as I want to preserve the meaning of the words in which it'll compare the words against a dictionary.\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    \n",
    "    # Stop words\n",
    "    stops = stopwords.words('english')\n",
    "    # Addin additional stop words\n",
    "    stops.extend(['nvidia', 'amd', 'card', 'gpu', 'http', 'www'])\n",
    "    stops = set(stops)\n",
    "    \n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for post in df:\n",
    "        # Lowercase the text\n",
    "        post = post.lower()\n",
    "\n",
    "        # Find the https websites and removes them\n",
    "        post = re.sub(r'\\(https:.*?\\)','',post)\n",
    "\n",
    "        # Removes youtube links\n",
    "        post = re.sub('https:.*?\\\\n','',post)\n",
    "\n",
    "        # Removes uncaptured url links at the bottom of the text\n",
    "        post = re.sub('https.*?[\\\\n|\"]','',post)\n",
    "\n",
    "        # Removes characters: \\n\\n&amp;#x200B;\n",
    "        post = re.sub('\\\\n\\\\n&amp;#x200b;\\\\n\\\\n','',post)\n",
    "\n",
    "        # Removing the special characters, like punctuation marks, periods\n",
    "        post = re.sub(r'[^\\w]',' ',post)\n",
    "        \n",
    "        # Removes digits and keeps the letters\n",
    "        # post = re.sub(r'[^a-zA-Z]', ' ', post)\n",
    "\n",
    "        # Removes underscores\n",
    "        post = re.sub(' _', ' ',post)\n",
    "\n",
    "        # Removes addtional white spaces\n",
    "        post = re.sub(' +', ' ',post)\n",
    "        \n",
    "        # Stores the words in a list \n",
    "        lst = [] \n",
    "        \n",
    "        # If the word is not in the stop words then, lemmantize the words\n",
    "        #for word in post.split():\n",
    "        #    if not word in stops:\n",
    "        #        lst.append(lemmatizer.lemmatize(word))\n",
    "        lst = [lemmatizer.lemmatize(word) for word in post.split() if word not in stops]\n",
    "            \n",
    "        new_lst.append(\" \".join(lst))\n",
    "        \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     pre order time releasing 17th seems\n",
       "2                               going hard grab 3080 17th\n",
       "4       uk price 3090 1399 3080 649 3070 469 scan aib ...\n",
       "5       badly want 3080 especially price still concern...\n",
       "6       nobody talking many spatula jensen whole pot full\n",
       "                              ...                        \n",
       "2433             even motherboard say pcie 3 0 compatible\n",
       "2434    thanks understanding know course none 3000s dv...\n",
       "2435                                             buy 1400\n",
       "2437                          lol know ill wait 3070 3060\n",
       "2438    generational compatibility either also compati...\n",
       "Name: Reddit comments, Length: 28809, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleans the Reddit comments column\n",
    "combined_df['Reddit comments'] = clean_post(combined_df['Reddit comments'])\n",
    "combined_df['Reddit comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping comments that are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28809</td>\n",
       "      <td>28809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>27964</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>rx_6000_rdna2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>156</td>\n",
       "      <td>9869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reddit comments            tag\n",
       "count            28809          28809\n",
       "unique           27964             14\n",
       "top                     rx_6000_rdna2\n",
       "freq               156           9869"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 156 empty comments that have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_comments_idx = combined_df[combined_df['Reddit comments'] == ''].index\n",
    "combined_df.drop(empty_comments_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df['Reddit comments'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty comments have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the tags with either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rtx_3000', 'rtx_3060ti', 'rtx_3070', 'rtx_3080', 'rtx_3090',\n",
       "       'amd_gpu', 'radeon_rx_6000', 'rx_6000_nov_18', 'rx_6000_rdna2',\n",
       "       'rtx_3090vs3080vs3070', 'scalper_warning', 'rtx_3090_memory',\n",
       "       'rtx_3080ti_priced', 'rtx_3080_3090_leak'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags need to be renamed to either `Nvidia` or `Amd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_tag_lst = list(nvidia_gpu_sub_dict) + list(add_nvidia_gpu_sub_dict)\n",
    "amd_tag_lst = list(amd_gpu_sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rtx_3060ti',\n",
       " 'rtx_3070',\n",
       " 'rtx_3080',\n",
       " 'rtx_3090',\n",
       " 'rtx_3000',\n",
       " 'rtx_3090vs3080vs3070',\n",
       " 'scalper_warning',\n",
       " 'rtx_3090_memory',\n",
       " 'rtx_3080ti_priced',\n",
       " 'rtx_3080_3090_leak']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvidia_tag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['tag'].replace(nvidia_tag_lst, ['nvidia' for _ in range(len(nvidia_tag_lst))], inplace = True)\n",
    "combined_df['tag'].replace(amd_tag_lst, ['amd' for _ in range(len(amd_tag_lst))], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amd       15818\n",
       "nvidia    12058\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>pre order</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>pre order</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>http evga com article 01434 evga geforce rtx 3...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>scared new mining performance card new memory ...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>3060</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>case</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>one</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>think</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>good bot</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>good bot</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>941 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Reddit comments     tag\n",
       "58                                            pre order  nvidia\n",
       "130                                           pre order  nvidia\n",
       "159   http evga com article 01434 evga geforce rtx 3...  nvidia\n",
       "408   scared new mining performance card new memory ...  nvidia\n",
       "450                                                3060  nvidia\n",
       "...                                                 ...     ...\n",
       "2131                                               case  nvidia\n",
       "2143                                                one  nvidia\n",
       "2261                                              think  nvidia\n",
       "2405                                           good bot  nvidia\n",
       "2406                                           good bot  nvidia\n",
       "\n",
       "[941 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.duplicated(subset='Reddit comments', keep = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 941 comments that are duplicated and have to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27876, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(subset='Reddit comments', keep = False, ignore_index= True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26935, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.duplicated(subset='Reddit comments', keep = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicated comments have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find information'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly going through the rows to check if it's cleaned properly \n",
    "combined_df['Reddit comments'].loc[np.random.randint(1707)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english words in the reviews (Possible to ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non english by creating a helper function\n",
    "from langdetect import detect\n",
    "def isenglish(text):\n",
    "    try:\n",
    "        if nlp(text)._.language.get('language') == 'en':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GPU_df['isenglish'] = GPU_df['Customer Review'].apply(isenglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows with the string deleted in them\n",
    "GPU_df['Customer Review'].map(lambda x: x.count('deleted')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_df[GPU_df.loc[:,'isenglish'] == 0][['Customer Review']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 130 rows were non-english reviews. These have to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the cleaned csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "combined_df.to_csv('./reddit dataset/cleaned_combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre order time releasing 17th seems</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>going hard grab 3080 17th</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk price 3090 1399 3080 649 3070 469 scan aib ...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>badly want 3080 especially price still concern...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nobody talking many spatula jensen whole pot full</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26930</th>\n",
       "      <td>even motherboard say pcie 3 0 compatible</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26931</th>\n",
       "      <td>thanks understanding know course none 3000s dv...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26932</th>\n",
       "      <td>buy 1400</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26933</th>\n",
       "      <td>lol know ill wait 3070 3060</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26934</th>\n",
       "      <td>generational compatibility either also compati...</td>\n",
       "      <td>nvidia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Reddit comments     tag\n",
       "0                    pre order time releasing 17th seems  nvidia\n",
       "1                              going hard grab 3080 17th  nvidia\n",
       "2      uk price 3090 1399 3080 649 3070 469 scan aib ...  nvidia\n",
       "3      badly want 3080 especially price still concern...  nvidia\n",
       "4      nobody talking many spatula jensen whole pot full  nvidia\n",
       "...                                                  ...     ...\n",
       "26930           even motherboard say pcie 3 0 compatible  nvidia\n",
       "26931  thanks understanding know course none 3000s dv...  nvidia\n",
       "26932                                           buy 1400  nvidia\n",
       "26933                        lol know ill wait 3070 3060  nvidia\n",
       "26934  generational compatibility either also compati...  nvidia\n",
       "\n",
       "[26935 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the existing csv file\n",
    "GPU_df = pd.read_csv('./reddit dataset/cleaned_combined_df.csv')\n",
    "GPU_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reddit comments</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26935</td>\n",
       "      <td>26935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26935</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>know right say new rtx 3080 starting 3500 ron ...</td>\n",
       "      <td>amd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Reddit comments    tag\n",
       "count                                               26935  26935\n",
       "unique                                              26935      2\n",
       "top     know right say new rtx 3080 starting 3500 ron ...    amd\n",
       "freq                                                    1  15154"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26935, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reddit comments    0\n",
       "tag                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "GPU_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for duplicated comments\n",
    "GPU_df.duplicated(subset='Reddit comments', keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of AMD and Nvidia comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amd       15154\n",
       "nvidia    11781\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPU_df['tag'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most of the GPUs are under Nvidia with a proportion of 70% while Amd has a proportion of 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title = \" \".join(GPU_df['Customer Review Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review_title)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the word cloud, it seems that consumers are mostly satisfied with their GPU purchase with 'good', 'great' and 'best' words coming out at the top. The consumers are mostly gamers and most of them play in 1080p resolution and they seem to be price sensitive with the words such as 'bang buck' and 'great value' having a bigger size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_title_list = customer_review_title.split()\n",
    "customer_review_title_dict = {}\n",
    "\n",
    "for word in customer_review_title_list:\n",
    "    if word not in customer_review_title_dict.keys():\n",
    "        customer_review_title_dict[word] = customer_review_title_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_title_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'words': customer_review_title_dict.keys(), 'freq': customer_review_title_dict.values()}\n",
    "customer_review_title_df = pd.DataFrame(df)\n",
    "customer_review_title_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_review_title_df['freq'].hist(bins=150)\n",
    "# plt.xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \" \".join(GPU_df['Customer Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, \n",
    "                      contour_width=5, contour_color='steelblue', width=700, height=500)\n",
    "wordcloud.generate(customer_review)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the customer review title word cloud, consumers who purchase GPUs tend to be gamers and they play on 1080p resolution. GPU fans are an important factor when making a GPU purchase as the word 'fan' size is rather big. The word 'issue' and 'problem' shows up big which suggests that consumers may have encountered issues with the GPUs they have purchased. The two brands 'amd' and 'nvidia' shows that these 2 are the major players in the GPU market. GPU drivers seem to play an important role in making sure that the GPU is functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review_list = customer_review.split()\n",
    "customer_review_dict = {}\n",
    "\n",
    "for word in customer_review_list:\n",
    "    if word not in customer_review_dict.keys():\n",
    "        customer_review_dict[word] = customer_review_list.count(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "customer_review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = {'words': customer_review_dict.keys(), 'freq': customer_review_dict.values()}\n",
    "customer_review_df = pd.DataFrame(review_df)\n",
    "customer_review_df.sort_values('freq', ascending=False).set_index('words').head(10).plot(kind='barh', figsize=(11,7),\n",
    "                                                                                              title='Frequency of words in customer review title')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows consistency with the word cloud on the frequency of the words appearing in the customer review title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LDA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using only Customer Review to conduct the LDA Analysis as it makes up the bulk of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to the customer reviews from series to a list.\n",
    "data = GPU_df['Customer Review'].values.tolist()\n",
    "data[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the first document with up to 30 words in them\n",
    "print(texts[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                      passes=20, random_state=42)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join(os.getcwd()+'\\\\visualization\\\\'+'ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if False:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, os.getcwd()+ '\\\\visualization\\\\' + 'ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
