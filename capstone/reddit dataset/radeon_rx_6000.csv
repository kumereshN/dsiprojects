Reddit comments,tag
Does RDNA 2 have any new encoder? Is it good as Ampere? I looked at GN video and they didn't mention it.,radeon_rx_6000
"I see a lot of people saying AMD is better at 1440 and 1080, but Nvidia is better at 4k.  I've got a 1440p ultrawide monitor.  For the sake of these comparisons, would you guys consider performance closer to 1440 or 4k?",radeon_rx_6000
"No matter what the reviews say, they will sell out because Nvidia is sold out.",radeon_rx_6000
[I'm thinking somebody on Twitter is owed $10 by AMD](https://twitter.com/AzorFrank/status/1309134647410991107) the more I read about stock levels at different retailers.,radeon_rx_6000
*Rage Mode*^TM enabled for people trying to buy these cards /lol,radeon_rx_6000
"Are there any mentions of driver support yet? The hardware seems to be delivering as expected, but, for many, the software is almost the bigger concern given AMD's history.

&#x200B;

Great to see a competitor at the high end regardless!!",radeon_rx_6000
"I just watched Linus video and my opinion on 6800XT hasn't changed. Despite the fact it does some weird shit in productivity software. But 6800 seems like a bad sell. It's 80$ more expensive 499 vs 579 which is a lot in this price category, also just 70$ below 6800XT. And while it reliably outperforms 3070 in raster it has all the downsides of XT. IMO it's a bad sandwich. People with extra dosh might as well drop another 70 and get XT and people without have no say in it. 3070 is a better value.

&#x200B;

EDIT: It's 10% below 3070 in perf per dollar at 1440p in TPU review, 15% at 1080p. That's without any extra features. This card is straight up bad. IMO AMD missed the mark with 6800.",radeon_rx_6000
"It's great that we have new top-end GPUs from more than one vendor for the first time in a while. And it seems like both of them were pretty good at pricing, all the new models are basically on a straight line in [Computerbase's price/performance chart](https://www.computerbase.de/2020-11/amd-radeon-rx-6800-xt-test/6/#diagramm-preis-leistung-3840-2160-stand-18112020).

That said, given that AMD and NV seem to give you roughly the same rasterization performance/$, and also almost the same energy efficiency (with AMD ahead by a few percent), I feel like the choice comes down to significantly better raytracing performance and the NV software ecosystem on one and and significantly more VRAM capacity on the other.

And which one you can get of course.

Edit: oh, and RDNA2 really suffers in Control. Which is a shame, since it's also IMHO the most visually impressive RT implementation in a AAA game so far.",radeon_rx_6000
"TL;DW WITHOUT RAYTRACING for the **6800XT** ($50 less than 3080)
 
* Great at 1080/1440, trades blows/beats 3080 at those,
* falls behind at 4K where memory bandwidth matters. 
* at the price (slightly less than a 3080) it's a pretty good deal for 1080/1440 gamers that aren't worried about raytracing or DLSS in the future. But for 4K gamers or anyone who wants raytracing, the 3080 is going to be superior. 

**With raytracing or DLSS:** 

* No contest, NVIDIA 3080 whoops it.



**6800**: $80 more than 3070, 

* about the same or loses with ray tracing and all that. 
* poor value compared to the 3070
* If this is your range get the 3070 or go up to the XT or 3080.

**FOR STREAMING**

* NVIDIA wins hands down no contest thanks to superior hardware encoding. AMD's is, quite frankly, garbage in comparison (LTT video goes into this)

Edited: formatted, added XT vs non XT, added streaming",radeon_rx_6000
"I guess will go to 3080 because vray rtx, cuda, and other features. Welp, Nvidia held hostage my workflow. But nice to see AMD come back to competition, really Zen moment for their GPU. Next 3 years will be very interesting",radeon_rx_6000
"Do we expect SAM to be a boost at all with NVIDIA cards like they do with AMD right now when the feature gets enabled? or are we most likely needing to wait until next generation of cards for that feature for them? 

I'm torn between 3080 or a 6800XT for purely 1440p gaming without ray tracing (Don't really care for it that much, until it at least becomes a universal standard like PhysX was years ago).

SAM seems to be a nice boost at pure resolution, while DLSS upscaling seems promising too, but so limited... Is it even worth it now?",radeon_rx_6000
"Saw absolutely nothing at NBB, Mindfactory, Caseking, [Amazon.de](https://Amazon.de), Mediamark, Saturn, Computeruniverse, Cyberport and the official amd germany store. Guess that was a fail",radeon_rx_6000
"I honestly kind of wish the 6800 non-XT had 8gb of VRAM, and was $80 cheaper because of it. It would actually look much better in price to perf graphs compared to the 3070.",radeon_rx_6000
"Skimming through the GN benchmarks. Trades blow with the 3080, sometimes beating, sometimes losing, sometimes within 1fps. 

3080 destroys it in Raytracing. Especially with DLSS, but still beats it without as well.

Edit: That was at 4K, 1440p the 6800 XT looks to beat it more often than not. So if you game at 1440p and don't care about ray tracing this card is really good.",radeon_rx_6000
"My takeaway here is that their native raster performance falls right in line with their price points. Awesome jump in performance. That said, they've got a lot of ground to make up in RT and DLSS. I also think they dropped the ball with lower memory bandwidth, and the 128MB cache can only make up for so much. Radeon is also missing RTX voice and that greenscreen thing, and their video encoder *ssssuuuuuuuuccccckkkkkkksss*. The video encoder is a real disappointment, because you know they can do better. I don't know what the deal is there. Hopefully it's a software fix, because that's just ugly. 

I recognize that, at this point, they can charge pretty much whatever they want because hey, what are you gonna do, dream about buying one of the 3 cards Nvidia managed to ship this week? But in 3 months or so, they really need to consider a significant price drop. At $100 cheaper for either of these cards, I might be one of the frothing hordes outside of Microcenter. But at this price? Eh, I can wait.",radeon_rx_6000
"That difference in power consumption!!!!!

https://tpucdn.com/review/amd-radeon-rx-6800-xt/images/power-gaming-average.png",radeon_rx_6000
"Good to see competition 

But damn the RT cost almost makes RT not even worth it. 16gb is great for 4k, but 3080 > 6800xt at 4k so does it matter?! 3070 FE seems better for 1440p. 6800xt rocks at 1080p. But why 16gb ram then?

Edit: 8gb 6800 for $500 would have made me happier. Perfect 1440p beast.",radeon_rx_6000
I would love to see a 6800XT + 5600X review.,radeon_rx_6000
"Pretty happy with my 3080 right now tbh. But I am glad there is competition going forward though. I want more improvements faster, I don't want to sit on a card for a few years again because their isn't enough improvements again. 

Basically I paid $50 for better ray tracing and DLSS. And I am okay with that. Little upset about the efficiency difference though.",radeon_rx_6000
"You really have to choose between what you want to [do in games](https://imgur.com/a/hbsdxRj) or professional workloads.

And LTT mentions that some of their OpenCL rendering tests were just outright broken on AMD.",radeon_rx_6000
"When comparing the 6800xt to the 3080 at RRP, Nvidia seems to make the better product. However in my part of the world where the 6800xt is the same price as the 3070, it sure is a much harder choice...",radeon_rx_6000
"Seeing as there's been a lot of debating over VRAM capacity, now that there's a competing product between the 3070 and 3080 with 16GB vs 8GB, what benchmarks would someone expect to show off an advantage there?

The common examples I've seen mentioned are Doom Eternal and Flight Sim 2020 at 4k ultra, but both of those don't show the 8GB as a limiting factor.",radeon_rx_6000
"I have 2 perspectives. Absolute and relative.

Relative to past AMD launches this is huge. AMD actually comes out ahead in raster performance in some games. RT is basically useless on these cards until Superres and awful in fully path traced games like Minecraft.

In absolute terms IMO it's a bit of a meh. It's 50$ less with more VRAM, but less features(godawful encoder, no DLSS, no Voice, Broadcast, etc) and first gen RT performance. With AIB partner cards the price difference is going to be even smaller, reference AMD cooler isn't as good as nVidia FE one. IMO in overall value proposition nVidia and AMD are basically even. At this point you need to assess what you want from your card and get the one that does it better.

About VRAM. Funnily enough more VRAM is needed for 4K, but AMD is behind at 4K because of bandwidth. So it might just become a dead weight for games at 1440p or 1080p. At this point I'm thinking nVidia fits my needs better.",radeon_rx_6000
"The 6800 seems underwhelming, could do with a price cut to match the 3070.

The 6800 xt looks good, especially for 1440p high refresh rate gamers not interested in ray tracing. SAM is intriguing as well.",radeon_rx_6000
"The GPU market has shown itself to be extremely price inelastic. Even though the RX 6000 series is only equivalent in value to the RTX 3000 series at best (worse value when DLSS, RTX and NVENC are counted), AMD is absolutely going to sell every last card they make until the end of the year.

Q1 2021 will be a different story, I think. It's likely that the $579 and $649 prices are only temporary for as long as Nvidia remains supply constrained. Once RTX 3000 cards can stay on shelves for longer than a day, AMD will probably start lowering prices in order to improve their value proposition.",radeon_rx_6000
"[https://twitter.com/VideoCardz/status/1329048157074886657?s=20](https://twitter.com/VideoCardz/status/1329048157074886657?s=20)  here's the gigabyte benchmarks before it was set to private

Using Ryzen 9 5950x",radeon_rx_6000
"According to the LTT review SAM is fairly effective. Most importantly it reduces the 1% and .1% lows which is pretty rad. Excited to see resizable BAR options on Nvidia cards. 

Overall, I'm still leaning towards Nvidia for my next GPU. RTX broadcast and DLSS make up the gap in price for me. It's going to be a bit before I'm ready to replace my 1080ti so AMD could close that gap for sure.

Interested to see the 6900xt though. 3090 performance for $500 less and more efficiency is enticing.",radeon_rx_6000
The ray-tracing performance is downright abysmal.,radeon_rx_6000
Wasn't the NDA lifted 30 mins ago? Haven't received 300 youtube notifications yet,radeon_rx_6000
Annnddddd.......sold out worldwide,radeon_rx_6000
"I have to say I'm more impressed by SAM than I thought. It's a bit discouraging, though since I'm stuck on a z370 Intel board. Curious if anyone will be able to find a hack around that. Looking forward to to Navi 22 for an upgrade",radeon_rx_6000
Did Techpowerup measure the power correctly? They're reporting wildly low power consumption compared to other sites.,radeon_rx_6000
"Reference cooler actually seems pretty decent. Not sure how AIB cards will perform better, but a reference 6800XT will at least fit in my case.",radeon_rx_6000
Is there any reviews that compare the visual quality of AMD's RayTracing vs Nvidia's RayTracing? with video comparison.,radeon_rx_6000
"To make a comparison to the cpu side of AMD, this launch feels a lot like the ryzen 2000 series

Finally close to or matching the competition at a cheaper cost with much less power draw

They might not beat nvidia right now,  but they are close enough to provide competition.  As someone who doesn't need an upgrade this makes me confident for a solid nvidia 4000 vs amd 7000 next year or in 2022",radeon_rx_6000
GN review for the 6800XT : https://www.youtube.com/watch?v=jLVGL7aAYgY,radeon_rx_6000
It's interesting that nvidia seems to win at 4k even with less vram. My hypothesis is that the new cache system helps a lot on lower resolutions but 4k is just too much data for it to be very useful compared to nvidia's larger memory bandwidth.,radeon_rx_6000
Radical point of view: its now up to Intel to save us from the gpu shortage.,radeon_rx_6000
"Looking at 1440p rasterization performance from TPU's review: 3070 < 6800 < 6800XT < 3080. Basically following the pricing structure.

Looking at 1440p ray-tracing performance from TPU's review: in Metro the 6800XT barely beats the 3070 and in Control it is well behind. 6800 is behind the 2080 Ti. 

Overall when you factor in DLSS and new consoles opening the gates for ray-tracing games it feels like Nvidia is a more well-rounded choice - it's just a lot better at ray-tracing. The 6800 feels like a misfire pricing it $80 above the 3070. The 6800XT seems like a decent buy at $50 less than a 3080 if you don't care about ray-tracing.

I expect the 6900XT to edge out the 3080 by a few percent in rasterization but still trail on ray-tracing. If Nvidia debuts the 3080 Ti at the $1000 price point I think it'll be the clear preference there.",radeon_rx_6000
"One thing I noticed is that the 6800xt outperformed the RTX 3080 in newer titles such as AC Valhalla, Godfall, and Watch Dogs Legion. Is this possibly due to optimization for the consoles running RDNA 2? Also, I'm wondering if the 10gb vram is bottlenecking the 3080, as some developers said that it would.",radeon_rx_6000
"Looks like the ""RT isn't important"" crowd at least has some options.   Personally I'd pay that $50 to secure some nvidia 3080 performance in everything, or the 6800 if you just don't care.",radeon_rx_6000
Anyone seen any reviews on the new AMD encoder ?,radeon_rx_6000
Any VR reviews?,radeon_rx_6000
Does anyone know if SAM will be supported by PCIe 3.0?,radeon_rx_6000
Why is everyone referring to MSRP when none of these cards are selling anywhere near it? It makes all these value comparisons meaningless.,radeon_rx_6000
Damn so AMD have pulled off the best perf/watt cards so far this generation. I wouldn't be surprised if the low-end chips beat them but this is very impressive considering where they were only a year ago.,radeon_rx_6000
"So basically:

\+ A lot more VRAM than counterpart

\+ Equal or better rasterization performance to 3080 depending on the workload

\+ Cheaper

\+ Better power efficiency compared to Nvidia

\- Far worse Ray Tracing performance, falling behind 40-50% on intensive tasks

\- Lack of feature set, like DLSS or better video encoder

\- OpenCL will break in certain workloads",radeon_rx_6000
No one here is really talking about Smart Access Memory. Gamers Nexus compared SAM on and SAM off and the difference was negligible. I don't really know what I was expecting but it's pretty disappointing and an extremely minor selling point at best.,radeon_rx_6000
"pretty much perform the same as 3080 in 1080p and  1440p. But in 4k, 3080 still beat it.

And in RT performance and with DLSS on , it is not even fair and close. 3080 trashes 6800XT.

I think in GN video in minecraft 4K RTX on DLSS ON, 3080 get 87FPS.  In the other hand, 6800XT get 13.9 FPS lol. 

Back to F5 ing for 3080.",radeon_rx_6000
"Phoronix.com / Linux: https://www.phoronix.com/scan.php?page=article&item=amd-rx6800-linux&num=1

Excellent support at launch. Very good performance across the board. Sometimes double of 5700. Also close to 3080 performance, with significantly lower power usage.

FPS/W winner.",radeon_rx_6000
"I don’t have money to get it, but they look cool",radeon_rx_6000
"/u/Nekrosmas Can you add Techgage to the list? 

https://techgage.com/article/amd-radeon-rx-6800-xt-rx-6800-gaming-performance-review/",radeon_rx_6000
"That RT performance is very underwhelming. And with no DLSS competitor released yet, it doesn't look good. The 6800XT needed to be $100 cheaper than the 3080 to be compelling. When we get to a point that both are in stock, there will be little reason to go with the 6800XT over the 3080.",radeon_rx_6000
"TLDR: If you care about ray tracing, NVidia is the clear winner, if not, the 6800xt is a great option.

I will say though, I think if AMD can focus on their drivers a bit, and push out their answer to DLSS, I could see this gap closing quite a bit. Will it ever reach the 3080 with ray tracing? Probably not, but you'll have gaps closer to non ray-tracing scenarios.  
IMO though, there is no market for a 6800, if you're considering that, you may as well get either a 6800xt or a 3070.  


Of course if they're in stock.",radeon_rx_6000
"With insanely low availability (of cheap version of 6800XT reference), being in EU (VAT, worse availability), and paying the marked up price of the AIB cards, probably I might choose 3080 over the 6800XT if I feel like upgrading next year. The delta 50 EUR MSRP in the end would not matter much probably, if the price delta is bigger like 100 EUR it would have been easier to choose AMD.


I only use DLSS from Nvidia as a determining feature. Playing in UW 1440p which is between 1440p and 4K, I felt they both are quite similar or the Ampere card is slightly better than RDNA2. Even though the performance per watt is worse for Nvidia, I still think my SF600 is enough to manage it. Also Nvidia stated to look on the implementation of SAM as well. 


Will be curious to see how the driver goes for AMD in the coming months and if they have more info to be revealed on the fidelityFX/DLSS equivalent.",radeon_rx_6000
"Fullpath raytracing is ugly for AMD @ 4K:

https://i.imgur.com/PN6aVx9.png (Source: LTT Video Review)

3080 RTX is **5x** faster than the 6800XT with DLSS",radeon_rx_6000
"Its kind of hard to make a case for 6800XT if the price difference between 3080 is only around $50, like if you have that kind of money then just go for 3080 with more features even if you say you would not use them. Even The 6800, I would just buy the 3070 and spend/add the extra $ on other components.",radeon_rx_6000
"I know people keep saying that AMD will fine wine the ray tracing performance but this is quite the gap with it on. 

Though I guess with stock issues you have no choice but to wait and see if that's true.",radeon_rx_6000
[Gamers Nexus teardown video](https://youtu.be/0s7bOaa6X9E),radeon_rx_6000
"[https://www.youtube.com/watch?v=yg0Xiy3N1AU](https://www.youtube.com/watch?v=yg0Xiy3N1AU)

&#x200B;

Looks like a nice enough review. They even tested AMD vs Intel for these cards.",radeon_rx_6000
"Regardless of the mediocre ray tracing in these cards, they will still sell out like hot cakes. All AMD needed to do is make an Ampere competitor for less $$$ with some ray tracing, which they did. It all now depends on how much stock they have to capture market share.",radeon_rx_6000
"Seems like the take away is that if you really care about RTX or DLSS then go with the 3080. But if not, then getting whatever is available is fine.",radeon_rx_6000
I got one boys can't believe it! Will it actually ship who knows. I'm happy just to have new card.,radeon_rx_6000
"[AMD Radeon RX 6800 XT Review (techspot.com)](https://www.techspot.com/review/2144-amd-radeon-6800-xt/)

>The Radeon RX 6800 XT delivers excellent performance. Just two months ago, the RTX 3080 completely [blew us away](https://www.techspot.com/review/2099-geforce-rtx-3080/) with its performance, and we weren't overly confident AMD could pull this one off. But for the first time in a long time, the latest Radeons are able to catch up to newly released high-end GeForce GPUs. As it's often the case, depending on the game and even the quality settings used, the RX 6800 XT and RTX 3080 trade blows, so it’s impossible to pick an absolute winner, they’re both so evenly matched.  
>  
>The advantages of the GeForce GPU may be more mature ray tracing support and DLSS 2.0, both of which aren’t major selling points in our opinion unless you play a specific selection of games. [DLSS 2.0 is amazing](https://www.techspot.com/article/1992-nvidia-dlss-2020/), it’s just not in enough games. The best RT implementations we’re seen so far are Watch Dogs Legion and Control, though the performance hit is massive, but at least you can notice the effects in those titles.

...is pretty much where I'm getting at.

Also, most outlets are testing *without* SAM, which I think is a show of confidence from AMD. So if you have Vermeer CPUs, it's even better than what you are reading right now. I think many gamers prefer 1440P HFR over 4K, and with HFR it looks like 6800XT is a better choice overall.",radeon_rx_6000
"Even if they didn't best or sometimes even match Nvidia everywhere, the fact that they pulled this off after lagging behind in high end for a decade is impressive and gives me hope this is kind of a *Zen"" moment. Doesn't take the crown immediately but secures a solid base for future architecture.

Availability is another question entirely, where if the AIB rumor is true, this puts AMD as the only available manufacturer.",radeon_rx_6000
"As 4k gamer seems I was lucky not to snatch 6800 xt today. I was expecting lower results in rtx but this is really big gap. But for folks that play in 1440p/1080p and don't care about rtx, 6000 could be interesting proposal.

Btw no wonder nvidia will push 3080ti and price.",radeon_rx_6000
Rasterization performance is decent but they are way behind on ray tracing. AMD does have a slight advantage with SAM but that's going to change pretty soon with NVIDIA supporting it as well.,radeon_rx_6000
"Overall very disappointed with big navi, the moment i saw the RT perf i jumped on an in-stock 3070 aorus master.",radeon_rx_6000
Did any of them test MSFS 2020?,radeon_rx_6000
[deleted],radeon_rx_6000
"Just my personal feedback on the matter. As always, YMMV.

I care about performance first and power efficiency second, but it's weighted heavily. Years ago I bought the GTX 1060 and RX 480. The former was ~10% faster while consuming FAR less power (~120W vs. ~200W in AIB models). It was an easy call on which to keep.

I have a small case on my desk next to my head. Higher power draw means more heat which ultimately leads to more noise. My 190W MSI Gaming Z RTX 2060 is pushing it, and I'd like to step down to ~150W or lower this gen.

So, to see the 6800 series from AMD have average gaming power draw as low as it is compared to Nvidia is quite exciting to me. At 164W average in gaming, the RX 6800 offers power draw on par with the reference model 2060/1080, as well as the 5700. AMD has lowered their power draw tier. This would be like the RTX 3080 matching RTX 2070 power draw (lol).

I run a 2060 in my main gaming system and my old 1060 in the living room hand-me-down build. The 3060 Ti is looking to be too power heavy for what I want. I could honestly see a 3060 Ti competitor come from AMD that has power consumption on par with or lower than a base 3060. And if that happens, I might go team red again (driver situation pending).

As for other features:

* I don't stream, so don't care (made this same argument to the AMD fans when I chose my 9400f over the 2600, and I am consistent).
* The cards in my segment aren't powerful enough to utilize ray-tracing in most games, so I'm not worried about that for another generation at least.
* I have one game that supports DLSS, and I had to disable it to prevent crashing (known issue). So at least a generation away from me caring about this feature.


Overall, if AMD can sort their drivers, they'll likely get my money this gen. I do, however, want to see if their HDMI 2.1 implementation works with the LG C9's VRR. That's another issue that matters for me.",radeon_rx_6000
"If the 6800 were $500 and the 6800 XT were $600, it'd be a lot more acceptable than it currently is right now. A great deal of AAA games coming out now and in the future are going to be supporting RT, so that's far from unimportant. 

Seems like I'll just wait another few months and see if availability and pricing starts to make more sense. Not dying on a Fury X anyway.",radeon_rx_6000
Is Anandtech not doing GPU reviews anymore? They don't seem to be doing this AMD launch and they didn't do the Nvidia 30xx launch either.,radeon_rx_6000
"Looks like the usual AMD affair: ""Okayish hardware, shame about the features/software"".

AMD have to know that these cards just aren't competitive and won't be if you can pay $50 more for vastly superior ray tracing performance, hardware encoder quality, DLSS, OpenGL support, better drivers. Definitely worth it, especially considering that last gen people were paying a $100 ""Nvidia tax"" for the 2070s over the 5700XT for pretty much the exact same reasons. They absolutely NEED to address these things next year, if not sooner.

Of course, this all hedges on whether you can actually find either of these cards in stock.",radeon_rx_6000
"Well now we know why they entirely left ray tracing out of their launch presentation. That's worse than I expected to be honest. 

I don't really see the reason for anyone in the market for a top end GPU to go for AMD here. The small increase in price for a 3080 seems more than worth the massive jump in ray tracing and DLSS. 

People often say oh ray tracing isn't a big deal yet, but it absolutely is becoming so. The majority of AAA titles are likely to have some form of ray tracing, meaning that unless you're sure you're never going to turn on ray tracing then the gap between these cards is going to be pretty substantial with maxed out settings, which let's be honest, is what people buying a top end card is looking to do

Add DLSS into the mix and the gap widens even more",radeon_rx_6000
"I wonder how well FidelityFX SuperRes will work, since as I understand it 6800XT don't really have any dedicated ML processor like Nvidia's Tensor cores.",radeon_rx_6000
Fuck trying to buy a gpu or new cpu this year unless i'm a bot or live by my F5 key all day long and fuck 2020!!!,radeon_rx_6000
"As expected **raytracing** is a big downside of those cards. Computerbase.de for example shows that the 3080 is around 16% faster than the 6800XT with raytracing off in Control at 4K but with raytracing the Nvida card has a whooping **71%** performance advantage. In 1440p the deltas are 12% and 66% btw. 

Arguably Metro and Shadow of the Tomb Raider show less extreme differences, but we are still talking about 23% and 35% more performance on the Nvidia hardware, all w/o even using DLSS. 

According to the same site AMD is only accelerating ray traversal via a hardware unit but uses the compute shaders for BVH while Nvidia has hardware units for both.",radeon_rx_6000
"That Hardware Unboxed review is...how can I say this...unabashedly poor.

There seemed to be an attempt at damage control that I don't think should be present in an unbiased comparison on competitive parts.  The RT selection is highly suspect.",radeon_rx_6000
"Ray tracing perf is crap.

Performance overall is OK, 6800xt similar to 3080 and 6800 a tad better than 3070.

Overclocking performance is very good and the headroom is especially is surprising.

Efficiency is shaming nvidia, well well. How the tables have turned. Efficiency is very good!

To me it at depends on pricing, availability and drivers, especially.",radeon_rx_6000
What I learned today: I’m not going to find a 3080 before the end of the year.,radeon_rx_6000
"/u/Nekrosmas - can you please add this review to the list:

https://www.extremetech.com/gaming/317476-amd-radeon-6800-xt-review-big-navi-battles-the-rtx-3080


Good roundup.  

It's become apparent why Nvidia may want the rumored 3080Ti - AMD does have a competitive GPU, at least at rasterization. Winning the power efficiency fight is a big win and offering more VRAM can come in handy for some games (not all). It doesn't win everywhere, but it is competitive and is  priced accordingly. 


It's giving last generation Nvidia performance at ray tracing, which is a big weakness, but this is closer to a competitive GPU than AMD has released for a while.",radeon_rx_6000
"As expected, low - mid tier resolution like 1080p - 1440p AMD actually equal or even better while 4K is slightly lower depend on titles.

But once RT enable, it's the different story.

I mean it's not bad consider AMD GPU postion in market last year but if you want new card for Cyberpunk in full experience, I doubt AMD are the best choice.",radeon_rx_6000
"Guru 3D showing that the 6800XT beats the 3090 by 10% in AC: Valhalla at 1440p. 

I wonder if that's a sign to come for future next generation console games, where a lot of the optimizations for PS5 and Series X transfer over.",radeon_rx_6000
GN has posted the 6800 non XT review: https://www.youtube.com/watch?v=NbYCF_h2aVM,radeon_rx_6000
"Ideal card to pass the couple of gens: enough VRAM + RT & DLSS.

Reality: Ampere is likely to be VRAM constrained in the future, Big Navi lacks feature-wise on release. Take your pick.

I already have got a 3070 at MSRP though, good luck beating its value.",radeon_rx_6000
"Return of HD48xx series with very good performance at lower resolutions and dropping off at 4k. 

Ray tracing performance should improve later on with better drivers and console games optimizing for RDNA2.

perf/W improvement is great as well, though much of the issue lies with nvidia power consumption.

**edit**: More observations :

4k performance drop can be pretty high, maybe folks at AMD are wishing now that they had used HBM2 instead.  

SAM can be fantastic at <4k resolutions, both in avg and min. framerates.",radeon_rx_6000
"Here's a review from a respected Finnish reviewer (in Finnish):

https://www.io-tech.fi/artikkelit/testissa-amd-radeon-rx-6800-6800-xt/",radeon_rx_6000
Does someone know if Anandtech's review is delayed or something?,radeon_rx_6000
"/u/Nekrosmas - Could you add this Dutch review from Tweakers.net?

https://tweakers.net/reviews/8354/amd-radeon-rx-6800-en-rx-6800-xt-het-zen-moment-voor-radeon.html",radeon_rx_6000
"Well, im even more confused now.",radeon_rx_6000
"TL;DR for the 6800XT:

4-6% slower than the RTX 3080 in pure rasterization across 1080p, 1440p, and 2160p

Hybrid Ray-tracing: between 2080S - 2080Ti level

Full-path tracing: Nvidia is twice as fast

If you don't care much about ray-tracing, the 16 GB buffer is more future proof and given the fact the next-gen consoles are based off AMD's architecture, games will likely play well with it for the future.

Also slightly cheaper than Nvidia, but AMD could've had a way better value proposition if they retailed it for $600. 
I'd argue RTX 3080 is better bang for the buck right now given its superiority in raytracing, DLSS 2.0, tested drivers, and slight edge in rasterization, all for an extra $50.",radeon_rx_6000
3080/3070 Look better overall but I think it will just come down to availability since the 6XXX isn't that far off outside of ray tracing.,radeon_rx_6000
DLSS is being underrated (not only here though).,radeon_rx_6000
"According to TPU the 3080 is about 5% faster than the 6800XT, not counting RT or DLSS

https://www.techpowerup.com/review/amd-radeon-rx-6800-xt/35.html",radeon_rx_6000
"Seems like the jury will be out for another couple of months which card gets the rasterization title. With SAM and RAGE mode 6800XT pulls ahead in <4k, but Nvidia claims to have their own SAM equivalent in works. On the other hand, AMD claims to have their own DLSS equivalent in works that is game agnostic, obviously we wont see the huge DLSS-like uplifts, but even 10% across the board will give the 6800XT a clear victory in non-4k non-RT.

On the plus side AMD is able to eek in under Nvidia's power consumption which is a nice bonus, but not a huge selling point.

Despite both being unavailable, I feel like AMD's $650 might not be the best price point, since you are giving up better software, currently functioning in a handful of titles DLSS, and significantly better RT. $599 would make it very very easy for me, but I think $619.99 would also be sweet spot.

Overall RDNA2 has done surprisingly well, I know its probably not better than Ampere in most people's eyes, but who wouldve thought that AMD would even be this competitive at the high end up until a few months ago? They've closed the gap on Nvidia faster than they did on Intel.

The biggest disappointment this launch has definitely been availability though. I know 'RDNA2 AIBs are coming soon with 5x the volume of Nvidia', but here we are with another release and people not even really seeing a card even for a fraction of a second. I saw the 6800 on Amazon, and that was gone instantly, and I was sitting here all morning with auto-refresh on several websites. Realistically more people will die of Covid this christmas than a new GPU, fun times.",radeon_rx_6000
"That's OK, but yeah, Raytracing and Productivity doesn't look great this generation.

Now we need to see their availability in the next few weeks.",radeon_rx_6000
"Meh. For the pricing AMD went for, glad I got my 3080.",radeon_rx_6000
"[https://www.youtube.com/watch?v=MIkb2LF1Rfo](https://www.youtube.com/watch?v=MIkb2LF1Rfo)

Gigabyte already release a quick review",radeon_rx_6000
Honestly I'll buy the first one that'll be in stock for a reasonable price and from a brand that I like.,radeon_rx_6000
Looking at the reviews now its pretty clear why Nvidia panic launched like they did.  A 1599+ 3090 card should not have been marketed for gaming at all and at this point would seem to be a bad move to purchase in retrospect.  This gen is all about the battle of the marketing.,radeon_rx_6000
wow the power efficiency seems to be unparralleled. I think I'm gonna go for the RX 6800 (the new most recommended graphic card for linux gaming),radeon_rx_6000
"That is quite some power efficiency. [~~230W~~ 210W](https://tpucdn.com/review/amd-radeon-rx-6800-xt/images/power-gaming-average.png) for near-3080 performance. I was skeptical they could do that while increasing performance and they seem to have done it.

Raster performance is on par with 3080 more or less, RT performance is really, really bad, and no DLSS.  [Same raster perf/$ as 3080.](https://www.techpowerup.com/review/amd-radeon-rx-6800-xt/37.html)

I think the argument is pretty clear, it’s 16GB and it’s more efficient, vs the 3080 doing obviously better in RT today and probably gaining an advantage in both raster and RT in DLSS titles.  AMD is clearly not going for underdog pricing and apart from the 16GB it's a little bit of a tough sell in performance or perf/$ terms given the feature deficit.  But the efficiency is really really great if that's your thing.",radeon_rx_6000
"Nvidia = Dr. Pepper

AMD = Dr. Thunder",radeon_rx_6000
"Overall, underwhelming other than performance per watt. It seems like RTX 3070 is still a better deal for anything less than 4k.",radeon_rx_6000
"As someone who doesn't care about RTX or DLSS, these are looking like a good option for my linux gaming rig.

Now if these have built in support for SR-IOV that just makes them killer for any virtualization setup.

Honestly AMD should come out and state they kill Nvidia at price to performance for GPU passthrough by supporting SR-IOV on consumer cards since we know Nvidia would never do that as it would make their over priced quadro cards pointless.",radeon_rx_6000
"Disappointing honestly. AMD needed to either smash Nvidia in perf (at least non-RTX) or give a great perf/$ card relative to what's on the market. They have done neither. 

For 50$ less you are getting lower power consumption (solved on 3080 by undervolt) and that's about it, perhaps better 1080p performance for eSports players. This is at the cost of near unplayable RTX performance and lack of DLSS. I don't see this being a good buy outside of very specific use cases.",radeon_rx_6000
Yikes on that RT performance. Something tells me their DLSS competitor is also probably a pile of shit.,radeon_rx_6000
"The 10GB of VRAM on the 3080 and 8GB on the 3070 is still a joke. It should have been 11GB for the 3070 and 12GB for the 3080.

Not that VRAM makes a card faster, but you are just cutting it was too fine with either of those cards in 2020.",radeon_rx_6000
"Not bad AMD. Competition is key!

I hope the 6900XT wrecks the 3090. I mean, even the 6800XT almost does.",radeon_rx_6000
"I like my games like my women, so im going with Nvidia again for this generation.",radeon_rx_6000
"6% performance vs 7% price...fair trade 

Unless you really need RT and DLSS.",radeon_rx_6000
"Huh, 6800XT is surprisingly meh, fine if you can live without your life raytraced, pretty bad otherwise. 

What the hell were they thinking with the vanilla 6800's pricing though?",radeon_rx_6000
Damn B&H not selling any: https://www.bhphotovideo.com/pages/availability-update/amd-rx-6000-series.html?origSearch=rx%206800,radeon_rx_6000
Review in Romanian: [https://lab501.ro/placi-video/amd-radeon-rx6000-part-iii-amd-radeon-rx-6800xt-amd-radeon-rx-6800-performanta](https://lab501.ro/placi-video/amd-radeon-rx6000-part-iii-amd-radeon-rx-6800xt-amd-radeon-rx-6800-performanta),radeon_rx_6000
Wtf are Linus' spreadsheets... That format made me so angry I skipped the entire video,radeon_rx_6000
AMD has confirmed they coming out with DLSS like mode for these cards right? Just not at launch?,radeon_rx_6000
"Here's a good review that probably won't get added to the megathread:

[https://techgage.com/article/amd-radeon-rx-6800-xt-rx-6800-gaming-performance-review/](https://techgage.com/article/amd-radeon-rx-6800-xt-rx-6800-gaming-performance-review/)",radeon_rx_6000
Something feels off with the HUB review,radeon_rx_6000
"This really gives me hope for Navi 22. 

Seems AMD needs have at least 5% more CU count compared to the amount of SMs on Nvidia to match it (68 vs 72). So I was expecting 40 cu to not get close to a 3070's 46 SMs. But looking at the 1080p results, it seems even with a deficit AMD might be able to close that gap at lower resolutions.",radeon_rx_6000
"If these cards are actually available, then AMD wins by default. The performance per dollar is virtually identical. RT and DLSS are the only features truly not competitive, but if they are the deal breakers for you, keep smashing F5.

If these cards are also scarce, then AMD fucked up even more than than Nvidia.",radeon_rx_6000
"I want to play at 1440p, looking at this reviews the RTX 3070 has better value and better RT.

Why would anyone buy AMD for 1440p? And for 4K the RTX 3080 has better results and better RT and is just slightly more expensive. 

I'm just not seeing why would anyone buy these cards, I wanted to go all red and I thought this would be it but I just don't see why.

5600X and a RTX 3070 with little VRAM it is.",radeon_rx_6000
"This thread must contain every single 4k gamer in the demographic.

Y'all are high as hell.

everyone is freaking out about it. It's hilarious.

The last cycle RT wasn't a big deal, and def not at 4k. Now 1 card (3090) is a viable native 4k card, and  XT looks like a beast of a card offering between 3080 and 3090 performance in raster, but only offers a 1440p60 experience with RT features on.

If you're waiting for an upsampling competitor, then keep waiting. But I'm sure it just won't be good enough then either.",radeon_rx_6000
"All the newly released AMD sponsored titles give AMD a very strong showing, did AMD gimp them for Nvidia cards for launch review?",radeon_rx_6000
So is RTX just better reflections in games?,radeon_rx_6000
That RT performance is the exact embarrassment I told everyone it would be.,radeon_rx_6000
The review titles sound promising!,radeon_rx_6000
Does anyone know when the 6900XT review embargo lifts?,radeon_rx_6000
Pretty much what I expected and why I went with the 3080 a few weeks ago.,radeon_rx_6000
[removed],radeon_rx_6000
So suddenly when AMD has better raw performance everyone cares about ray tracing because nvidia said you should,radeon_rx_6000
Looks like AMD got themselves a nice win with these cards! I only slightly regret having bought a 3080. Maybe in a few years I'll jump back to AMD.,radeon_rx_6000
"Damn, AMD stomping competition this year, both in CPU and GPU market.

6800XT looks like a go-to card for 1440p if drivers will be good.",radeon_rx_6000
"i quess 5600x + 3070, ur gpus still not at lvl to compete vs nvidia nt tho",radeon_rx_6000
"So basically...

6800xt is better than 3080 but less features. 

Also trades blows with 3090 which is pretty insane. 

The 6800 price tho is pure lol. What was AMD smoking",radeon_rx_6000
"PC Gamer:

 https://www.pcgamer.com/amd-rx-6800-review-radeon-benchmarks/

https://www.pcgamer.com/amd-rx-6800-xt-review-benchmarks-performance/",radeon_rx_6000
">Some games won't run in RT at all, either - Quake 2 RTX, an open-source  but Nvidia-backed project, refuses to start, while Shadow of the Tomb  Raider enables RTX and DLSS together, so you don't have the option to  turn on ray traced shadows on AMD hardware at all.

From Eurogamer. I thought SotTR was full DXR, no proprietary RT. I don't have SotTR, does anyone know what this is or have any image comparisons?",radeon_rx_6000
Will SAM even matter when direct storage rolls around?,radeon_rx_6000
"Ray tracing is still a meme. DLSS is amazing but without widespread support meaningless. 

AMD caught up in rasterization with lower power usage. That’s good enough for now.

And all of the idiots that kept saying, “rDnA2 WiLl cOmPeTe wItH tHe 3070 aT bEsT,” will shut up and learn to stop talking in the future.",radeon_rx_6000
[deleted],radeon_rx_6000
So it's worse then an rtx 3080 glad yall came out guys,radeon_rx_6000
I got heavily downvoted for suggesting the 6000 series had Turing level RT performance and above Ampere rasterization performance.,radeon_rx_6000
I know I should probably wait another month or two but those 5700XTs are looking real nice right now,radeon_rx_6000
embarrassing. 1080p lol,radeon_rx_6000
JayzTwoCents: https://www.youtube.com/watch?v=NXRvqjXf4Sc,radeon_rx_6000
[removed],radeon_rx_6000
"It does not. Linus Tech Tips touched on it in [their review](https://youtu.be/oUzCn-ITJ_o?t=560) of the cards. LTT calls it ""rubbish"".",radeon_rx_6000
"Watch LTTs

TLDW; No, it sucks.",radeon_rx_6000
"No, and I think they want to downplay GPU encoding because they want to sell you high core count Ryzens.",radeon_rx_6000
I'm interested in this too,radeon_rx_6000
"Ultrawide 1440p is about 50-60% pixels of 4k, so much closer to 1440p

For reference, even 2x 1440p is only about 85% of 4k",radeon_rx_6000
"Well, considering you are giving up all future raytracing usage and DLSS among other nvidia software suite features, you should go 3080.  The new AMD codec unfortunately still sucks, LTT touches on that, so if you stream/record/play VR at all thats also a benefit to the 3080.  The % difference in most games is marginal.",radeon_rx_6000
"Who is ""a lot of people""? According to TPU https://www.techpowerup.com/review/amd-radeon-rx-6800-xt/35.html nvidia is ahead at 1080 and 1440",radeon_rx_6000
"Haha lol! Today, Frank Azor literally tweeted that he bought himself a 5800 XT from the website, after tons of PC enthusiasts and gamers failed to get their hands on one because of the paper launch. Lmao!",radeon_rx_6000
"And like real life rage mode, it does nothing but make noise.",radeon_rx_6000
"In the L1t Linux video Wendell says there seem to be far fewer hardware bugs that need to be worked around on the Linux side when compared to Vega and first gen Navi. That doesn't necessarily mean the experience on Windows is going to be better let alone flawless, but it's certainly a good sign in my eyes.",radeon_rx_6000
I speculate that yields on Navi 21 are good enough that AMD doesn't really want to sell that many 6800 non XT's. Demand is high enough that they can go for higher margins since they're basically guaranteed to sell everything they produce for the next couple months at least.,radeon_rx_6000
"I agree. Would’ve been more compelling at the same price, but it’s not worth paying *any* premium for a 6800 vs 3070",radeon_rx_6000
"I'm still undecided between 3070 and 6800. Sff, so less W is a plus, and I don't care about 4k or ray tracing.",radeon_rx_6000
Always wait for price drops with AMD,radeon_rx_6000
"Also don't forget the gains with DLSS and that AMD is still working on their super aampling technology.

Personally I'm looking more forward to the mid-range battle on the beginning of next year.",radeon_rx_6000
Rasterization is gonna be bottlenecked before VRAM in 99.8% of games.,radeon_rx_6000
VRAM quantity is pretty irrelevant. By the time it matters the GPU will be slow and out performed by $350 options.,radeon_rx_6000
"Yeah but are we also forgetting that amd also powers both next gen consoles. I feel like Nvidia takes the lead for now but future proofing wise AMD will come out on top in gaming, as more games will use dx12 super resolution and ray tracing compared to Nvidia rtx. Seeing how far they have come in just the last 5 years. I expect them to fix their software and driver issues, AMD has changed for the better and I think their products will just keep getting stronger.",radeon_rx_6000
[deleted],radeon_rx_6000
That's the take I am getting too.  Trades well with 3080 until the RTX and DLSS are enabled its no contest.,radeon_rx_6000
"Looks like the 6800XT is an impressive leap over the 5700XT, but the price, performance and feature set don't match up with Nvidia in my eyes. 

In a hypothetical world where I can find both the 6800XT and 3080 FE at MSRP, I'd gladly pay $50 extra dollars for the 3080 despite similar performance. With Nivdia you get DLSS, better ray tracing and more peace of mind regarding drivers (especially for VR).

AMD needed a clear performance win or a more drastic price difference to overcome the reservations I have regarding their GPUs. But availability will likely be the real factor in which GPU I end up buying.",radeon_rx_6000
TechPowerUp has a good summary. 3070 is a better value compared to 6800. 6800 really does suffer with ray tracing on,radeon_rx_6000
Feels like the 5700 XT vs 2070 super/non-super all over again. Same pros and cons as before,radeon_rx_6000
"Yep, I'd say this is the perfect summary. It's basically what everyone should have expected after they shied away from showing RT numbers at the launch. 

There was a lot of variation (especially across resolutions), so I look forward to seeing the meta-analysis so we can examine average performance differences in rasterization. It seems like it will be very close to parity at 1080/1440p, and maybe 5-10% in the 3080's favor at 4k. 

I can't imagine giving up the RT/DLSS for $50, but I also play at 4k anyway, so there's literally no incentive for me to go with the 6800xt.",radeon_rx_6000
"I think the real battle is going to be 6900 XT. Linus hinted that Super Resolution may be available by that time and at that price point, it'd be the obvious choice for 4K gaming due to the higher VRAM. 

If super resolution is something that is universally supported it may even outperform the 3080/3090 significantly.",radeon_rx_6000
"Really not sure what AMD was thinking here. They needed to wait another generation and actually match or beat Nvidia before trying to break out of their ""value"" market segment. The only card of theirs that beats Nvidia in price/performance is the 6900XT, which Nvidia is probably going to be one upping with the 3080TI. As it stands the only real reason I see to get AMD cards this generation is because Nvidia cards aren't readily available and even then it looks like AMD is going to have supply issues as well.

I'm not sure if their dominance in the CPU market has made them cocky or what, but I really feel they jumped the gun.",radeon_rx_6000
"What's the pricing difference again? 

I am mostly interested in rasterization but ray tracing can be a nice feature in games that are more about graphics and atmosphere than raw FPS.

Right now I'm leaning more towards the 3080.",radeon_rx_6000
"Was waiting to see 6800 reviews to make a decision on 3070 vs 6800. 3 weeks later, zero 3070 cards in Canada (excluding scalpers obv.)",radeon_rx_6000
"I wonder if the drivers are still AMD trash.

That easily makes AMD a failure for me.",radeon_rx_6000
"NVIDIA broadcaster also has better background removal, I'd suspect?",radeon_rx_6000
Is there any reviews that compare the visual quality of AMD's RayTracing vs Nvidia's RayTracing? with video comparison.,radeon_rx_6000
"> at the price (slightly less than a 3080) it's a pretty good deal for 1080/1440 gamers 

Imagine being told 5 years ago that a £600 graphics card is a pretty good deal for 1080p gaming. Then imagine that at the same time you could buy an entire gaming system for less than that with similar performance.",radeon_rx_6000
Good unbiased summary.,radeon_rx_6000
[deleted],radeon_rx_6000
"Same here. Doing ML with AMD is possible, but CUDA support is much more widespread. I can't even consider AMD for my machine, even though I'd certainly like more options.",radeon_rx_6000
"Really interested in Radeon as well, but also a captive consumer for nVidia's added features. I travel a lot for work, so the shield streaming capability via moonlight is a massive value added feature for me. As well as the fact I own an nVidia shield and generally prefer couch gaming. Really hope AMD does well enough with this generation that they can devote some funds to catch up on value-added features like this and I'll happily jump ship.",radeon_rx_6000
SAM is just AMD marketing for BAR. Nvidia is testing it on current AMD/Intel platforms and with Ampere cards.,radeon_rx_6000
"The Nvidia software (Overall drivers and Nvidia Broadcast) is enough to sway me, but the rtx performance makes the 3080 heavily preferred. The performance at 1440p seems to effectively be even between them.

I already have a 3080, but if I had a shot at a 6800xt and didn't have either I don't think I'd turn it down.",radeon_rx_6000
"Nvidia said they are seeing the same uplift in their testing. However, as the Gamers Nexus video mentioned it’s detrimental in some titles. Steve said in his communications with Nvidia they are looking into whitelisting games that benefit from adjustable BAR in the driver so when you play a game that benefits it will just be enabled automatically.",radeon_rx_6000
"AMD is also coming with a DLSS equivalent, but it isn't here yet.
SAM seems like a really cool technology, but correct me if I'm wrong but it boosts performance by letting the CPU access the GPU memory. The 3080 has a decent amount of memory, but much less than the AMD cards so I wonder if the performance boost will be impacted a lot by that.


I would look at which card you are able to buy for a good price if I were you. I know that's what I'll be doing",radeon_rx_6000
SAM should provide similar benefits for NVIDIA. Its possible it could be worse but it could also be better.,radeon_rx_6000
"MF has (had?) the 6800 in stock, was able to view the 6800 XT (for 789€!!!!!!!) page but they were out of stock at 15:00",radeon_rx_6000
Yup i even went to MediaMarkt and the employee couldn't tell me when they get any gpus at all,radeon_rx_6000
"Had it in my cart two times on Mindfactory, but the site timed out once I wanted to checkout.
Next time I loaded the product page it was already gone.",radeon_rx_6000
"According to Computerbase only MF, Alternate and AMD itself sold these cards. The cards were gone after seconds.",radeon_rx_6000
And people called the nvidia launch a paper launch lmao,radeon_rx_6000
No doubt there will be an 8gb aib when they drop.,radeon_rx_6000
"> That was at 4K, 1440p the 6800 XT looks to beat it more often than not. So if you game at 1440p and don't care about ray tracing this card is really good.

The TPU review shows it trailing the 3080 at 1440p on average slightly. But I agree if you don't care about ray-tracing, saving $50 and getting a 6800XT is a good choice.",radeon_rx_6000
Which review had the encoder stuff in? I was hoping the new video engine had improved things,radeon_rx_6000
"I don't know that they dropped the ball on bandwidth, seems more like NVIDIA basically bought the entire world supply from Micron.",radeon_rx_6000
[deleted],radeon_rx_6000
3070 stock has been very good the last few weeks (relative to 3080 and AMD LOL),radeon_rx_6000
LOL these cards have ran out or stock pretty much everywhere.,radeon_rx_6000
"Wonder why TPU got such a giant difference, hwunboxed was quite a bit less, but in general yeah the 6800XT is averaging lower power consumption.",radeon_rx_6000
"Similar results on https://phoronix.com/

Significantly better FPS/W than 3080.",radeon_rx_6000
"Neat, but in the end the vast majority of users really don't care about anything other than performance.",radeon_rx_6000
"It will matter in the future, when games start using more than 10gb vram. I would not buy 3080 for 4K unless I planned to upgrade in like 2-3 years(which I wouldn't).

And none of those cards were made to 1080p gaming. Even tho they win there, it's not their target.",radeon_rx_6000
"This. 3070 still seems like a better deal...only if you could buy one at MSRP. 3070 is supposed to be $809 in Australia, yet the most available cards are sold for $949 and more.",radeon_rx_6000
"Nope.  6800XT not worth it (and not available).  3080 worth it but still not available. 

Well crap....",radeon_rx_6000
Or even the bang for buck test with the 6800 and 3070 with and without SAM.,radeon_rx_6000
"You can always undervolt your 3080, 100 less Watts for identical performance

https://www.youtube.com/watch?v=FqpfYTi43TE",radeon_rx_6000
"Wow, AMD got fucking destroyed in raytracing.",radeon_rx_6000
"I dunno, there's no compelling ""6800XT is best for this"" 

Really it matches the 3080 for a 7% cost savings. Then it gets destroyed if you want to turn on any raytracing features.",radeon_rx_6000
"With what software was made those Opencl test? 
The other problem I see is with ROCM, that from what I read is almost unusable, only support a couple of OS and even then doesn't work properly. Looks like AMD outside of games is not a viable option yet.",radeon_rx_6000
cuda alone sways tons of buyers. amd as usual needs to step up their software game,radeon_rx_6000
LTT trying real hard here to justify the 6800 non XT.,radeon_rx_6000
"6800XT at the price of a 3070? At that price, it wouldn't be a hard choice for me to buy AMD even with the subpar software stack :)",radeon_rx_6000
In my part of the world amd cards are not even listed so I don't know the local price,radeon_rx_6000
Modded Bethesda games would probably do it.,radeon_rx_6000
"There are literally zero games where the vram size provides advantage to amd, it's actually the opposite, games with higher vram requirements favor Ampere due to better bandwidth. According to hardware unboxed we gonna see the benefit two years down the line. That review however was really different than the rest, felt like it made the 6800xt look way better than other reviews.",radeon_rx_6000
Isn't VR VRAM-intensive?,radeon_rx_6000
"Haven't we already kind of known this since the 3070 benchmarks? It was basically neck and neck with the 2080ti which has 3gb more, but doesn't really outperform it.",radeon_rx_6000
Exactly how i feel. I'm extremely happy that AMD finally has something to compete against Nvidia's high-end offering... And it does in rasterazation benchmarks. But its missing so many great features that makes it hard to sell...,radeon_rx_6000
"It's a decent tradeoff between the 6800XT and 3080, but I think for most people the 3080 makes the most sense.  For me personally all the extra features while having better performance in old games/openGL stuff like minecraft makes it a better deal.

NV's pricing isn't that good either tho so in absolute sense, $700+ for a GPU is pretty ridiculous anyway.",radeon_rx_6000
"IMO it’s still a great choice for someone who wants to game in 1440p.

All the stuff about encoding, streaming/recording, workspace usage, etc or 4K gaming isn’t super relevant for someone who just wants a high powered GPU to stick in a machine and game at 1440p.",radeon_rx_6000
"RDNA2 is like Zen2 in that respect but it lacks the price value. If Radeon can keep up the growth momentum, I’m pretty hopeful that the next gen will be amazing.",radeon_rx_6000
"It's the best AMD GPU since R9 290X. It's kinda like Zen 1, not perfomance leading but on good track.",radeon_rx_6000
Won't the markups on AIB be the same on both sides?  How does that close the gap?,radeon_rx_6000
What makes the encoder so bad on AMD? Is it behind Intel and Nvidia?,radeon_rx_6000
"I agree that it's price should be closer to the 3070, but that 16GB of video memory is going to make the price cut difficult. I think AMD should have just put less memory (8-12GB) into the card so they could make the pricing competitive.",radeon_rx_6000
"Nvidia announced SAM is coming to their cards and will work on older CPU platforms, not just Ryzen 5000.",radeon_rx_6000
">Once RTX 3000 cards can stay on shelves for longer than a day, AMD will probably start lowering prices in order to improve their value proposition.

I think you're right on the money with this one",radeon_rx_6000
"Nvidia did win out in that everyone is talking about how this card compares to a now nearly non existent $700 3080.  The only source for the FE is currently BestBuy in the US and I don't know how much they are restocking that card.  Newegg only has 3 additional models listed at $700.  There are an additional 2 cards at $730, and 3 cards at $740.  But the majority of the 21 cards listed sit at $750, which is the same as the mean price.  This is where I more realistically see the 3080 sitting.  By all accounts, Nvidias margins are thinner than they have been in a long time.  To the point that board partners are raging with the planned pricing on the RTX 3060.  Given the pricing expectations they are getting from Nvidia, they can expect to make a few bucks a card at best.

I am interested to see how pricing on the AIB cards are going to compare.  And like you said, how that pricing is going to change over the next few months.",radeon_rx_6000
AMD timed their launch for the Nvidia shortages. Great business decision imo.,radeon_rx_6000
"Hmmm, not as good as expected. Really just looking for best 4k/60 card once all the dust settles",radeon_rx_6000
This makes me cry more than buying an Asus TUF 5700xt before the reviews dropped.,radeon_rx_6000
Isn't the 3080 twice as fast as the 5700XT? This makes the 6800XT look only about 75% faster.,radeon_rx_6000
"wow, I was really curious how that was going to turn out in tests. But now I want to see if there's any difference between AMD's implementation and the normal version in the PCIE 4 standard.",radeon_rx_6000
No wonder they didn’t lift the review embargo until today,radeon_rx_6000
RIP my next-gen console raytracing dreams.,radeon_rx_6000
At 4k. At 1080p60 it's sort of acceptable in some games.,radeon_rx_6000
At same level in most cases to nvidia's first try and this is amd's first try,radeon_rx_6000
Do you trully care?,radeon_rx_6000
gigabyte already made it private hahahaha. They've showed it too early. Guess amd learned they've broke the NDA,radeon_rx_6000
It’s in 30 mins,radeon_rx_6000
"There is no good reason for AMD to actually limit SAM support, they've supported it on Linux for a while. Just needs the 64-bit PCIe addressing support and resizable BAR and it will do it. I have it enabled on my Z390 system with a 5700XT, but I know people also have it enabled on much older GPUs as well. If AMD doesn't enable it for all devices like they do on Linux, it's an artificial limitation.


I haven't down benchmarks because I'm lazy, so I have no idea how much it helped if at all.",radeon_rx_6000
Iirc nvidia said it would work in their cards from the 7th gen onwards for intel so you should be fine ?,radeon_rx_6000
"They're reporting 210w average and 285w peak gaming power draw, that lines up with Guru3d, GamersNexus, etc.",radeon_rx_6000
Gamers nexus was trashing the reference cooler.,radeon_rx_6000
"Actually yes, Optimum Tech did in [their review](https://www.youtube.com/watch?v=5bKaJDbKVt0&t=11s).  At least in their video, Nvidia looks way better objectively.  I'm actually surprised nobody else did this because it's important but I feel like it probably takes more work to get the side by side shots.",radeon_rx_6000
"I'd say it's somewhat worse than that. Ryzen 2000 was worse in gaming but better in productivity. 6000 series is the same with traditional gaming, but worse in both raytracing and additional features.",radeon_rx_6000
"Except, unlike Intel, Nvidia isn't a sitting target. Should be even better for consumers than Ryzen if these two are slugging it out.",radeon_rx_6000
"Yeah, looks like I'll sit on my 5700XT/3700X. No reason to upgrade, even with a B550 board.",radeon_rx_6000
"Id say this is ryzen 3000, it needs one more generation to finally be there. If rdna3 comes out next year will nvidia have an answer?",radeon_rx_6000
It's more that Nvidia gets better at higher resolutions than AMD getting worse.,radeon_rx_6000
"More VRAM wouldn’t help you in 4k though. 90% of games use less than 6GB of VRAM at 4k, and the rest of them still come nowhere near 10GB. So of course the 3080 beats the XT with its faster memory and a wider memory bus.",radeon_rx_6000
"Resolution is a red herring, it is about amount of math per frame, next gen games that use more compute will scale on Ampere even at 1080p. Ampere has more raw compute and more bandwidth to feed the cores.",radeon_rx_6000
">It's interesting that nvidia seems to win at 4k even with less vram. 

**There are no next gen games yet.** Right now you get close to 8-10GB of VRAM use on current gen games.

Once you will get next gen games then VRAM use will shot up into stratosphere",radeon_rx_6000
"You either die a villain , or live long enough to become the hero...again.",radeon_rx_6000
Isn’t their gpu still like at least 6 months out?,radeon_rx_6000
Intel's response: watch me release this new GPU on our cutting edge 10nm tech.  Never mind that we don't have good yields or any capacity.,radeon_rx_6000
"While this is true, availability is the real dimension here.

At RRP, the 3070 may seem like a better buy, but if in a few months the 6000 stock is good and you still can't find a 3070 anywhere in stock/at RRP then just by existing the 6800 wins. The opposite also applies of course.",radeon_rx_6000
I have a feeling AMD will definitely use the console optimisation to their cards advancement. Nvidia should really step up their drivers game or the gap will become even bigger with future cross platform releases,radeon_rx_6000
"The new consoles are using AMD hardware, so next gen console ports will be optimized for AMD rather than Nvidia.",radeon_rx_6000
"I think the MSRPs are a bit misleading, especially this generation. I always look at the sale prices for specific models. It looks like the 3080 FE and the MSRP AIB cards will be scarce going forward. If the actual sale price of most 3080 models hovers somewhere in the $750-850 range, while a significant portion of 6800XTs are offered at their MSRP of $650, the 6800XT suddenly becomes a lot more compelling.",radeon_rx_6000
"yeah its a no brainer and you should definitely take the rtx3080. Better driver support, better overall performance and miles better ray tracing. Thing is you just cant get a rtx3080 and if you can it will be hundreds of dollars more expensive. So the question shouldnt be amd vs nvidia but rather 'can you wait?'",radeon_rx_6000
LTT called it rubbish compared to Nvidia & unwatchable. https://youtu.be/oUzCn-ITJ_o?t=561,radeon_rx_6000
LTT featured it briefly just saying that it still doesn't look very good with lots of artifacting and sampling issues.,radeon_rx_6000
"a 3070 can already max-out most headsets, shouldn't be a problem with any of the above cards really",radeon_rx_6000
Gotta wait for Babeltech.  Unfortunately they're not popular enough to get on the manufacturers 'send cards before release on NDA' list.  I don't think anyone else even bothers?,radeon_rx_6000
At least with nVidia yes. HUB tested SAM with PCI-E 3 forced in BIOS and there was no difference in performance. AMD might extend the support to other CPUs and chipsets later down the line.,radeon_rx_6000
Nvidia said that it is. This is part of the PCI spec.,radeon_rx_6000
"For now AMD isn't supporting any non 500 chipsets. I expect this to change when nvidia launch their support for this.

However hardware unboxed tested their x570 mobo set to pcie3 in the bios and it worked with SAM at least it ran identical performance as when it was in pcie4 mode they said.",radeon_rx_6000
"Of course, it has nothing to do with PCIe revision. If they want to turn it on, they will. It's just a software implementation.",radeon_rx_6000
"Because MSRP is a persistent price, unlike current ones, which are temporary and vary day to day and place to place.",radeon_rx_6000
"That seems mostly fair, but

> + Equal or better rasterization performance to 3080 depending on the workload

looking through the reviews it seems more like ""equal or better or worse"" depending on the workload. E.g. in the Computerbase review the 6800 XT is 5% behind the 3080 in 4k rasterization.",radeon_rx_6000
"going by benches, most games it seems still perform better on the 3080 even at a modest res like 1440p even in pure raster. for example in the tom's hardware test 6 of the 9 games perform better on the 3080, but forza horizon made up the difference in the 9 game average by performing far better on the 6800xt. 

this could be a statistical outlier, or it might be indicative of something we see from microsoft first parties who have been key in the development of RDNA 2.",radeon_rx_6000
"> Equal or better rasterization performance to 3080 depending on the workload

  


Not really. At 1080p, there's a cpu limitation that's equalizing performance more often than not. At 1440p the 3080 starts to pull slightly ahead with the 6800xt trading blows. At 4k the 3080 is slightly-to-moderately ahead most of the time with a few outliers.

AMD hobbled the performance at 4k with the narrow memory bandwidth, and their cache isn't helping out with that. With a proper bus width, it would probably be trading blows with the 3080 at 4k instead of losing by a bit most of the time.

Still, it's a pretty competitive card if you don't care about dlss or Ray tracing. I think this is to GPU what the ryzen 1000 series was to CPU - a good start, in need of some refinement. AMD is going to have to keep pushing because Nvidia isn't going to be on that god-awful 8nm node forever",radeon_rx_6000
"I'd also put a con for driver worries- Nvidia has a history of consistent driver updates for new releases and fixes, AMD really needs to step up in that regard.",radeon_rx_6000
[deleted],radeon_rx_6000
Still think I'm going with a rtrx 3070 over the 6800 for the CUDA and video encoder. I do want to see how their power draw compares in games especially with custom cards.,radeon_rx_6000
"Depends on the game. Some nothing, others quite big like bigger than 15%. Which is weird.",radeon_rx_6000
"HUB/TechSpot did show a massive difference in Valhalla, leading to insane 40-53% performance lead vs the 3080.

https://www.techspot.com/review/2144-amd-radeon-6800-xt/

IMO it's a definitely interesting feature and deserves more exploration.",radeon_rx_6000
it also doesn’t matter much since intel and nvidia will have it too,radeon_rx_6000
"Some tests showed up to 11% performance increase, in Hitman I believe, it really depends on the game.",radeon_rx_6000
[https://www.techspot.com/news/87596-nvidia-working-competing-technology-amd-smart-access-memory.html](https://www.techspot.com/news/87596-nvidia-working-competing-technology-amd-smart-access-memory.html),radeon_rx_6000
"Yes, SAM differences are negligible.

If I may to express my opinion, would be one of the less-known selling point of RX 6800 XT series is its much higher VRAM?

My point in this post is that questioning whether the RAM type is that much different compared to the capacity. RTX 3080 cards have GDDR6X 10 GB VRAM while RX 6800 XT cards have GDDR6 16 GB VRAM. Would 6GB extra of VRAM will outclass the GDDR6X VRAM of Nvidia? Curious on your thoughts about their massive VRAM differences at the card placed at the same price point.

I have watched GNs videos on the performance offered by these (AMD) cards and I see little reason to buy them; yet after watching HUs videos, I am at a fence due to the VRAM on AMD cards were literally double than the similarly priced RTX 3000 series. Additionally, their closing statement was ""8 GB VRAM would not be enough"" for future gaming and their price/FPS performance had shown that RX 6800 XT holds an advantage compared to the equivalently priced Nvidia cards.

I would like to hear on your opinion on this... I'm no expert on the hardware side; I hope by posting this, I would have my ignorance beaten out of me from just blindly saying ""bigger VRAM is better"" and perhaps learn better on how to interpret the benchmarks offered for such products.",radeon_rx_6000
Now we can confirm what many thought with how silent AMD was about raytracing during the reveal.,radeon_rx_6000
"Not sure if you have done it yet but EVGA is putting people on a waitlist if you took Auto notify on their site.


I did this the day of release and they sent me an e-mail about 2 weeks ago now saying they reserved one for me I didn't even know that was the case",radeon_rx_6000
"Kind of, the Hardware Unboxed benchmarks suggest the 6800XT is better at lower-than-4k resolutions. I think it's a good thing that we now seem to have equivalently performing products but with a clear place for each. If you want more VRAM and less power usage, you go AMD. If you want better encoding, raytracing, CUDA etc, you go Nvidia.",radeon_rx_6000
">  in minecraft 4K RTX on DLSS ON, 3080 get 87FPS. In the other hand, 6800XT get 13.9 FPS

Holy crap, that's practically non-existent lol",radeon_rx_6000
Even with with double the memory it performs worse at higher resolution.,radeon_rx_6000
"> pretty much perform the same as 3080 in 1080p and 1440p

Not the same, from what I've seen. The 6800XT performs even better than the 3090 in some games at those resolutions, according to GN's review.",radeon_rx_6000
You can turn DLSS on while RTX is off correct?,radeon_rx_6000
">And in RT performance and with DLSS on

who cares about those",radeon_rx_6000
Feel a bit salty about SAM (Zen 2 CPU) but with the Linux support I'll be buying this. Excellent stuff.,radeon_rx_6000
"Well, even if you had the money, you probably still couldn't get it...",radeon_rx_6000
"Im getting the best card i can get in my 250usd budget come first week of December.. anything is better than my 1050, hoping new mid rangers are announced soon so the older cards get a bit cheaper by then",radeon_rx_6000
"I am kind of in the same place. It is very promising. And it gets them really close.  Next gen could be a lot more interesting.

Assuming it is ever in stock, I will probably go 3080.",radeon_rx_6000
"Realistically all AMD needs to be on the pricing front relative to the 3080 is ""in stock."" $650 6800xt vs $700 3080 does seem to give the advantage to the 3080, I agree. But if you can't buy the 3080 while you can buy the 6800xt... It's still a great card that I'd be willing to buy if I was looking at several months extra to get a 3080.",radeon_rx_6000
"It appears the TLDR of the card is, great classic/standard GPU performance without RTX/RT/DLSS shenanigans - which the vast majority of games out there these days don't utilize those features (yet).",radeon_rx_6000
Depending how much value you put on RT currently.,radeon_rx_6000
">  The 6800XT needed to be $100 cheaper than the 3080 to be compelling.

right now it is more than $100 cheaper in germany. if we talk realistic prices and not bullshit msrp prices (how do you guys keep using the msrp prices? theyre bs, you wont see them for a very long time), an available 3080 here is 950€, an available reference 6800 xt is 830€. that is if you get to see an available 3080, its rare you do. if amd manages to produce some decent stock, they could reduce prices further and simply give people no other choice.

i dont understand why people here even argue about this because its month 2 of amperes release and you still cant find any 3080 and if you do, theyre heavily overpriced. so what makes you think itll be any better? as far as we know its thanks to samsungs failed 8nm and what makes you think thatll be solved quickly? if anything, the tsmc rumors makes you think that ampere on samsungs 8nm wont be there for long and theyll be on tsmc as soon as they can.",radeon_rx_6000
">there will be little reason to go with the 6800XT over the 3080.

Except if you dont want furnace and extra 100W power consumption?

And if you dont care about RT, same as you never did? 

Dunno, I feel like there are so many comments saying the same thing, but to me it reeks of nvidia bias. 

AMD is competitive in high segment after like decade, beats nvidia in power efficiency... and enlighten redditors like this chlamydia will go how its meh...

jesus christ",radeon_rx_6000
"I'm not an expert, but can you explain this if this is because  the difference in architecture between the two cards(ray accelerators vs. rt cores).",radeon_rx_6000
">Will it ever reach the 3080 with ray tracing? **Probably** not, but you'll have gaps closer to non ray-tracing scenarios.

Considering the difference in hardware, you didn't need to use the word probably.

> push out their answer to DLSS, 

Hopefully, they push out something competitive. I don't think it will perform as well as DLSS though simply because it doesn't have dedicated hardware to drive it.",radeon_rx_6000
"From Sweden. Aside from two models of the 3080, most are around the 950+ euro mark. The 6800 XT was supposed to be  690 euro, but all retailers sold it for 785, almost 100 markup. Everything sold out in less than 5 min. At that price is 200 euro less than a lot of 3080, which have models going 1100 euro.

So here,  the 6800 XT is the clear winner.",radeon_rx_6000
"Even if one looks at non-DLSS performance 3080 is about 2x better in averages. 

If AMD had a similar technology to DLSS it would have to work hard. I think it's safe to say that they won't be able to account for the difference.",radeon_rx_6000
"5x AS fast NOT ""5x fastER"" (that would be 6x as fast).

Also, it is interesting to note that without DLSS the 99% min fps is 14 for both 6800XT and RTX3800.",radeon_rx_6000
"Agreed. At those price points, it would *not* make sense to **not** go for the RTX3080. Availabilities aside.",radeon_rx_6000
"I think its hard to make a case just yet, i'd wait a month or 2 for new drivers. Amd has shown in the past that their cards can improve from day 1 performance. And Nvidias habe been out for a while now.
Bit its still going to be hard for AMD and i'll likely go for nvidia as well sometime next year.
Really hoping for AMDs next gen. 
If they improve like they do on their cpus this could get intresting.",radeon_rx_6000
"The price comparison is misleading right now. The FE is only available from BestBuy currently, and Newegg only lists 3 additional cards at that price that is heavily dependent on availability.  The more expected price to pay for a 3080 seems to be $750.

To really come to a conclusion based on price performance, we are going to have to wait to see what the board partners cards look like next week, and really comes down to availability over all.

As far as features, we are going to see DXRT expand at a rate far surpassing DLSS implementation, especially as games get developed for the new consoles, which may see some improved performance for AMD going forward.  As admittedly awesome as DLSS 2.0 is, it is only in around a dozen games and because it is proprietary and requires licensing and active involvement and compute time from Nvidia, will continue to be slow to roll out and will never be a 100% implemented feature. There are currently only a couple games I would play that would support it.

However, if you are a creator. I would still go Nvidia.  There are enough small features that just add just enough in every way to be an advantage, not to mention encoding performance and quality.  AMD may have segmented off that part of their development a little too much on their GPUs.  Seems like I would still go AMD CPU either way though.

Where I sit, as a non creator, 1440 UW gamer. It's up to AIB pricing and availability.  If there is a >$50 price difference for AIB cards, and there is availability.  I'll likely go 6800XT.  For the hand full of times I might do creative work, the 5900X I plan on getting will be enough to handle the infrequent workloads.  The advantage Nvidia has wouldn't really be worth spending the extra money on.",radeon_rx_6000
The 3080 will be the fine wine as ray tracing and DLSS get integrated into more and more games imo,radeon_rx_6000
"Look at guru3d frame rate spikes. I have basically zero doubt they are anything other than cache misses reaching out to memory. Tweaks to the code to keep stuff more localized should give big boosts to frame rates. I'd bet 10-15% improvements are easily possible in some titles with 60-ish millisecond worst case frame times.

Who knows if raytracing improves. Unless there's cache thrashing or blatant Nvidia favoring algorithms/optimizations, I doubt it'll improve. There's just not that much to raytracing itself.",radeon_rx_6000
That was fast...this has been one unproductive work day today.,radeon_rx_6000
"I don't know where you come from, but where I am hot cakes are a lot easier to get than video cards these days.",radeon_rx_6000
"> they will still sell out like hot cakes

Already sold out, looks like stock was very low for this launch.",radeon_rx_6000
The higher end cards are hot garbage. Ray tracing performance is bad and still no good AI upscaling like NVIDIA. If you want to go with great non-ray tracing 1080-1440p performance the lower end cards or even  last gen are absolutely sufficient,radeon_rx_6000
"For me, the ray tracing performance being dogshit severely hampers these cards competitively. I was expecting 2000 series level ray tracing. It was ***WORSE*** than that. It's nice that the raster performance is so good, but even with that said there is just so much missing from this card that the slight drop in price doesn't justify it.

1. Ray tracing performance is night and day. We're talking more than 2x the performance if you use the competition.

2. Still no DLSS competitor in sight, and NVidia just added 4 more games to their solution.

3. Still no real answer to NVidia's NVenc.

I really wanted this to do well because I run Linux, and NVidia's cards are simply not well supported. But it just doesn't matter if the differences are so stark.",radeon_rx_6000
"> Regardless of the mediocre ray tracing in these cards, they will still sell out like hot cakes. All AMD needed to do is make an Ampere competitor for less $$$ with some ray tracing, which they did.

Reasoning? None...

I don't buy new GPU's for +400 Euro to play games at below console settings so why shouldn't I care about raytracing performance?",radeon_rx_6000
I got one too :),radeon_rx_6000
"Steve did test a bit with SAM, and gains depend on the game. I'd assume the full update will be once he tests everything with the 5950X.

He didn't test with R A G E M O D E tho",radeon_rx_6000
"LTT Tested with and without it, and it depends from game to game.

And Also it seems that SAM it's just a Microsoft feature ...",radeon_rx_6000
[deleted],radeon_rx_6000
"This is the 2000 series right here. They are on NVIDIAS ass, it just remains to be seen if they can overtake. They will probably sell every card they can stock as long as NVIDIAs supply is scarce, and after that stops they will probably drop prices. 6800XT at $600 would be far more competitive.",radeon_rx_6000
"I was thinking Nvidia might go for $999 on a 3080 Ti, but with the numbers on 6000 series + leaked specs indicating it’ll be better than the 6900XT in almost every way, that’s looking less and less likely. $1200 MSRP again, here we come...",radeon_rx_6000
"Dirt 5 is the latest RT title on the TechSpot review, and I believe it's AMD sponsored. The performance hit on that is actually less than Nvidia. 20% vs 23% for Nvidia.

To me that shows something I was kind of suspecting. AMD is better at handling light/subtle implementation of ray tracing than Nvidia, but is overwhelmed by heavy amounts which plays in Nvidia's favor.

I haven't watched more reviews than the TechSpot one yet, but I'd really like to see testing at ""Low RT"" settings vs Nvidia. Curious if there is more recent titles that prove AMD is better at handling light amounts of RT.",radeon_rx_6000
"I’m interested in overclocked 6900XT perf at this point. Probably won’t be enough to upgrade from the 3080, but maybe enough to invoke a tinge of buyer’s remorse.",radeon_rx_6000
"Just the fact that they match on rasterization and go between 20 and 30 series on RT is impressive to me personally. I hope this is their ""Zen"" moment in some way.",radeon_rx_6000
"To be fair to AMD, it is their first run at RT. Nvidia had the same performance problems with their 2000 series but managed to bounce back. I’d say this is a step in the right direction. At 1080 and 1440, this is really the first time we’ve seen AMD compete and even beat NV through raw horsepower. Nvidia is heavily investing and “all-in” by having dedicated components for RT in their cards. I guess they (amd) need more time to adapt RT technology that is on par with the competition.",radeon_rx_6000
"Yep it’s embarrassing. But worst thing is that the new consoles have exactly the same issues. Series X and PS5 are beasts (for consoles) for normal rendering but ray tracing tanks their performance.

What a let down",radeon_rx_6000
I know Hardware Unboxed did.,radeon_rx_6000
LTT did,radeon_rx_6000
"MSFS will probably be way ahead on ampere as soon as DX12 is released, which will happen around the console launch. They will enable Raytracing and DLSS with that update. I run it with a 3080 paired with a 8600k@4,7GHZ on 3440x1440 atm and get around 50-60 fps on ultra everything (apart from  motion blur, who wants that)? I did not increase the lod via config though, which I'm planning to do.

Still waiting for my 5800x so I undervolted the gpu by around 100 mv since I still have my 550 watts psu installed until then.

So if you don't want hope for amd to step up their Raytracing and DLSS equivalent Ampere is the better choice atm.

This will be even more important for VR since this will more or less need DLSS.",radeon_rx_6000
"No, that's the card youtubers got. It needs to be handed over to another youtuber that runs that benchmark until it shows up a second time.",radeon_rx_6000
"The driver fiasco from the Navi generation does leave me with a bad taste; their driver issues back in the 2019 was so bad, it was enough to dissuade me from buying RX 5700 XT and went with RTX 2070 Super instead.

Ampere series cards are not free from driver issues either, but AMD graphics card drivers are infamous for being ""bad"" for longer than it needs too. While Nvidia drivers are not problem-free (case in point... stuttering in SteamVR in version above 450, and I mainly use VR for playing games), AMD drivers are rife with problems... so much so that RX 5700 XT was ""held back"" until mid-2020s that their performance were shown to be equal or even exceed RTX 2070 Super (aside RT).",radeon_rx_6000
"yeah 6800 seems like the card to go for , though it seems like drivers need some work. My hope is to get a decent aftermarket card in 3-5 months",radeon_rx_6000
"> I have one game that supports DLSS, and I had to disable it to prevent crashing (known issue). So at least a generation away from me caring about this feature.

What if another game you want to play or multiple that come out over the next few months (like Cyberpunk or Watchdog for example) use DLSS? It seems like a very strange conclusion that DLSS is at least a generation away for you just because you can't utilize it for the games you own today.

> The cards in my segment aren't powerful enough to utilize ray-tracing in most games, so I'm not worried about that for another generation at least.

That is the thing, they are with DLSS enabled...",radeon_rx_6000
"One of the lastest games to come out with dxr support is dirt 5, with dx12 ultimate being used in tandem with the console implementation has amd ahead of the 3080..
Like they say its about dev implementation more than anything, i dont think there will be much of a difference in the future..dlss is more of a miss than their rt implementation",radeon_rx_6000
"Well my only caveat is that Nvidia promised a ""great deal of rt support ""for upcoming games when turing launched and only like three total games ended up supporting it. 

I still think we are a gen or two away from rt being really good and wide spread.",radeon_rx_6000
"I don’t have a source, but I heard that their GPU reviewer was affected by wildfires in California and is unable to do any testing.",radeon_rx_6000
"I honestly think the prices are only like they are because AMD priced in the the bad supply situation. As long as NVidia cards (at MRSP) are rare, might aswell make some extra bucks. They can still decrease the price in a few months.",radeon_rx_6000
idk if okayish is fair for this gen. In traditional gaming workloads they're arguably amazing. I suppose it depends how much weight you put on ray tracing.,radeon_rx_6000
"In the case of the 3070 and 6800, spend less to get significantly better RT, DLSS, encoder, etc. It’s a tough sell, but at least theyre decent cards",radeon_rx_6000
I you get slower card and pay more lol.... Look at the charts of the new games.,radeon_rx_6000
"It remains to be seen how raytracing is going to perform in games made with AMD cards in mind. Consoles have AMD hardware, and they have raytracing, so it should work in some way.",radeon_rx_6000
"Majority of people are playing at 180p and 1400p, AMD is just better at these resolutions.Also we will have to wait for games with AMD-RT optimizations to come out.Every game is gonna have RT like you said but you forget that the RT on those games will be optimized for RDNA2 chip in consoles.",radeon_rx_6000
"The problem is that RT still comes with such a massive performance hit that in majority if not all realistic GPU + resolution combos you probably just want to turn it off. I have RTX 3070 and at 1440p I simply prefer not using RT in any of the titles as I'd much rather have the extra fps. Once we start seeing above 100 fps even with raytracing on, I'll start considering it.

Some might argue that a few titles with RT + DLSS 2.0 will have quite decent performance and I guess that's fair, but how many games is that really? Control, CoD and soon Cyberpunk, I don't even know any others. But honestly even in them you'll be choosing between 60-80 fps and 120+ fps in most cases. It's just not worth it in my opinion.

Obviously I rather have better RT performance than worse, but still I have to side with people saying that RT performance doesn't really matter much in practice yet for this generation of hardware. Maybe there's gonna be a rare exception or two, but typically you don't want RT on whether you have geforce or radeon.

DLSS however could be a big deal if we get more games with DLSS 2.0 implementation and it's as good as titles like Death Stranding or Control.",radeon_rx_6000
"You don't need ML accelerators to upscale an image.

Firstly you can do DL upscales on regular GPU ""cores"". We don't know how much the tensor cores actually help DLSS besides some vague wording around improvements done in v2.0 over v1.0, so Running it on regular GPU ""cores"" might not be too much worse but who knows. Maybe big Navi has a hardware quirk in its ""cores"" that enables decent performance in this task?

Secondly, we don't know whether DL is actually even used in AMD's alternative. From the very limited amount of information they've given, it could just as well be algorithmic.  
Actually, since they've never specifically said it'd be DL and a big tech company missing out on a chance of saying that their new technology is ""powered by *AI*"" seems pretty unrealistic in 2020.  
It wouldn't be the first time an algorithm beat DL (see DLSS1 vs. regular interpolation + CAS) but I'm not aware of an algorithm that can hand hold a candle to DLSS2's upscale quality and especially not its temporal AA.  

Since AMD has next to no footing in AI anything, I wouldn't expect them to deliver a decent DL DLSS competitor, even if they tried (much less one that works well on less suited hardware). Kinda has to be algorithmic at this point.",radeon_rx_6000
"DLSS does not require Tensor cores.

Tensor cores are for learning purpose. DLSS does that offline via superservers.",radeon_rx_6000
[deleted],radeon_rx_6000
I would be willing to bet it doesn't provide as much of a performance uplift since AMD is going to be reusing other hardware instead of using otherwise unused hardware like Nvidia.,radeon_rx_6000
Did u ever buy a gpu at release?,radeon_rx_6000
"Couldn't agree more with fuck 2020.

Was upgrading my PC for my 30th in September and in the UK I haven't seen a single retailer in stock of any 3080 card for less than £1,200.",radeon_rx_6000
"U do realize that control
Heavenly preffers nvidia cards
That game is basically a nvidia tech demo",radeon_rx_6000
"It's pretty weird that they've benchmarked Control multiple times over the last 2 years but somehow didn't feel it was worth including over DiRT5, a game from a series that has traditionally had [unusually high performance on AMD GPUs.](https://static.techspot.com/articles-info/1791/bench/DiRT.png)",radeon_rx_6000
I'm actually kind of shocked how biased they were and how much they praised the 6800.,radeon_rx_6000
Damage control of what exactly?,radeon_rx_6000
"Yep, everyone memed on Nvidia for ""RTX on, FPS off"" but when AMD's RT solution hits FPS even harder than the 20 series they deserve flak for that, not a pass.",radeon_rx_6000
"Their review is also the only one I have seen so far that puts 3080 behind at 1080p and 1440p, in overall averages. In every other review, 3080 leads by a small margin.

For instance, according to techpowerup: 3080 leads by 6% at 1080p, 4% at 1440p and 6% at 4K. While HUB review suggest  3080 trails by 6% at 1080p, trails by 3% at 1440p and leads by 5% at 4K.

Edit; I went a head and looked at techspot review (which is basicly a written HUB review). And three of the games 6800XT lead 3080 at 1440p, in the text they claim there is a cpu bottleneck. Lol what the hell, than your benchmark is not accurate.

https://www.techspot.com/review/2144-amd-radeon-6800-xt/",radeon_rx_6000
Yeah I don't know why but the hardware Unboxed videos when it comes to AMD products always different than other channels. It is so weird,radeon_rx_6000
"Man that review was tough to watch. I usually like HUB reviews but it was so obviously biased this time around. It's the same thing every time they discuss non-rasterisation related features like RT or DLSS. There's this elitist attitude about it where only rasterisation performance matters and the rest of the suite is completely irrelevant. RT might not be widespread just yet but the new consoles have it and it will spread to other games but DLSS is real and the benefits that comes with it are real.

Regardless of the overzealous marketing and feeding off meme culture, LTT's review is by far a much more complete review of the product in its entirety. GN still have the most thorough review of performance across the various features though.",radeon_rx_6000
"> To me it at depends on pricing, availability

so neither",radeon_rx_6000
At normal price or in general?,radeon_rx_6000
I just want a 3070,radeon_rx_6000
Dlss alone makes nvidia better for Cyberpunk.Add RTx and yea.,radeon_rx_6000
"TPU 1440 average has it a few % under a 3080, it seems mixed tbh. Also clear that Ampere has a utilization issue at 1080p, those shaders are not being used properly.",radeon_rx_6000
The 3090 doesn't start stretching its legs until 4k.,radeon_rx_6000
"but the weird thing is Sony wont support 1440p output because ""nobody"" wants/to few that want ot it and 1440p is exactly rdna2s strong point.",radeon_rx_6000
"Not really a shock there, the 3080 and 3090 are more suited for higher resolution play. Look at the 4K results.",radeon_rx_6000
">	Reality: Ampere is VRAM constrained

If this were true their lead wouldn’t widen over AMD as texture settings and resolution went up. I really cannot fathom how people are seriously typing out that Ampere is VRAM constrained lol.",radeon_rx_6000
"I don't think that ampere is VRAM constrained, plus games don't need that much VRAM (allocation of VRAM reported by most tools is not actual usage/requirement), despite whatever nvidia says, 8K gaming is not a thing this gen and pretty much no one cares about it. SAM and the someday-upcoming nvidia pendant to it as well as directstorage - which certainly both AMD and Nvidia (""RTX IO"")  will utilize - directly reduce the amount of VRAM required as well I imagine. These features make it possible to use less VRAM or use it more efficiently because devs can write more quickly to it and thus use it more flexibly (less ""cost"" in terms of time/complexity of memory management), e.g. only load into vram what they actually need at a given time or preload things faster instead of allocating a huge amount of VRAM up front and loading all kinds of stuff into it  ""just in case"". The speed of gddr6x should help with that too a little and maybe PCIe 4.0 will make a bit of a difference at that point compared to 3.0 within this gen.

Back in 2002 with Geforce4 there was a ruckus about whether to go for a 64MB card or 128MB one. They just released both. It wasn't an issue with 64, but there already were games that could benefit from more and obviously, look at where we are now, 64/128 MB made a difference then, but this right now just doesn't for the time being and I don't think it will within the next 3-5 years.

For 6800xt vs 3080 - I'd get any I can before none of them and given the choice an rtx 3080 in regards to RT, 4K, DLSS, streaming and something akin to SAM not being exclusive to AMD (though if petty enough, they could lock it on chipsets/cpus for AMD GPUs only). For 1080p/1440p and competitive gaming only without streaming, a 6800xt seems to be a better choice and it's a bit cheaper, so that's nice - though for 1080p an older even cheaper GPU might be good. I got lucky with nvidia at launch and will stick with it, but it's really nice to see that AMD is right there and nvidia can't slack in the near future. I bet AMD will also catch up on RT performance and DLSS.",radeon_rx_6000
"The 3070 should have plenty of vram for 1440p.  If I could get an FE card for MSRP, that's what I'd go with.  But at this point, all the AiBs are so expensive.  Even a lot of the dual fan cards are pushing $550.  3 fan cards seem to be like $600.  At that point, you're almost competing with the 6800 xt.",radeon_rx_6000
It is likely that next gen bandwidth requirements would push that difference down to 1440p too,radeon_rx_6000
"You’re not going to get +50% from drivers and “optimization”. Even if you could it still uses the DX12 RT API, which means Nvidia would also benefit. If you care about RT buy Nvidia lol.",radeon_rx_6000
"Translation of what really matters:

>As a separate observation, the new Radeons and their drivers work completely smoothly and stablely during the test period. Recently, beta drivers pre-delivered to the media at launch may have contained bugs in features such as overclocking, but this time everything worked normally.",radeon_rx_6000
"I only watched HUB so far. 6800xt is faster than even 3090 on 1080p 18 games average, faster than 3080 at 1440p and and slower than 3080 at 4k",radeon_rx_6000
"Don't forget Nvenc, AMD just locked themself out of a lot of streamers AND a massive chunk of the VR market. My 5600xt isn't even able to run my Quest2 due to the stone-age encode, and chances are that the 6000 aint gonna be any better so far...",radeon_rx_6000
Power consumption is a pretty big pro for these AMD cards and a pretty big sore spot for Ampere.,radeon_rx_6000
The real dollar question comes in January. I'm betting AMD can drop prices a lot more than Nvidia because TSMC N7 yields are great (0.09 defect/cm^2) while Samsung's process sucks (worse than 0.5 defect/cm^2 which is the industry standard for decent and 5x higher than TSMC).,radeon_rx_6000
"I dunno if I'd say nvidia is better overall, the general trend is more like nvidia is better for 4k or raytracing but AMD is better for 1440/1080p and offers much better performance per watt.

Given that 4k and raytracing aren't exactly widely adopted at the moment it seems to me like AMD is going to be better for most people but nvidia is going to be better for people with more high-end setups. The differences are pretty marginal outside raytracing though, honestly if you're buying right now availability will be the main factor as you said.",radeon_rx_6000
"Yeah my thoughts are the following: Would I pay ~50$ more for a GPU that trades blows in traditional rasterization with the cheaper one but has a better history with software and has better features (CUDA, DLSS, DXR performance, NVENC you name it tbh).

I'd certainly do, but not everyone would.",radeon_rx_6000
"Yea DLSS 2.0 jump on every game they make it available for is huge, literally doubling the fps on some cards in some games",radeon_rx_6000
The key is performance uplifts without sacrificing visual quality. That was the major breakthrough with DLSS 2.0 in most cases.,radeon_rx_6000
"> With SAM and RAGE mode 6800XT pulls ahead in <4k

1080p benchmarks are for CPU's.  The % difference between some of those is nothing.  480 vs 492 fps isn't a win when you get trounced at 4k and raytracing with or without DLSS.",radeon_rx_6000
Why productivity not?,radeon_rx_6000
"I'm glad I don't buy hardware @ launch.  

 ouff",radeon_rx_6000
"Unless you somehow found your 3080 at msrp somewhere, I'm not sure you, if you are the person to complain about pricing here mate",radeon_rx_6000
I wish I could find a 3080. Where did you get yours from?,radeon_rx_6000
It was set to private when I checked your link. 1330utc,radeon_rx_6000
Already removed lol,radeon_rx_6000
This is the way,radeon_rx_6000
Yeah I feel like there's a big AMD vs. NVidia battle going on since they're comparable but stock is going to be the deciding factor for the vast majority of people.,radeon_rx_6000
[deleted],radeon_rx_6000
[deleted],radeon_rx_6000
"Where are you seeing 230w? GN is showing 293w which is right in line with most reviews.

That isn't impressive power efficiency at all. AMD has a huge node advantage, a considerably smaller die, no GDDR6X (80w alone) and is still only managing a 25w efficiency advantage.",radeon_rx_6000
"Has any gamer ever bought a gpu for better efficiency, though?",radeon_rx_6000
"Tell me, where does Pibb fit in here?",radeon_rx_6000
"Steve from either Gamers Nexus or Hardware Unboxed summarized it well, he said for 1080p and even in most cases for 1440p the RX 6000 GPUs scale better than Ampere but at 4k Ampere is still better.",radeon_rx_6000
I'd argue 6800 is better if your goal is 1440p 144fps.,radeon_rx_6000
Just ordered my 3070 last week it's a relief albiet a sad one. I might have cancelled if the 6800 was unusually better.,radeon_rx_6000
"As someone who was sceptic as well, having recently played through Control with DLSS 2.0 enabled, it is actually a huge difference maker. If it gets widespread use, it is enough to base the purchasing decision on it imho - and with the next iteration of Unreal Engine having support baked in natively, I have no doubt there will be tons of games supporting it.",radeon_rx_6000
"> Honestly AMD should come out and state they kill Nvidia at price to performance for GPU passthrough by supporting SR-IOV on consumer cards since we know Nvidia would never do that as it would make their over priced quadro cards pointless.

I had to lookup what SR-IOV is and ""SR-IOV is a hardware standard that allows a PCI Express device – typically a network interface card (NIC) – to present itself as several virtual NICs to a hypervisor."" honestly sounds really specifically niche that you could only hope to ""wow"" my network admin, maybe?? with that featurelol (Doesn't PFsense do nearly the same thing almost) For the most part AMD would get laughed at, considering these cards main use is gaming. Vs DLSS which is equivalent to magic afaik and probably even more important than ray tracing. 
Look at this, wth??
https://videocardz.com/newz/nvidia-enables-dlss-in-four-new-games-with-up-to-120-performance-boost
Magic.",radeon_rx_6000
I bought an RX 6800 XT for the same price as any 3070 I could find. Winning 2020 is just about fulfilling orders at MSRP these days..,radeon_rx_6000
"Here where I live, the price differential is 100€ in favour of 6800XT vs RTX3080 and the average FPS is c. 4% lower. XT has the memory advantage (16GB) for later/bigger/4K-Ultra titles, while RTX is the proprietary RT champion, but in some titles (Dirt 5 RT) XT is better (go figure).

To me it boils down to actual prices in real shops (not RRP) and availability.

Personally I'm in no hurry and can wait for AIB boards and even more info on the rumored RTX3080Ti w/ 20GB VRAM.",radeon_rx_6000
Are you implying that the 6800XT cannot be undervolted?,radeon_rx_6000
"The irony in you saying not to buy a 6800 XT outside of very specific use cases, and say the reason is because of RTX and DLSS, both only available in very specific use cases???",radeon_rx_6000
Better 1440p performance too in many cases. 4k is where it's not amazing.,radeon_rx_6000
"Its ok. Not great, but ok. So if you dont need CUDA or DLSS and dont care that much about RT the purchase decision is rather about availability.

The value for MSRP is pretty meaningless if you can only get the cards for a good 200$ more, if at all.",radeon_rx_6000
"DLSS is in like 2-5 games at the moment.

RTX also is in like 6-8 games mostly producing really invisible effects for huge cost in performance.

So yeah in my book 50$ less is actually something great.",radeon_rx_6000
Undervolting 3080 will result in lower performance than 6800XT according to some users here.,radeon_rx_6000
Next generation is going to be glorious for competition.,radeon_rx_6000
RT isn't that far behind in the 2 titles that are recent enough to have gotten optimization for RDNA2 though. It's ahead in RT in Dirt 5 and just slightly behind in Watch Dogs: Legion.,radeon_rx_6000
MSFT and Sony won't let them do that.,radeon_rx_6000
"> It should have been 11GB for the 3070 and 12GB for the 3080.

Thats not how gddr memory works, though. Both Samsung  and Micron makes gddr6 chips in 8 gigabit and 16 gigabit packages. So you have to double the vram if you want more.",radeon_rx_6000
I think the amount of VRAM will likely be fine for the given performance. Especially if things like direct access IO starts really getting used by devs.,radeon_rx_6000
With GDDR6X you don't need as much memory as you would before. Agreed that 8 GB GDDR6 on the 3070 isn't great. Even AMD shows that high speed storage helps a ton. Their high speed cache overcomes the lack of G6X at lower resolutions while at 4K you can see the limits of the smaller volume of cache where their G6 VRAM plays a larger part while the 3080's G6X VRAM allows it to pull ahead.,radeon_rx_6000
Is there any real world example of games even coming close to needing that much?,radeon_rx_6000
"Yea amd made great cards,  its just i think they should be little cheaper for lack of features",radeon_rx_6000
You like your women upscaled with real time ray tracing?,radeon_rx_6000
">6800XT is surprisingly meh

It is on pair/trading blows with the 3080 on most benchmarks, with AMD coming from a historical disadvantage, on a lower wattage and price point. 

What's ""meh"" about it?",radeon_rx_6000
"They were like fuck it, we are out, stop f5ing our website.",radeon_rx_6000
I wonder if AMD didn't like what they did with Zen 3.,radeon_rx_6000
"No it's a good thing they aren't.

People have to wait until next year for their ryzen 5k chips to finally ship like me and we all ordered the second it went up for sale on their site; I also recently had a monitor take a month and a half to get to me after ordering from them.

They don't have anywhere the capacity to handle any demand it seems, so a good idea on their part to just not sell them.",radeon_rx_6000
"First post in Romanian.

Alright boys I'm going in.",radeon_rx_6000
What was the problem? I didn't think they were any different from the other LTT reviews.,radeon_rx_6000
"1. Why are you getting angry over a benchmark video?
2. I didn't see any problem?",radeon_rx_6000
"It requires game integration probably. They have these thing called FidelityFX, and they are still upgrading and working on it.

It is coming. We need to wait, probably two months.",radeon_rx_6000
They bashed RT so much that they are now afraid to admit it's going mainstream in this new gen. I don't think they expected next gen consoles to have it,radeon_rx_6000
"Navi 22 will likely be clocked higher than Navi 21, just like the PS5. So you can't judge it by CU count.",radeon_rx_6000
Leaks put Navi 22 at 12.8 TFLOPS vs 13.9 for the 6800. The big questions will be cache size and memory bus width.,radeon_rx_6000
WAIT FOR BIGGER NAVI,radeon_rx_6000
"> If these cards are actually available

There are a few big retailers already posting they received no stock at all from AMD at launch. 

https://twitter.com/hardwarecanucks/status/1329067340642836487",radeon_rx_6000
DLSS and RT can definitely be a big dealbreaker depending on what games you play. It's a 4x difference in fps for Control at 4k for DLSS and 2x the performance in Minecraft with RT.,radeon_rx_6000
Aside from the fact that AMD's launch was just as bad as NVIDIA's - pretending that RT and DLSS alone are not worth the $50 for the average consumer is dumb.,radeon_rx_6000
"I actually dont think its that simple. Everyone's favorite game cyberpunk will use dlss and ray tracing. If it does well you will see lots of other games taking it up. Lets hope AMD implement something similar and quickly, like not next generation of cards.",radeon_rx_6000
Did you just roll out of bed? This is just as bad as the Ampere launch.,radeon_rx_6000
"Where are you seeing the 3080 with better results at 1440p? The 6800xt leads by 3-5% at 1440p for $50 less. It's objectively better at non-RT 1440p.

If RT matters to you, buy Nvidia. If you're playing 4k, strongly consider Nvidia (though 10GB VRAM may become a problem very quickly). If neither of those are true for you, the 6800xt is quite compelling.",radeon_rx_6000
"What? The 3080FE is faster in traditional rasterization performance than the 6800XT with SAM. How is the 6800XT sitting ""between 3080 and 3090"" then? The 3080 is a perfectly capable 4k card btw, I have it on a high refresh rate 4k monitor ;)",radeon_rx_6000
They are mostly console ports. So they are very well optimized for AMD hardware. Nvidia generally comes a little closer with driver updates later on.,radeon_rx_6000
"AMD traditionally doesn't do that, but it's possible.",radeon_rx_6000
"It's more like better lighting, and everything that ""better lighting"" encompasses. So better reflections, better shadows, etc.",radeon_rx_6000
"No, it's ray tracing. In some games it's ""just"" reflections (although reflections can be a huge deal in material realism) but then it's also global illumination, realistic shadows, etc. Like just compare Minecraft RTX on vs off, that's the kind of lighting which is going to get more common in future as more and more AAA titles adapt to it.",radeon_rx_6000
"No. Physically accurate light, shadows and even sound can be traced with RT",radeon_rx_6000
"I mean, people will tell you otherwise, but functionally that's the biggest difference.",radeon_rx_6000
RT isn't that far behind in the 2 titles that are recent enough to have gotten optimization for RDNA2 though. It's ahead in RT in Dirt 5 and just slightly behind in Watch Dogs: Legion.,radeon_rx_6000
about 5 seconds before they sell out,radeon_rx_6000
They are not even released yet. And I figure it would be the same day as it goes on sale (the latest information points out that it sells in 8th December 2020).,radeon_rx_6000
And we now know why control wasn’t on AMD presentation for RT.,radeon_rx_6000
"That's the hard truth. If one or more RT/DLSS games are at the top of your use case list, the 6800xt is just not where you should put your money.",radeon_rx_6000
"Yeah the pricing and rasterization performance is very close, but then if you do want to enable some of the ray tracing features in a game, you'll regret going AMD.",radeon_rx_6000
"It's marginally better. The price difference is not significant. 

Ultimately, at MSRP, Nvidia cards are just better value. RTX, DLSS 2.0 and all.",radeon_rx_6000
"Most of them are regurgitating Linus' opinion, and he will never have anything good to say about Radeon.",radeon_rx_6000
"3080 is fine, seems like a good deal with+$50 getting you Better RT and DLSS.",radeon_rx_6000
"Why are you regretting the 3080? It has similar to better performance in rasrasterization , better rt perf and dlss ... Etc? I think you made the right choice",radeon_rx_6000
"Im confused, why the regret? The 3080 is far better value (if you haven't overpaid) because of rtx, dlss, drivers and more features.",radeon_rx_6000
Probably would have if there were any in stock. Now I'll just keep on waiting for the 3080 ordered on launch day.,radeon_rx_6000
"> 6800XT looks like a go-to card for 1440p if drivers will be good.

6800XT is slower than the 3080 in 11 out of 14 games Computerbase.de has tested and significantly slower in raytracing enabled games, with Control showing a whooping 71% advantage for Nvidia in 4K (and 66% in 1440p). Arguably Metro and Shadow of the Tomb Raider show less extreme differences, but we are still talking about 23% and 35% more performance on the Nvidia hardware, and we are still talking about raw performance w/o using DLSS as well.",radeon_rx_6000
Which review did you watch? In Linus review AMD got stomped by Nvidia.,radeon_rx_6000
"Not at all.

It wins in price, by a small margin, and gaming performance, by an even smaller one. And loses in... everything else.",radeon_rx_6000
Let’s be honest. It’s all pretty irrelevant if they don’t sort the fucking drivers!,radeon_rx_6000
they really trade blows in rasterization,radeon_rx_6000
"Better at 1080p in some games by a slight amount with horrid raytracing performance, no DLSS, worse 4k performance, worse codec for streaming/recording/VR. How is that better?",radeon_rx_6000
TPU average has it under a 3080 by a few %. Also in RT games it trails by 30%+...,radeon_rx_6000
">Did Techpowerup measure the power correctly? They're reporting wildly low power consumption compared to other sites.

 

6800xt is **better** than 3080 but less features. How so? I haven't seen a review that suggests that.",radeon_rx_6000
How is the 6800xt better? It’s roughly equal in traditional rendering but gets blown away when you enable the features of the 3080.,radeon_rx_6000
"I have seen multiple reviews with SoTR tracing enabled, so no idea.",radeon_rx_6000
"it's bullshit, you can turn rt and dlss independently, many other reviewers compared the rt performance in sottr",radeon_rx_6000
"It seems to be full DXR, but the devs didn't plan ahead to support a graphics card that supported DXR but didn't support DLSS since at release, no card like that was forthcoming. I wouldn't be surprised if they patch that in in a couple weeks.

AMD's drivers also don't seem to support Vulkan RT, so games like Q2 and Wolfenstein Youngblood don't have access to it. Again, probably something that AMD will patch in a few drivers.",radeon_rx_6000
Both are completely different things,radeon_rx_6000
With the consoles supporting ray tracing pretty much every major title is going to offer some sort of ray tracing.,radeon_rx_6000
"It's coming to consoles now - so you can expect over the life of these GPUs for Ray Tracing to become far more prominent in games. Maybe since it is on AMD Hardware on the consoles, optimisations there will also result in comparable improvements for AMD on PC, but as it stands it is clearly behind in that aspect.

Given that most of us aren't upgrading with every generation, and a lot are going even more than 2 generations between upgrades, this is an important thing to consider.

It's great they can compete in rasterization - this does feel very much like first generation Ryzen, and I hope that their subsequent cards see trajectories as their CPUs have, but there's no benefit in lying here.

Ultimately they're close enough that if you're not that bothered about ray tracing and know you won't be during the duration you expect to have the card, then yeah, just get whatever you can get - they're close enough to each other in rasterisation that it doesn't matter. 

But if you care about Ray Tracing or essentially anything else, it's hard to recommend going for a 6800XT over a 3080 if you have the choice between them.",radeon_rx_6000
"With these statements you are making, you are one of those idiots.",radeon_rx_6000
[deleted],radeon_rx_6000
the 3070 is cheaper than the 6800 in much of the world making the 6800 a terrible value,radeon_rx_6000
"At these prices both AMD cards are pretty bad value propositions.  The 6800xt should be $100 less than the rtx 3080 based on its parity rasterization performance, horrible rt and lack of other important useful features like dlss.

The 6800 should be the same price as the 3070 based on better rasterization but same issues on the feature set side.

If this were a normal year that's pretty much what would happen with prices in the retail market.

Seeing as how everything is out stock people will basically have to grab whatever they can.",radeon_rx_6000
"3070 obsolete LMAO

AMD reddit is actually delusional

RTX performance of your cards is straight doodoo and no one but reddit nerds are buying a $500+ gpu to turn the settings down",radeon_rx_6000
"It really depends. I would not say it is ""obsolete.""

Considering regional pricing, there is a chance that my country may sell Nvidia offerings at lower price, skewing the actual value of the card in comparison with other locations.",radeon_rx_6000
"It's not a blowout.

RTX 3080:

\- Slightly faster at 4k

\- Better ray tracing perf

\- Better feature set

6800 xt:

\- $50 Cheaper

\- Slightly faster at 1440p

\- 16gb vram

\- Fair bit more power efficient.",radeon_rx_6000
"No you didn't, literally everyone predicted it would be turning level performance. Stop making yourself the victim.",radeon_rx_6000
"It only beats Ampere at 1080p in most games, not quite at 1440p, and definitely not in 4K. and Turing level RT performance is what everyone everywhere has been saying for months. IE you were wrong on the raster point, and the other claim isn't even your own",radeon_rx_6000
"Any comment other than ""AMD will destory Nvidia and have ample stock"" was downvoted for weeks lol",radeon_rx_6000
">above Ampere rasterization performance.

It doesnt though. On 4k, the 3080FE is performing better than 6800xt+SAM.",radeon_rx_6000
[removed],radeon_rx_6000
Cooler is pretty garbage? What?,radeon_rx_6000
"I just don't think they are prioritising it. Despite what you see around here, most people don't actually stream or often record gameplay.",radeon_rx_6000
"3440\*1440 is 59% of 4K.

Also roughly 50% more pixels than 1440p. I would say it's half way.

Edited: 3440 has 34% more pixels than 1440p.",radeon_rx_6000
"Between the 3070 and the 6800, which one should i go for? They both cost within margin of each other (580 Vs 550 GBP)

The 3070's vram seems a little too low, since apparently Watch Dogs Legion takes more than 8 gigs of vram on 1440p iirc from this video

https://youtu.be/5OtZTTwvOak

I want to play on UW 1440p",radeon_rx_6000
"You should check out the next two charts on that review. Yes in absolute performance terms nvidia comes out ahead, but we're talking a difference of like 3-4% (effectively unnoticeable in real world terms) and AMD offers better performance per dollar AND better performance per watt at 1080 and 1440p. It's definitely the better of the two for 1080p/1440p without raytracing, although it's a very marginal difference.",radeon_rx_6000
"It's not using SAM, so it's leaving performance on the table for people with 5000 series CPUs",radeon_rx_6000
"Man i saw that...what an incredibly douchey tweet.  I feel like with Nvidia’s launch issues and the lack of stock for the PS5 etc., it was understandable that launch stock was going to be pitiful but him rubbing it in that way after shitting on Nvidia was a real bad look.",radeon_rx_6000
Isn't he the jebaited guy?,radeon_rx_6000
"See, that's what I was thinking, but it doesn't explain the supply we saw today.  Microcenters seemed to be getting 5x more 6800s than 6800xts.  If it was a case of AMD trying to push people to the xt because they didn't want to cut down working dies, then you would have expected a relatively higher percentage of xt cards on launch.",radeon_rx_6000
"VRAM is big for textures. 3080 performance at $350 is probably ~4 years away (2 generations), while >10gb VRAM being necessary for best textures is going to look more like ~2 years.

Depends on how long you intend to keep the card for before upgrading.",radeon_rx_6000
"Not when you gimp one generation using similar VRAM values you used 2 years ago.. Ubisoft most recent games at 4K with RT enabled, one is already bordering 9GB and the other is over 10GB. 

So a months old 700$ card that is already with VRAM issues because NVIDIA gimped on VRAM and you think it's gonna last 3 years? With a new generation here which always comes with a graphic fidelity jump right behind it?",radeon_rx_6000
"I'm not sure what TPU did, but their power results are completely different from everyone else.

E.g. here are the CB results: https://www.computerbase.de/2020-11/amd-radeon-rx-6800-xt-test/5/#abschnitt_6800_xt__3080_bei_270_watt_6800_5700_xt__3070_bei_220_watt",radeon_rx_6000
"Yep, so slightly cheaper than the 3080 is proper, but ultimately i'd still suggest the 3080 for the vast majority of users as once you're spending that sort of money the $50 ain't gonna matter. 

The non XT though makes no sense for anyone to buy",radeon_rx_6000
"It's a shame that consoles signed their deal with AMD. Just because they're missing out on DLSS. 2.0 is really good, and would have made these consoles so much better than they already are.",radeon_rx_6000
"For me, 100 watts less power consumption (so a cooler and quieter PC) and top notch in-kernel linux support make AMD a no-brainer compared to nvidia this time around. Nvidia needed a clear performance win over AMD for me to put up with that shit, and they don’t have it.",radeon_rx_6000
What about no RT,radeon_rx_6000
yep just edited the comment with XT vs non XT,radeon_rx_6000
"I'd say the cons are the same, but pro has shifted from better perf/$ to more VRAM.",radeon_rx_6000
"Same boat. $50 price difference for a $300 card is a bit deal. $50 for a $700 card is a 7% savings, so not really meaningful to justify giving up raytracing.",radeon_rx_6000
"Hypothetically, would you give up RT/DLSS for 100$? Would a 450$ 6800 or 600$ 6800xt be enticing purchases?",radeon_rx_6000
"I can't see anyone giving up superior RT perf for $50, especially when all the new games are going to start using it with consoles supporting it now.",radeon_rx_6000
"They had to get into the high end market even if slightly behind. They'll sell okay, and sets them up for the 2nd-gen bump in a year or two like NVIDIA saw with the 2000 to 3000 series. Yet to be seen if NVIDIA can stay 1-gen ahead or not essentially.",radeon_rx_6000
I just hope their low and middle tier cards don't disappoint.,radeon_rx_6000
"I don't know if they want to ramp up supply either, the bigger margins are in the CPU side of the wafers.",radeon_rx_6000
"AMD is, surprising to me, competing and trading blow, and has far better power efficiency... but suddenly insightful redditors who never breath through nose decided today that raytracing is really important and post these deep comments.",radeon_rx_6000
rx6800 is 80$ more than rtx3070 but for 70$ more you could get the XT version which is then 50$ cheaper than the 3080. but at that point I also would just shell out the 50$ more and get the 3080.,radeon_rx_6000
You can easily get a 3070 in Canada now if you try to. I've gotten 4 (For my friends and myself) in the past 2 weeks. Just sit on Newegg for from 7:00 to 7:15PM Wednesday to Friday. I doubt it will take you more than 2 days.,radeon_rx_6000
"Khronos really dropped the ball when it came to OpenCL.

The consortium is finally doing things right with Vulkan, and OpenCL may have a good path forward with 3.0, but CUDA has an insane lead now that Im not sure how much it matters.",radeon_rx_6000
Oh man it's been a long time since I wow'd at the 8800 GTX in the PC World magazine.,radeon_rx_6000
"HW Canucks said it's unstable, leads to performance regression and crashes in a few titles. Nvidia mentioned doing a whitelist for games with Resizable bar.

IMO it will be forgotten as an independent feature the moment DirectStorage is here.",radeon_rx_6000
[deleted],radeon_rx_6000
[deleted],radeon_rx_6000
"Saw the product page of the 6800 xt on alternate later on as well, but honestly for 820 € you may as well just get a 3080",radeon_rx_6000
"I don't think AIBs get to make that choice. Technically they could, because they are the ones buying the memory. They would just have to swap the 2gb modules for 1gb modules, but that's probably against AMD's rules. Don't think any AIB has ever done that.",radeon_rx_6000
I've never heard of a 3rd party card that had less RAM than the official specs.,radeon_rx_6000
"RT is one thing, but DLSS is definitely worth the extra 50.",radeon_rx_6000
"I mean, it's $50, but that's $50 at the 650-700 range. It's really a 7% price difference, so not really anything meaningful. 

You get the same performance (little better below 4k, little worse above 4k) but lose hard when you enable any raytracing features.",radeon_rx_6000
"TPU was comparing it to the reference 3080 while GN was comparing it to a similarly priced 3080 eagle, that probably accounts for the difference.",radeon_rx_6000
"Thanks, I will check it out.",radeon_rx_6000
"HWU's 18 game average has 6800XT beating 3080 at 1440 on average.

The RT and DLSS argument for Nvidia continues to be silly since support is limited and most RT implementation tanks frames with almost no visual improvement. If you play those titles, it could tip in favour of 3080 however.",radeon_rx_6000
And Hardware Unboxed shows it quite a bit beating 3080 at 1440p in 18games average benchmark.,radeon_rx_6000
LTT has a good example. https://www.youtube.com/watch?v=oUzCn-ITJ_o&t=9m21s,radeon_rx_6000
Linus,radeon_rx_6000
AFAIK GDDR6X is made by Micron for Nvidia exclusively.,radeon_rx_6000
HWunboxed also only tested one game. TechPowerUp seemingly took the average power consumption across all the games in their test suite. It would be insightful to see what power consumption TechPowerUp got in the one game that HWunboxed tested.,radeon_rx_6000
"It's a big deal to /r/SFFPC enthusiasts at least, but yeah power consumption isn't THAT important to most of the market.",radeon_rx_6000
"I agree to some degree, BUT and that is a big but. If you are like a friend of mine that still sits on his RX580 and you wanna upgrade now. You will have to make the decision between 3080+new Power supply or keeping your old one and a RX6800.

I would definitely agree that in most cases power consumption doesnt bother the customer, but now we are in a situation, where it will strongly influence loads of peoples decisions with the increased need for stronger power supplies.",radeon_rx_6000
"Eh, 100 W is actually kinda big. I mean, sure, when 3080 gets to use it's features, it makes those 100W worth it, but otherwise...",radeon_rx_6000
"That's not true. If people had the choise between two graphics cards that were completely identical, but one had half the power consumption of the other, the vast majority would choose the card with the lower power consumption. It saves money on your electrical bill, and leads to less waste heat and, given the same cooling solution, less noise. 

Of course, many people might care *more* about raw performance, and there might be other things or features they value above power consumption.",radeon_rx_6000
"True, I wonder what happens if you bump up that power",radeon_rx_6000
Huge for mobile and possibly server applications tho.,radeon_rx_6000
"Most AAA games released in the past year target 4-6GB. Newer games will target 6-8GB. Some might target 8-10GB 2-3 years but most won't.

The visual difference between Ultra/High/Medium in many games is minimal in gameplay. Going to high is not going to be a big deal in 3 years time, a Ampere GPU will need to turn down other options by then anyway.

DirectStorage/RTX IO will hopefully make this a non-issue, hopefully in 3 years we will see widespread adoption.",radeon_rx_6000
"Every single AAA game I play doesn't even fully use 8gb. WD:L and MSFS are exceptions, yet they bottleneck rasterization before VRAM.",radeon_rx_6000
"I mean, at 4k pure rasterization the 6800 XT is better. VRAM is used in more than games as well. 

If you want the highest visual fidelity and gameplay advantage overall at 4k, you need a 3080. This may change with AMD supersampling but it doesn't sound like a thing till next Summer.",radeon_rx_6000
"I do undervolt mine. I don't get equal performance though, I lose a little. Not a big deal. It's worth not having a furnace in my room. It does make a difference in heat for my room.",radeon_rx_6000
"undervolted tuf 3080 here, 950mV at 1950Mhz 270W max in red dead 2.

PS: same performance if not better than oc at 2070Mhz",radeon_rx_6000
You're surprised by that?,radeon_rx_6000
"Minecraft is a pretty big outlier for RTX, nvidia beats AMD for sure on raytracing but this particular chart makes it look worse than it really is.",radeon_rx_6000
"Is raytracing even usable at all??? Anything that makes my game run less than 60 fps is absolutely a non starter. And from what I've been seeing, rtx is making games run at 30-40 fps. It's still more of a tech demo than a usable element.",radeon_rx_6000
And in non SAM performance.,radeon_rx_6000
"Sure, there is performance on linux with open source drivers.",radeon_rx_6000
"I actually agree, but some people keep saying they simply don't care about RT, so I just made a safer statement.",radeon_rx_6000
Looks like AMD has good 1% low frame rates generally,radeon_rx_6000
Best for power consumption and VRAM capacity,radeon_rx_6000
you literally cant buy a 3080 unless youre ready to pay 950€ and youre telling me theres no compelling argument to buy a 6800xt? lol,radeon_rx_6000
I watched ot to the end. They really really don't know why the non xt exists beyond promotional purposes.,radeon_rx_6000
? I just watched. He pretty much said the 6800 doesn't make much sense and that the 6800xt was pretty good but still a tough sell over the 3080 because of all the extras that come with the 3080 for only $50 more... pretty much saying the same stuff everyone here is saying.,radeon_rx_6000
Not even listed online? You are lucky to have websites. In my part of the world we have to wait for the newspaper.,radeon_rx_6000
"Yep, those can be pretty demanding",radeon_rx_6000
"Not really, people only say this because HL:A will allocate up to 10GB or so if you have it available. Most VR games are not that intensive VRAM wise, despite the Vive Pro and Index having high res displays.",radeon_rx_6000
"I mean, if you're willing to spend $650 does the extra $50 really matter to you? 3080 is just superior given the market segment and pricing. This is great news for PC gamers though, AMD is back in the game and the competition is real. Everyone wins.",radeon_rx_6000
"Yeah, as someone who just games at 1440p and gives zero fucks about RT or 4k for the foreseeable future, i'm probably good with whatever actually comes in stock at this point. I'm probably not alone in that regard.",radeon_rx_6000
"Yeah but the difference is nvidia isn't struggling like intel is/was, also the software features are way behind when it comes to amd",radeon_rx_6000
"I am pretty hopeful that this is the start of a real competitive arms race. Ampere beats rdna2, rdna3 beats ampere, nvidia beats rdna3, etc. Would be a dream come true.",radeon_rx_6000
"Same AIB partners will price their cards similarly, but certain partners work only with 1 vendor. Sapphire, Powercolour, EVGA, PALIT, etc. Plus after added costs of an aftermarket card 50$ might not be that big of a gap.

But to be absolutely honest I was under the impression AMD reference cooler is a poor design after watching some reviews. But apparently it's not the case. So I mostly alluded to the fact you can go with Founders and get a good card and with AMD you had to get a partner card.",radeon_rx_6000
Watch ltt video for video evidence. It's way behind.,radeon_rx_6000
They still can do it later. It makes no sense for them to do it in a supply-constrained situation.,radeon_rx_6000
Looks like good 1440 which is what my use is for.,radeon_rx_6000
There was already one direct comparison using same settings on PC and console and xbox was roughly similar in rt-performance to rtx 2060 super. It can do low rt settings at 4k 30fps.,radeon_rx_6000
"Well, console raytracing will probably target 30fps and use dynamic resolution / upscaling along with other raytracing speed / quality tradeoffs (lower resolution, fewer samples, etc. ). PC will give you the higher quality options.",radeon_rx_6000
Oh yeah all those people buying $700 cards to play 1080p60 will be super satisfied.,radeon_rx_6000
Yes because I buy $700 graphics cards to game at 1080p60,radeon_rx_6000
I wouldn't be spending this much on a card for 1080p performance,radeon_rx_6000
"It’s on par or worse than a 2060 non-super in RT according to TPU. It might actually be worse rt perf/$ than the 2080 Ti, which is just impressively bad.

If you want RT I think a 2070 super decisively beats it for less money.",radeon_rx_6000
Who cares,radeon_rx_6000
"Doesn't matter to the consumer, anybody wanting to play cyberpunk to the full effect should get Nvidia",radeon_rx_6000
"It's actually worse, but that's fairly irrelevant to the customer as the other 2 commented",radeon_rx_6000
Yes? Why else would you get an RT accelerated gpu lol?,radeon_rx_6000
Timezones are hard hahahaha,radeon_rx_6000
Oh shit hahaha,radeon_rx_6000
So hyped to see the same video from 6 different channels with almost the same spreadsheets and one good in depth video which comes out later but is everything I actually need to know <3,radeon_rx_6000
oh seriously?  why did i think it was 1200UTC?,radeon_rx_6000
"I do have  ""4G decoding"" in my BIOS, and I've been told it's a similar thing, but I've also been told there is something missing, and it's not the full resizable BAR support.",radeon_rx_6000
"They did? well, damn. I didn't want to go Nvidia, but if they offer that on older boards I'll have to no choice.",radeon_rx_6000
Will it only be for 3000 series cards or anything older as well?,radeon_rx_6000
"Pretty sure GN isn't reporting 210W

[https://youtu.be/jLVGL7aAYgY?t=1425](https://youtu.be/jLVGL7aAYgY?t=1425)

So I guess it's guru3d and TPU, although guru3d doesn't show an average at all.",radeon_rx_6000
I've seen others who are happy with it though.,radeon_rx_6000
"Seemed pretty decent in the TPU review, especially compared against the 3080 FE. A 6800XT reference card or a 3080 FE would be the cards I'm looking to get, anyways; they're both short enough to fit in my current case. I might end up just buying a CPU and GPU instead of a new build, anyways.",radeon_rx_6000
"That's kinda strange since it'd usually be AMD where colors popped more than nvidia with both at default.

I'm surprised how much better the frametimes on 6800XT can be in some games, RDR2 just looks horrible on 30xx.",radeon_rx_6000
"It's not ray tracing. AMD looks washed out in general. Even in tests without RT.

At least what this review show is AMD Radeon suite is still broken.",radeon_rx_6000
"Yeah it's not a 1:1 comparison.   But nvidia has also sat on their hands much less than intel

People forget that this is the second gpu on their new architecture.  They weren't going to go from almost bankrupt to beating nvidia in price and performance in 2 releases.  I'm genuinely impressed at how close they are within such a short time period.   Plus if you believe the rumors and ""leaks"" the 7000 series on 5nm will be a game changer",radeon_rx_6000
"I trust that nvidia has a lot coming down the pipe

Hell look at how fast they have improved dlss and ray tracing",radeon_rx_6000
"Yes, which would be explained by the effect of the cache. At smaller datasets the cache significantly reduces average latency but the larger the dataset is the less it helps.",radeon_rx_6000
But shouldn’t SAM essentially bypass the size of the bus  and speed of ram? I think Gear Seekers saw higher frame rates with the 6800 XT when the SAM was enabled,radeon_rx_6000
[Xe's on TSMC 6nm dawg](https://www.tweaktown.com/news/74549/intels-next-gen-xe-hpg-gaming-gpu-will-be-made-by-tsmc-on-6nm-in-2021/index.html),radeon_rx_6000
Consoles have used AMD graphics for past 8 years in case of ps and past 15 years in case of Xbox.,radeon_rx_6000
But then you already see AMD's cards' prices over MSRP. So it's no different.,radeon_rx_6000
"That's a bridge to cross when there is actual availability.   The msrp is what's being compared,  since several people already have both series at that price (admittedly scarce).",radeon_rx_6000
Given the AMD cards sold out in an instant as well I am not so sure they will be any more available than the Nvidia cards.,radeon_rx_6000
I'll wait,radeon_rx_6000
">yeah its a no brainer 

I'd go with the 3080 for my own reasons, but no, it's not a 'no brainer' at all.  These are entirely competitive GPU's.",radeon_rx_6000
"Oh damn, that’s only the h264 encoder so I hope the h265 one is good",radeon_rx_6000
"F, hopefully the h265 encoder is better as LTT only features the h264 one",radeon_rx_6000
"I’m using a 3090 for VR and am always wishing for higher refresh rate and higher SS. VR at 144hz with 1.5x SS looks amazing but at best even with a 3090 can only be achieved by turning down settings. Each frame needs to render two 2k x 2k images, which makes me wonder if a 6900XT (with its boost at lower than 4K output) may outperform the 3090.",radeon_rx_6000
"Yeah, but AMD said they have no plans to enable it outside of Zen 3 so far.",radeon_rx_6000
"I believe you need mobo support in the firmware, as well. I do not believe you can simply enable this on every PC with a GPU driver update.",radeon_rx_6000
RTX 2080Ti MSRP would like a word,radeon_rx_6000
"the current prices are going to be persistent for months given the shortage, maybe even a half a year so it would make more sense to do value analysis on reality.",radeon_rx_6000
Thanks for the input :),radeon_rx_6000
AMD drivers tend to improve performance over time which may make it a better long term proposition,radeon_rx_6000
[deleted],radeon_rx_6000
"> AMD hobbled the performance at 4k with the narrow memory bandwidth, and their cache isn't helping out with that. With a proper bus width, it would probably be trading blows with the 3080 at 4k instead of losing by a bit most of the time.

I only disagree with one part of this: the cache **is** actually helping. If not for the cache, a GPU with a 500 GB/s memory bandwidth wouldn't perform as it does.",radeon_rx_6000
It's funny how it's the opposite on Linux.,radeon_rx_6000
AMD just released a driver with RX 6800 support today.,radeon_rx_6000
"Well, so far I dont see RX6800 driver issue reports, and if we are to compare driver support I would personally put AMD above since their cards tend to age much better performance wise.",radeon_rx_6000
"And some can lose a little bit of perf.

Taken as an average, looks like a very minimal improvement.",radeon_rx_6000
it hurts to look at his charts when i see my 980ti towing the bottom line.,radeon_rx_6000
"This could explain the ps5s commading lead on performance vs the xbox-series-x on this title also.

Digital foundry just did a comparison and ps5 was 15 to 30% faster depending on scenarios.",radeon_rx_6000
"I feel cheated, I was told that they were saving the ray-tracing performance for the ultimate jebait!",radeon_rx_6000
"It's completely pointless to sign up now, they are still working through the day 1 sign up folks, and they've been working on it for nearly a month and a half.",radeon_rx_6000
"> better at lower-than-4k resolutions

If you are buying a card for lower than 4k resolutions, save your money and get a cheaper card anyway.",radeon_rx_6000
"Yes, you can turn it on while RTX is off.",radeon_rx_6000
"Well I care, especially about DLSS. Cards are essentially tied at 1440p and at 4k 3080 is slightly better. But in any title DLSS is supported, 3080 will absolutely trash 6800xt.",radeon_rx_6000
"Raytracing is basically the next big thing now that the new consoles have it, all of the new games will market for it.

DLSS is also massive as well",radeon_rx_6000
"DLSS performance is major, really wonder what AMD’s alternative will bring.",radeon_rx_6000
"I do. And yes, they are worth the extra $50 for me.",radeon_rx_6000
"Microsoft, Sony, AMD and Nvidia seem to along with thousands of gamers",radeon_rx_6000
If you don't care about DLSS you're a dumbass.,radeon_rx_6000
Mostly everyone who cares about how good a game looks..? Which is the only reason you'd buy a brand new GPU for gaming..?,radeon_rx_6000
A lot of people.,radeon_rx_6000
Literally everybody I know who is in the market for these cards.,radeon_rx_6000
"No one on 1080p, which is a vast majority of gamers. Probably even a majority of r/hardware. Lol",radeon_rx_6000
Anyone with a brain not controlled by AMD cares.,radeon_rx_6000
It will probably come in to Zen2 too. Be patient.,radeon_rx_6000
"Yeah. Super happy with good day 1 support. Waiting to digest stuff, and for 6900 xt release.",radeon_rx_6000
"The Linux driver already supports it without CPU restrictions, it only depends on the BIOS.",radeon_rx_6000
At that price a used 2060 is probably what you’re looking at,radeon_rx_6000
">vast majority of games out there these days don't utilize those features (yet).

And when they do, they'll be on the next version of RTX/RT/DLSS.",radeon_rx_6000
"I'm no expert either, but honestly, it probably comes down to the fact that this is AMDs first attempt vs Nvidia's second. 

Nvidia's R&D department has always been better than AMDs, so they are usually better at ""new"" technology anyway. In a few generations, or heck maybe even next generation, we will have some awesome ray tracing cards from both sides, but right now I think Nvidia just has the clear advantage with experience.

AMD could come up with an architecture that blows Nvidia out of the water on future cards. I think they just didn't see ray tracing as a must have feature last gen (and honestly even Nvidia's offering last gen wasn't really ready for primetime, but they wanted the hype), and are trying to play catch-up here.",radeon_rx_6000
"ray accelerators are not dedicated for rt like rt cores. RT cores are new hardware added to turing/ampere that specifically does rt and is very fast at rt. AMD's solution is to use software to assign part of their existing hardware to rt and call it a ""ray accelerator"". Obviously, this causes a big hit to performance when you turn on rt as it removes hardware that is used for normal rendering on top of it not being optimized for rt in the first place.",radeon_rx_6000
"It is.  3080 has tensor cores, 6800xt just won't ever be as good at AI.",radeon_rx_6000
"That is a lot of marked up there but for both 3080 and 6800XT. Here people reported 6800 XT reference in Alternate NL/BE was priced at 900 EUR. 

I hardly see something on stock in 3080 anyway (NL). Sometimes I see GPUtracker.eu too, there from my memory 3080 is just bad but 3070 is not that bad. I just saw also at least one 3070 in NL with a marked up price too (700 EUR, no way I pay for this) but this is also not in say top 5 PC retailers. 

In my relatively trusted retailers nothing is on stock (megekko, azerty, sicomputers, alternate), but you can order at the AIB 3080 MSRP at megekko still. The catch is that you have to wait in line to get the card for next year maybe. I might look at Ventus 3 MSI or Asus TUF. So, without a mark up, the extra cost getting the AIB (at MSRP) is fine to me.

We will see for the 6800 XT next week as well, so then we can see the comparison of the price between AIB 3080 vs AIB 680XT too, personally curious to see the Sapphire Pulse for sure.",radeon_rx_6000
I hope AMD’s DLSS competitor is damn good. Honestly these GPUs are useless for raytracing without it.,radeon_rx_6000
"What do you mean? 
6800XT is at 16 fps average
RTX3080 is at 80 fps average.

16 x 5 = 80. Therefore, 5x faster.

https://math.stackexchange.com/questions/12768/what-does-1-13-times-faster-mean",radeon_rx_6000
"Well, some people feel more comfortable with the extra VRAM.",radeon_rx_6000
"I also think that waiting until the end of the year to get the full picture will be beneficial. People are bashing AMD's RT performance when probably close to 0 games in the benchmarks have any kind of optimization for these new cards (which are from a different manufacturer nontheless). PS/Xbox are also interested in RT performance so 6800 cards might at least close the gap in the future, especially in multi-platform AA/A games.",radeon_rx_6000
Even with a 15% boost the performance is pathetic.,radeon_rx_6000
And still refreshing the bestbuy page,radeon_rx_6000
Just today? Man I've had a thoroughly unproductive day at least once per fortnight over the last 2 months... This is an insane time for hardware releases. Frustrating when I walk away on launch day with f\*\*k all but an exciting time nevertheless.,radeon_rx_6000
hot cakes have much shorter cycle times than silicon wafers,radeon_rx_6000
As is the 2020 way,radeon_rx_6000
"Selling out immediately on launch is a problem for any new tech that is desirable. What I am waiting to see is if it stays sold out for months like nvidia or more available longer term. 

&#x200B;

As a general rule for myself I have regretted most of my technology purchases made on release. I guess that makes me more patient.",radeon_rx_6000
"What matters is how quickly they come back in stock, if they stay out of stock of months then yeah that is a problem but if they are back in stock in a week and stay in stock then there is nothing to complain about.",radeon_rx_6000
"Can people stop saying that? like common, when demand is literally x20 the norm, ofc there's not enough stock, it's impossible to keep up with that for any company, giant factories to increase production take many years of planning/construction etc to be up and running. 

Just covid + really good gpu cycle creating an absurdly high demand, that no company could ever suddenly keep up.",radeon_rx_6000
"I feel the same unfortunately. Really wanted to go Team Red this round, but FidelityFX seems like such a distant development that it can't really be considered. I assume more games will be incorporating ray tracing throughout this generation so the near unplayable performance there is an L for sure. And not having NVENC will really hurt for streaming.

Love you Lisa but unfortunately you're only getting my money for a CPU this cycle",radeon_rx_6000
Nvidia's marketing department are all high fiving after reading this comment.,radeon_rx_6000
"1. Depends on the title. Dirt 5 runs better with ray tracing on AMD for example. Older titles have only been optimized for Nvidia's implementation.
2. It's been announced. Though I would not buy something based on announced features.
3. This is a good point. If you don't have enough cores, you need to go Nvidia.

IMO, I'd rather take AMD's open source drivers with multi-DPI support through Wayland than X on Nvidia, even in spite of DLSS and better ray tracing performance in existing titles.

This is not to say the 3080 is not a good option. It is, and I think people that care about ray tracing of existing titles, or NVENC, and that don't use Linux, should consider it as their first option.",radeon_rx_6000
"To me, NVenc is the most noteworthy thing missing here. It's a real reason to pick team green over red.

I think we're still 1-2 generations away from RT being a main selling point, and DLSS is still not utilized by enough games to be worth considering imo.

Still, the price and performance is quite compelling here. If the driver support is equal on both sides (big IF), I think I would prefer AMD.",radeon_rx_6000
"Big Navi is still much better than consoles, so you don't have to worry about that. But I agree you should care about RT performance as it is feasible now with Ampere.",radeon_rx_6000
Eh R A G E mode is likely just unlocked power limit as we all know. Still SAM seems to really benefit some games so we'll see if it's a one-off thing or a more long-term stuff.,radeon_rx_6000
"Well SAM is not Microsoft, I mean it's possible on Linux already.",radeon_rx_6000
">NVIDIA has far more future-proof features.

I said the same thing when I bought a 2070 to get the RTX features and now I'm looking to upgrade in the next year or so because it literally can't use RTX on most enabled games at an acceptable performance level.

Future proofing is a lie.",radeon_rx_6000
There is no way 3000 series is more future proof with it's anemic vram cap,radeon_rx_6000
"...which is debatable, and honestly overrated IMO.

* For one, I've been hearing that Ampere and RDNA2 like different kinds of RT implementation. If Dirt 5 is any indication (see link), Far Cry 6 and other AMD-sponsored RT implementation may be totally opposite.
* I *still* think that DLSS may end up with the same fate as myriads of other Nvidia proprietary AA technology. It requires specific works, which results in too few games actually using it, there's an AMD implementation in development, with possible other ways through DXR or Vulkan equivalent. Unless every single AAA coming out implements DLSS it's not a gamechanger some people believe it to be.",radeon_rx_6000
"Fully agree, it might turn out that nvidia actually doesn't want to produce 3080 as they understood they have underpriced it and just want to push 3080ti at 2080ti prices asap.",radeon_rx_6000
First impressions on this beta RT is that is does not look any better than native.,radeon_rx_6000
"You mean 6900XT? No way it's going to cause any buyers remorse. It only has 8 more CUs that the 6800 XT. Ray tracing performance will be less than 10% better, which is still far below the 3080. Raster will be a bit ahead, but not by much.",radeon_rx_6000
But for AMD to match Nvidia it required them to have the node advantage. TMSC's 7nm is significantly better than Samsung's 8nm. If both were on the same TMSC 7nm AMD would be in a lot of trouble.,radeon_rx_6000
The kicker is that AMD talked smack about DLSS and how bad the rendering quality is. Then they turn around and announce they are making their own implementation of it.,radeon_rx_6000
What are your clocks after undervolting?,radeon_rx_6000
"I sincerely wish this will come true! 

You will have an ample time to make a decision on which partner model you want to purchase.",radeon_rx_6000
"It's till too much for what I want in terms of power draw. Peaks are too high. I basically cap myself at a single 8-pin. Both of my systems use SFF 450W PSUs, though high quality ones at that (Corsair SF450 Platinum, SeaSonic SGX450 Gold).

I'll wait until early 2021 to see how the 120-150w, single 6- or 8-pin cards look, and go from there.

I COULD make an exception for the 3060 Ti if there's a good partner model. I've been impressed with the thermal handling of these coolers so far. Nvidia is doing something different that is above my head, so despite higher power draw, Ampere is running cooler and quieter than Turing did.

But I do want to see AMD's answer as well.",radeon_rx_6000
"> What if another game you want to play or multiple that come out over the next few months (like Cyberpunk or Watchdog for example) use DLSS?

I'm a patient gamer. I'm on GTA V right now. Next up in my backlog is Shadow of War, followed by a play through of Witcher 1, 2, and 3.

If I get Cyberpunk, it might be a few years. I stopped buying games until my backlog is caught up. Got sick of looking at unplayed games.

> It seems like a very strange conclusion that DLSS is at least a generation away for you just because you can't utilize it for the games you own today.

I think it's a solid conclusion that DLSS is a generation away from me when I'm at least a generation behind in games, and of the current supported titles list, there's one game I would play or would want to play.

> That is the thing, they are with DLSS enabled...

If they supported DLSS...",radeon_rx_6000
"Yep, I just referenced it in another comment and here's the graph you're talking about: 

https://static.techspot.com/articles-info/2144/bench/RT-2.png

The thing that I'm worried about is it's just the one game and Nvidia still does semi-decent in it. My prediction is Nvidia sponsored games will push devs to destroy AMD RT performance deliberately but AMD sponsored games will have RT effects that run pretty well on both cards, just like what's seen here vs [this very poor showing from SOTTR.](https://static.techspot.com/articles-info/2144/bench/RT-1.png) Or worse, Minecraft RTX. AMD needs to push to get their optimized path in to that ASAP.",radeon_rx_6000
"Which new or upcoming AAA games are you interested in that don't support some form of RT? I know Assassin's Creed doesn't, but CoD, Godfall, Cyberpunk, Dirt 5, Far Cry 6, Resident Evil 8 and Dying Light 2 all have RT. I expect a lot of the games further out than that to announce RT support at some point. Witcher 3 and RE7 are some of my favorite games, so Cyberpunk and RE8 are enough for me to really care about RT performance.",radeon_rx_6000
"Right now it's definitely thin on the ground, but because the consoles support it, I imagine there will be a LOT more developer interest now. 

Fortunately for AMD, they'll be building around AMD's specific quirks with their RT acceleration. In fact, AMD loses less performance percentage in Dirt 5 with RT on than the 3080 and 2080 Ti do, as seen here: https://static.techspot.com/articles-info/2144/bench/RT-2.png

So it seems that it IS possible to make performant RT--even MORE performant perhaps than Nvidia's method. Granted this is just a *single* game so I'm going to wait a few months to see if this becomes a trend in AMD-optimized titles.

The fear I have is RTX titles being unplayable with RT on for AMD, but RT being just fine on both vendors from AMD optimized games. That's what it's looking like will happen because both methods are so different.",radeon_rx_6000
"You're spot on. The ""it will come anytime soon"" mentality is a repeat of what happened when turing launched, and ""when you'll be on your death bed looking back how many years of your life spent gaming w/o ray tracing are you going to regret"" and we've seen how slow and often mediocre adoption has been. The industry will get there but I bet it's going to take much longer than people think before we have widespread worthwhile RT implementations.",radeon_rx_6000
This was the last thing about the 3080 review: https://twitter.com/RyanSmithAT/status/1306197243263737857,radeon_rx_6000
"With how terrible Radeon is at raytracing though, I think most will wait for RTX cards.",radeon_rx_6000
[deleted],radeon_rx_6000
DLSS is a bigger deal than ray tracing,radeon_rx_6000
"What? At 4k, the 3080FE is faster than the 6800xt even in rasterization performance.",radeon_rx_6000
"Look at watch dog legion, optimized for consoles.",radeon_rx_6000
"Check out dirt 5, its not a great implementation and id rather leave it off but amd is faster in that one.. One I'm most interested in is cyberpunk when it comes out
Edit: typos",radeon_rx_6000
Its slightly better at those resolutions at the cost of poorer RT and no DLSS plus all the other features that Nvidia cards ship with (including a free game and GeForce Now subscription). AMD optimized ray tracing wont make a huge difference as the cards both use the DirectX Raytracing,radeon_rx_6000
The amount of people on a 1080p panel buying 700€+ GPUs is tiny.,radeon_rx_6000
I have a 3080 and wouldnt dream of turning it off for games that support it. I wasnt really expecting much from it but I think its the biggest graphical jump for years. Control and Metro Exodus both look so much more realistic with RT on,radeon_rx_6000
"AFAIK FidelityFX Superres is made with DirectML, so the AI part is more than implied, although I'm way over my head here. 

> Firstly you can do DL upscales on regular GPU ""cores"". We don't know how much the tensor cores actually help DLSS besides some vague wording around improvements done in v2.0 over v1.0, so Running it on regular GPU ""cores"" might not be too much worse but who knows. Maybe big Navi has a hardware quirk in its ""cores"" that enables decent performance in this task?

That's fair, there's just way too many things we don't know yet.",radeon_rx_6000
DLSS do use Tensor cores while inferencing. That's probably the biggest reason why Geforce class cards have Tensor cores.,radeon_rx_6000
"> Nvidia touted DLSS as a feature of the tensor cores because it came out at the same time as them, but in actuality DLSS can be run on anything.

Sure, but Tensor cores are **much faster** at running inference than standard compute hardware, that's their main purpose. Even more so on Ampere where you can use them asynchronously. Running AI inference is only half the battle, the other half is doing so sufficiently fast to actually achieve a tangible benefit. 

> The tensor cores are far more important for RT calculations than anything else

Why do people still mix up tensor cores and RT cores? Now *those* are 2 things that are really only related by coming out at the same time!",radeon_rx_6000
Hopefully FXF it's designed to be better in raster pipelines and horrible on rt cores,radeon_rx_6000
What about the other two games?,radeon_rx_6000
"Well the DiRT series also had lots of AMD marketing injected into them. From startup screens to car liveries. Not that it matters, but at that point it would be funny if AMD didn't come out on top.",radeon_rx_6000
"RT performance, and no DLSS testing.",radeon_rx_6000
Especially since they had so much time to prepare. If you're gonna show up late to the party; you better at least show out.,radeon_rx_6000
">they deserve flak for that, not a pass.

Do they?

I feel like I am in some nvidia controlled universe.

Comments after comments complaining about RT, while for like entire 2020 no one gave a shit and everyone preferred smooth high FPS.

But AMD comes out with cards that are surprisingly great and competitive, and suddenly only reviews I see on top of /r/hardware are 4k and RT. 

* 4k, when no one gave a shit for 4k before and everyone talked how games should always be benched on lower resolutions to not be cpu limited. But what is that? Is AMD vram bandwith limited losing in 4k? Oh, yeah, 4k is really important now. Even when most of us game 1440p or 1080p.
* and RT where nvidia has obvious expected lead

god damn, everyone disregards so much better power efficiency and general great performance and surprisingly close release, even if lacking in supply..

I feels bit bad for them and bit annoyed by people who seemingly exists to dislike something. *How come amd is not releasing card that is in every aspect better than nvidia?!*",radeon_rx_6000
"He said he needed more time to switch test setups and rerun all the benchmarks on the 5950x. He was transparent about it. They're coming to it. 

Check out the Level1techs review. If HU made you unhappy you'll want to start a war with Wendell :)

Edit: And by the way there's plenty of other reviewers that use not optimal CPUs. Gamers nexus uses the 10700k. Where is the lament about their findings?",radeon_rx_6000
"Digital Foundry is the same, but for the other side. Both of their review methodologies are robust, but their game/scene selection and conclusions are opposite.

For instance, they don’t have a video up for Ryzen 5000 or these new GPUs yet. Only articles. Also probably the most modest praise for the new Ryzens out of all the reviewers. Meanwhile they had a special exclusive preview for Ampere and day 1 reviews.",radeon_rx_6000
[deleted],radeon_rx_6000
"Simply cause the reat is irrelevant for most people, because most people just want high fps at rasterisation",radeon_rx_6000
yes,radeon_rx_6000
[deleted],radeon_rx_6000
"The game can still be rendered internally at 1440p, even if the native 1440p output isn't supported by Sony.",radeon_rx_6000
"The other games are much, much closer though at non-4k resolutions. The 10% beat is pretty crazy.",radeon_rx_6000
"DirectStorage will not reduce VRAM requirement.  SSD-VRAM DMA is bottlenecked by PCIe x4, while system RAM-VRAM is PCIe x16.",radeon_rx_6000
"I’m not saying the constraint is a set fact... but, seeing that some games are VRAM hogs, it would be just nice to feel safe than sorry when it happens. Not by any means necessary though.

I’m impressed with what 3070 can do, despite 3080 being my original choice (but don’t have the patience to wait months).",radeon_rx_6000
"I do have a FE, for MSRP and nothing more 😌 Handles 1440p just fine.

Honestly, looking at modern games I can barely tell the difference between High and Ultra settings anyway ¯\\\_(ツ)_/¯ DLSS, however, is certainly awesome! Raytracing is nice too)",radeon_rx_6000
"I remember seeing discussion that the cache system will run out of juice at 4k, so effective bandwidth at 1440p should be good enough for future as well.",radeon_rx_6000
">You’re not going to get +50% from drivers and “optimization”.

Well, how about +20-30% then? 

Anyway, RT performance improving is pretty much a given, nvidia also had their teething problems. If their super-resolution technique can somehow do better than DLSS on performance while being slightly worse in image quality, that'd be pretty swell as well.",radeon_rx_6000
honestly HUB's review this time around seemed like it used a bunch of AMD favorable games for the most part - probably NOT on purpose and their RT section of the review was really inadequate.,radeon_rx_6000
"I read a few reviews, general consensus is that 3080 is slightly faster than the 6800XT (4-6%), but AMD comes closest at 1440p",radeon_rx_6000
"Realistically, I don't think most people care too much about power consumption, especially since the majority of people don't leave their systems running 24/7.",radeon_rx_6000
the difference between 3080 and 6800xt was ~25W on average. I doubt that will matter to most people.,radeon_rx_6000
"You can undervolt rtx3080 to get similar performance, only 2-3% perf loss.",radeon_rx_6000
"Agreed. You can *however* undervolt your 3080/3090: 100 less Watts for identical performance

Source: https://www.youtube.com/watch?v=FqpfYTi43TE",radeon_rx_6000
"The power consumption is much much better vs the Ampere cards, but I’m not sure that would be high on my list. My 3080 is typically running under 60 degrees in a closed case, so it’s not causing any problems.

Edit: I see that this opinion is being downvoted, but with no explanation. For me, and anyone with power headroom I would think, power consumption is going to sit lower than performance, price and thermals in terms of importance for the end user.",radeon_rx_6000
"Problem is, I dont see AMD dropping prices until Nvidia fixes their supply issue. Its not like these cards are bad, they will sell all day everyday as long as Nvidia cant actually offer their cards to people that just want to checkout out without using alerts or bots. And we may see retailers keep a larger inflated price (over MSRP) for Ampere due to low supply and higher demand.",radeon_rx_6000
TSMC is booked to the max and has pricing power. No way they lower their prices when they’re the only leading edge process manufacturer. Yields don’t matter when your wafers cost way more.,radeon_rx_6000
">AMD is going to be better for most people but nvidia is going to be better for people with more high-end setups

Aren't these cards by definition a high-end setup? So the 3080 is better than the 6800 XT for most people.",radeon_rx_6000
"Personally I wouldn't base buying a GPU around a proprietary feature that is only supposed in a bit more than a dozen games after 2 years. DLSS is very good when implemented and cool in general, but unless it was in a core multiplayer game I played (think hundreds of hours) or got widespread adoption, it's not essential to me. 

However, with the 3080 only being $50 more, and currently there is no DLSS competitor, I am obviously going to say the 3080 is the better buy since it has DLSS today and other features. If/once AMD debuts their competitor things might be different.",radeon_rx_6000
"Mostly slower than NVIDIA, I guess because most software optimized for CUDA.",radeon_rx_6000
"Yup I did, twice even.",radeon_rx_6000
It's possible just a giant pain in the ass. Took me over a month but I was able to snag a 3080 fe at msrp.  Not everyone who gets one is a scalper.,radeon_rx_6000
Newegg (Atleast in Canada) has been doing drops pretty much every wed-friday between 7-8pm est.  Just gotta keep checking and refreshing.... Then check out fast lol.,radeon_rx_6000
Nvidia for the FE and got lucky with EVGA for a friend.,radeon_rx_6000
hahahaha they've already realized it. They were too early,radeon_rx_6000
Are you sure the 3090 was not marketed for gaming...8k gaming with RGB?,radeon_rx_6000
"They did, they even did an 8k DLSS gaming demo in the presentation.",radeon_rx_6000
"> They literally didn’t market the 3090 for gaming

Yes they did. 

Look at these two pages: 

https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090/

https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx/

Gaming was clearly the marketing focus of the 3090 compared to previous titans.",radeon_rx_6000
">They literally didn’t market the 3090 for gaming.

Didn't they literally use ""8K Gaming"" in the 3090 marketing?

[Yes, yes they did.](https://www.nvidia.com/en-us/geforce/news/geforce-rtx-3090-8k-hdr-gaming/)",radeon_rx_6000
what do you use it for then? It's clearly not a titan card,radeon_rx_6000
3090 is missing plenty of workstation features compared to previous Titan.,radeon_rx_6000
Post a link?,radeon_rx_6000
"TPU.

I misremembered it, [210W average (93W less than 3080)](https://tpucdn.com/review/amd-radeon-rx-6800-xt/images/power-gaming-average.png)

[Peaks of 284W](https://tpucdn.com/review/amd-radeon-rx-6800-xt/images/power-gaming-peak.png) which is fine, average is what matters from a sustained power dissipation perspective.

Wonder if they had a testing problem and messed up their power testing.",radeon_rx_6000
Dude didn’t even get his degree,radeon_rx_6000
The GPU market isn't as competitive as the carbonated beverage market.,radeon_rx_6000
Intel Xe,radeon_rx_6000
"From what I have read so far, it is difficult to say RX 6000 is a clear winner over Ampere at both 1080p and 1440p. In fact, Ampere is still ahead in average FPS at both 1080p and 1440p gaming according to some reviews.

However, Ampere clearly smashes the competition at 4K and in raytracing. I mean Nvidia is so ahead in raytracing no wonder why AMD did not want to release charts with raytracing enabled.

The real winner is, I believe, RTX 3070. The card offers better bang for the buck across the board. I mean RX 6000 is good, but why would get RX 6000 over RTX 3070 if I don't need 4K gaming? Yeah, performance per watt is great, but overclocking is limited by AMD anyway.

Until AMD partners release factory overclocked cards, It is difficult to say any RX 6000 offers better value than RTX 3070, but that's just my opinion based on reviews and welcome others to disagree with me.",radeon_rx_6000
"3080 scales better to 4k but also pulls 30-40W more. Push a 6800XT to the same power and the performance is basically the same, too. I suspect a ton of partner models are going to be 3-5% faster at stock, at which point, the delta is smaller than a mouse fart.",radeon_rx_6000
"Both cards are equally good, in my opinion.

I trust that I am having a headache from deciding whether should I go Nvidia or AMD on their latest releases... (out of stock is not of my concern since they are not limited runs anyways).

it is not ""unusually"" better, it is relatively better in some games and worse in some games. That SAM and RAGE Mode for FPS Boost by AMD is mostly a ""gimmick"" and not a strong selling point.

One unquestionable advantage that Nvidia cards have is only for the ray-tracing performance in Ampere series cards. AMD is ""winning"" by giving the equivalent rasterization performance on a lower price point.",radeon_rx_6000
"SR-IOV means you can use your GPU in a gaming VM without a performance hit. You know NVIDIA locks out VMs use by having a bullshit VM check in their Geforce driver?

SR-IOV would mean things like 16 gamers, 1 CPU would be possible without needing expensive pcie daughter boards and PLX chips
https://www.reddit.com/r/VFIO/",radeon_rx_6000
"AFAIK, DLSS / RTX doesn’t work using proton/wine so it’s basically meaningless for a Linux gaming rig. None of the games that use either feature have a native port. 
    
Due to Better wayland support and the open source driver ,a dedicated Linux gaming rig is a actually a niche where the AMD cards are definitely preferable over NVIDIA.",radeon_rx_6000
"Nvidia supports the same thing, it is just disabled for Geforce cards. It allows you to pass an instance of your GPU to a VM so they can get full acceleration. This is useful if you are hosting a game streaming box or running Windows in a VM for something that won't run in Wine.",radeon_rx_6000
Where did you find one for sale?,radeon_rx_6000
GN said undervolting is wasn't effective.,radeon_rx_6000
">DLSS is in like 2-5 games at the moment.

Bruh, they just added it to new 4 games.

There are 27 games that support DLSS so far

https://videocardz.com/newz/nvidia-enables-dlss-in-four-new-games-with-up-to-120-performance-boost",radeon_rx_6000
"> producing really invisible effects for huge cost in performance.

Sounds like someone hasn't played a game with raytracing.",radeon_rx_6000
">	DLSS is in like 2-5 games at the moment.

Why are all AMD fans saying this? Lol",radeon_rx_6000
"dlss is in more games than you would think, however its only good in 2-5 games",radeon_rx_6000
"I've heard this one a lot, hopefully it's going to be true.",radeon_rx_6000
"It's kind of inevitable since DLSS runs on tensor cores and AMD doesn't have a real solution for tensor cores. Depending on the game, base resolution, and output resolution, DLSS can add 1-4ms to render time per frame, which can be a lot. And that's with 3080 tensor cores.",radeon_rx_6000
I assume you mean Micron but your point still stands.,radeon_rx_6000
I think Doom at 4k max settings needs around 9gb vram. It might be the only current game that needs more than 8gb.,radeon_rx_6000
"Yea it does,

RTX  
DLSS  
NVENC  
RTX Voice  
Streaming features",radeon_rx_6000
Expensive and hard to find but better than the competition :),radeon_rx_6000
"i don't think the historical state of things matters to most customers and they should've been a bit more aggressive with pricing. $600 would've made it a much better option, but the 10% price difference isn't nearly enough to negate all the other reasons to buy a 3080.",radeon_rx_6000
">	What’s “meh” about it?

Still slower than the 3080 by 5-7%, horrid RT performance, etc.",radeon_rx_6000
"if it doesn't obliterate Nvidia to oblivion, it's meh. That's what some people thinks.",radeon_rx_6000
"It's trading blows at 1440p and below (without RT), yet lost at 4k. I'll concede the power consumption point, but other than that I really don't see anything to be excited about. And it's not even that much cheaper.

I'm sure it will sell like hotcakes given current rtx 3000 availability, but if given the choice of 3080 at msrp or 6800xt, I'd say a lot of people will lean towards 3080.",radeon_rx_6000
"Unusable video encoder, unusable ray tracing, DLSS equivalent MIA, no extra features like RTX voice, no Cuda.",radeon_rx_6000
"doesn't amd have a manufacture advantage?  7nm vs 8nm??

I dont really understand these stuff so if you could can you please explain?",radeon_rx_6000
What did they do?,radeon_rx_6000
"Yeah I agree, the dismissle of ray tracing is very short sighted and surprising considering the importance it will play in gaming from now on. The HUB review kinda annoyed me abit lol",radeon_rx_6000
"What bothers me is how some cite the 6000 series having more VRAM as future proofing but the tensor and RT cores apparently aren't. Yes the 3080 should have more VRAM, but for many AMD won't quite cut it because of lackluster RT performance. Hopefully RDNA 3 brings something that truly matches pound for pound.",radeon_rx_6000
"I think AMD more less already revealed how much cache they will ship on their other GPUs in one of their past slides. [https://www.techpowerup.com/review/amd-radeon-rx-6800/images/arch7.jpg](https://www.techpowerup.com/review/amd-radeon-rx-6800/images/arch7.jpg)

There is an X marked at 128, 96, and 64mb. So I'd imagine their next step down from 128mb in Navi 21 would be 96mb in Navi 22. At least the uncut, full Navi 22 die. That with a 192 bit buss seems to me like it'll be less memory restricted than Navi 21. Probably like 25% less bandwidth, while having half like 45% less theoretical TFLOPs than a full 6900 XT.",radeon_rx_6000
"Na, I'm waiting for smaller Navi. Probably 1 step down from the full Navi 22.",radeon_rx_6000
>	Implying Cyberpunk ever comes out,radeon_rx_6000
"As a side note on Cyberpunk, it’ll probably be a staple of GPU reviews for the entire next generation or two, the same way Witcher 3 is still showing up in today’s benchmarks.",radeon_rx_6000
and most upcoming game are going to use raytracing,radeon_rx_6000
"Nope. I'm just waiting for a few days, and probably the weekend, to actually confirm AMD did massively drop the ball at the 1 yard line. It's actually pretty incredible that they watched the debacle Nvidia went through and thought, ""Yes, I want that too"".",radeon_rx_6000
"Kitguru shows the 3080 coming out ahead at 1440p. Just. 

>We’ll discuss overall value shortly, but in terms of raw performance, the 6800 XT does best against the RTX 3080, relatively speaking, at 1440p. It is still slower on average, but only by 2%. Of the 14 games we tested, it is faster than the RTX 3080 in 5 of them at 1440p.",radeon_rx_6000
"I never said i did. That's why the ""and for 4k"" is right there at the top of the sentence.

Giving the similar prices why would I gimp myself at RT for. Similar price better RT, better 4K, similar 1440p.

If I want to play 1440p. Why would I pay more than a RTX 3070 and not use the saved money for a better monitor or a better CPU?",radeon_rx_6000
"1440p and other lower resolutions always have a bigger CPU bounding than higher resolutions. If a game runs faster on the 3080 in 4K but doesn't in 1440p you can almost always assume a bigger driver overhead (or some other CPU / system RAM bottleneck) that causes this. IMO GPU's should mostly be tested at the highest common resolution (unless maybe you end up VRAM capacity limited) while CPU's should be tested at the lowest possible resolution. 

And this matters also for 1080p or 1440p gamers because newer next gen games will come with a higher demand on GPU power to reach good framerates at those resolutions. Similarly raytracing will simply be part of what it takes to play games at ""High"" and above quality settings. 

And that is all w/o taking DLSS into consideration or Nvidia's version of Smart Access Memory that they say they have working internally for both Intel and AMD CPUs.",radeon_rx_6000
"He did not say that RTX 3080 was better at 1440p, but at 4K. Even then, it is difficult to say 6800 XT is a clear winner at 1440p either. On the other hand, RTX 3080 is clearly ahead of 6800 XT in both RT and 4K gaming.

However, it seems like RTX 3070 is still a better deal than both.",radeon_rx_6000
"Based on the benchmarks I've seen, 6800XT crushed 3080 in 1080p (but that's sort of meaningless because no one at this pricepoint is doing 1080p)

The 6800XT is about 3% faster than the 3080 in 1440p (they trade blows back and forth upto 10% difference, but AMD winning more often) and at 4k the 3080 is consistently better than the 6800XT, but by a small margin, average of 5%.

In Raytracing the 3080 show significant advantage, though the 6800XT does show at least playable framerates at 1440p with raytracing.

&#x200B;

All in all, if you have a choice (which is the key right now with everything sold out) and you only play at 1440p and below, and you don't want to use raytracing at all, then the 6800XT makes sense. Outside of any of that, the 3080 is the better choice.",radeon_rx_6000
"Worth noting that ray tracing is not the *only* way to have better lighting, in the long run surely yes, but in the next few years the new graphic engines will have excellent illumination and shadows using other techniques. See the Unreal Engine 5 demo for an obvious example.

edit: It's fine to downvote but I'd like to see an explanation for that. See my comment below for the discussion around the Unreal Engine global illumination system. When I say ""other techniques"" I'm talking exactly about that, alternatives to Nvidia's rather brute-force approach.",radeon_rx_6000
Isn’t ray tracing also used for hit detection in some 1ps games?,radeon_rx_6000
">sells in 8th December 2020

sells out\*",radeon_rx_6000
"No shit they're not released, that's why I'm asking",radeon_rx_6000
I honestly think you have to be pretty set on buying AMD if you're willing to sacrifice 50% RT performance and DLSS for $50. I think playing just a handful of games with RT or DLSS would make up for that price difference... and this isn't even considering that the 3080 seems to outperform the 6800xt at 4k in standard rasterization as well.,radeon_rx_6000
"and given next-gen console's foray into RT, more & more games will come out utilizing it. 

If you care about RT, Nvidia makes more sense. I guess this generation will devolve into another ""Please AMD, help lower my Nvidia GPU price""",radeon_rx_6000
The regret comes from having an EVGA FTW3 Ultra... *That* was expensive.,radeon_rx_6000
"Just the cost... It's been like ""90% worth it"" but I sold a bunch of old hardware and basically recouped it all.. lol.

It IS an awesome GPU",radeon_rx_6000
"> (if you haven't overpaid)

To be fair it seems like it's the same thing with radeons, if you want one without waiting for months, overpaying is your best bet. Finding either at near MSRP prices is very unlikely. Maybe after christmas when there's cards in stock and prices start to stabilize you can make more meaningful comparison, but for now the prices are all over the price.

Idk how much the situation varies by country/region, but at least here in northern europe it's quite normal that people who actually got one of the new geforce cards paid 50-100€ over MSRP for the cheaper custom AIB cards or about 100-200€ premium for high end customs. And the situation doesn't seem to be any different for radeons.",radeon_rx_6000
That's subjective though.,radeon_rx_6000
"That's selection bias, the computerbase.de review doesn't contradict the results he's relying on for the same reason techspot doesn't contradict the computerbase review.

But yes, if rt is something you care about, Nvidia is the way to go. And diss will probably be better than AMD's solution. Since dlss is included in few games though, that's not much of a selling point. And since the games where it matters are primarily ones that also include rt, that's an even smaller subset of all games. But, it's also important that we note we don't know how optimized those games are for AMD's solution. It's possible that AMD can whittle down the rt advantage some, and given a competent upscaling solution, AMD may reclaim much of that lost ground. 

For raster performance though, it looks like the 6000 series is the best bet since its has significantly more OC headroom. It may benefit much more than the 3080 from aftermarket coolers. Especially the 6900xt and it's lower initial power budget. Based on the reviews I've seen.",radeon_rx_6000
"How much better does ray tracing make these titles look at high resolution, high refresh rates? 1%? 5%?

As well, you still have to compare the experience of higher refresh rate vs. ray tracing.

DLSS is a much more interesting feature, and in another year that battlefield might look more interesting, but I think raytracing is going to be a niche for at least another 2 generations of GPUs.",radeon_rx_6000
All of them show 6800xt stomping 3080 in 1440p and close to 3090,radeon_rx_6000
Winning price and performance is not insignificant like you're acting here. The vast majority of people will never use streaming or ai voice and RT is still very niche and many people don't care about it at all. If you're only playing 1440p this gen then the 6800xt looks like a great card.,radeon_rx_6000
"Winning in price is the most important thing lmao.

""The 3090 wins against the 3070 in everything!... except price""",radeon_rx_6000
"It's a gaming card. What else is there to lose in ?

Literally price and gaming performance is all that matters

You guys are grasping at straws here. Team green lost. Move on 

Not like it matters at these prices",radeon_rx_6000
Did they release a day 1 driver yet?,radeon_rx_6000
"Personally, I see no reason to pick up 6800XT **if you can** get a 3080 for $50 more. People buying $700 GPUs shouldn't be so budget constrained that they can afford a 6800XT but not 3080.",radeon_rx_6000
"Most reviews I've seen the 6800 cards win

Only 4k the 3080 becomes a competitor 

And AMD drivers are always trash so add another 20% performance in a few months as usual

It's a clear win for now but at these prices it doesn't make sense. This whole generation of cards should be a wake-up call

Why would I pay 2x when I can get a WHOLE ps5 console for half the price that does everything better 

For a taste of real ray tracing look at new spiderman",radeon_rx_6000
Isn't one part of direct storage is full access to the gpu memory from cpu?,radeon_rx_6000
"That doesn't make it any more important for people. Its what you get out of it, I'm not taking that performance hit for some shadows",radeon_rx_6000
The consoles are on amds RDNA2. The ps5 runs Nvidia proprietary ray tracing like utter shit. The push will be for an amd friendly ray tracing not proprietary rtx.,radeon_rx_6000
"The consoles don’t have sufficient horsepower to much more than limited effects. Basically what you see in Miles Morales. 

We are still another console generation away from mainstream ray tracing being fully integrated in real time graphics for consoles.",radeon_rx_6000
"With RDNA2 RT performance being this abysmal the adoption on console may not be as great as hoped. I think it’s probably going to be used quite sparingly.

There were some hints of this with analysis of the console release titles but it was hoped maybe they just hadn’t gotten the hang of it and it could get better over time. At this point it’s looking like it’s probably not going to improve much, RDNA2 just isn’t good at raytracing.

Somewhat sad that when AMD flops like this it has such a big impact on the industry as a whole thanks to the near-monoculture in the console industry. AMD didn’t deliver so... raytracing just stagnates for another few years.",radeon_rx_6000
"Oh, please do tell. 

I’m well aware ray tracing is the future. I got to spend a whole semester learning the math behind rasterization processes and I’d love true, honest path tracing for everything. 

But GPUs aren’t powerful enough yet to do it in real time without massive performance hit. They won’t be for at least another generation. So right now, most people turn on RTX, look at some shinies, then turn it off because they care more about 120 fps than RTX barely holding 60. How many PC gamers are STILL on 1080p because they want 144+ fps?

And DLSS 2.0 is fantastic. It’s the most exciting thing in my opinion to come out of nVidia in a long time. But the support is abysmal. Unless you the specific handful of games you play happen to be DLSS supported, it might as well not be there because you’re not benefitting from it.

So, yeah, I stand by my statements. RTX is currently a meme; won’t be that way forever, but by the time it’s important, there will be newer, better hardware for it.",radeon_rx_6000
"https://www.igorslab.de/en/nvidia-geforce-rtx-3080-founders-edition-review-a-great-step-ahead-and-the-gravestone-for-turing/12/

Rtx 3080 1ms spike 489watts.",radeon_rx_6000
[deleted],radeon_rx_6000
"3080 has less ram but it is significantly faster ram.
3080 also has tensor cores for AI their whole suite of features, productivity, etc.  AMD has zero answer for that.",radeon_rx_6000
"It's a significant win in my book. DLSS makes a huge difference at 4k especially if you play games with rtx.  4k is not slightly faster 10 fps -15 fps is big if it drops down to below 60.   


I mean you can argue ""availibility"" but it's already sold out on new egg and -50 bucks for what 10 fps thats like 5 bucks a frame well worth it imo. But i digress. It's not horrible but it is worse @ 4k i mean it does perform better at 1080p. But i don't fit in that subset so it doesn't bother me.",radeon_rx_6000
"I know right.  I'd love for him to link this ""heavily downvoted comment""",radeon_rx_6000
"For a good reason I guess, seeing that both of those claims turned out false.",radeon_rx_6000
[removed],radeon_rx_6000
That is the most pathetic way of defending the lack progress for Hardware encoder. People share gameplay clips across social media and their friends now more than ever.,radeon_rx_6000
[deleted],radeon_rx_6000
"Especially during the pandemic, video chat is more popular than ever. That uses GPU encoding usually.",radeon_rx_6000
"Lol what? Do you even math...

3440 is 34% more than 2560. You just divide the horizontal resolution since the vertical is the same.

So 3440*1440 is 34% more than 1440p, while 4k is 67% more than 3440*1440.",radeon_rx_6000
"That is a more difficult call.

I have played legion maxed and it does use quite a bit of vram BUT that is with everything maxed including raytracing.  

The LTT video has benchmarks for both of those cards if you want to compare:
https://youtu.be/oUzCn-ITJ_o

I personally would probably still go 3070 even with lower FPS in some games due to DLSS making up that difference+more in newer games and other nvidia features being superior such as NVENC codec for recording/streaming/VR.",radeon_rx_6000
"Don't forget 3080 comes with watch dogs legion and a years worth of gfn which more than makes up for it 8n value, but true in msrp the value is very close.",radeon_rx_6000
"TPU's power usage numbers are way off compared to other reviews. Rest of the reviewers have similar perf/W (+- about 3%) between the 3080 and 6800XT

[E.g. computer base](https://www.computerbase.de/2020-11/amd-radeon-rx-6800-xt-test/4/#abschnitt_energieeffizienz_in_fps_pro_watt)

(Side note, computerbase also shows the exact same performance margin at 1080p, 1440p and 4k)",radeon_rx_6000
"NVIDIA will get SAM, and it will work on Intel and older AMD boards as well https://www.reddit.com/r/nvidia/comments/jwr1h1/amd_vice_president_scott_herkleman_nvidia_sam_on/",radeon_rx_6000
"That sounds like reasonable speculation. But in the end, I really don't mind turning down the texture setting from ""*Ultra*"" to ""*Very High*"" for some new games in a few years time. The actual perceptible difference tends to be minimal and I've always preferred high framerates to maxed out video settings in any case.",radeon_rx_6000
Assassin's Creed valhala on 4k with ultra textures already requires more than 10GB of VRAM,radeon_rx_6000
"2080S performance for ~$400 is literally about to happen so idk where you’re pulling 4 years from. With the competition as fierce as it is and RDNA3 potentially coming next year, I have to totally disagree with you.",radeon_rx_6000
"Exactly, why would anyone that's already spending $700+ on a GPU not spend the extra $50 to get the objectively superior card when you factor in DLSS, RT and Nvidia's other software. That's not even taking into account that Nvida's drivers are normally much better. I'd spend the extra $50 just to avoid that headache.",radeon_rx_6000
"Nvidia Nvenc, CUDA, and Broadcast, are just icing on the cake.",radeon_rx_6000
100 percent,radeon_rx_6000
"The vast majority of users don't use raytracing or DLSS, nor do they play on 4k.

6800XT makes more sense for the average user. 3080 makes more sense for enthusiasts or those with higher end systems.",radeon_rx_6000
"Microsoft and AMD are working on a DLSS alternative for consoles (which most likely is what AMD are calling Super Resolution on PC), most likely Sony are too",radeon_rx_6000
"Nvidia was never going to give them a good price. Nvidia rarely make ""small margin big volume"" type of deals. I think they believe they can sell the chips that would be going to consoles at a higher margin elsewhere.",radeon_rx_6000
"8% faster than 3070 per TechPowerUp performance summary (aggregate), with a 15% higher cost.",radeon_rx_6000
People still bought Radeon 7 so they'll be fine,radeon_rx_6000
Looks like we are aligned for a price war TBH. AMD will likely drop prices once the supply has increased enough to better their value proposition. Then the pressure will be back on nvidia. Consumers may actually win in the end!,radeon_rx_6000
"I'm pretty sure they won't from here on. The lower we look, whatever feature imparity that is demanded in higher-end cards in the xx80s vs. x800XTs range is going to be less significant because raw performance to value is heavily weighted in mid to low price segments which is very possible given 6800XT's has one such advantage. 

RT is also a much less significant factor because the performance penalty on already slower cards for both sides wouldn't make much sense to be turned on the already limited occasions that RT is featured. Live streaming though, that's up for discussion and vary depending on the user, though I'm pretty sure outside of enthusiast circles, it simply also isn't something the majority would care.",radeon_rx_6000
"So instead of spending 500 bucks on the 3070 if you can even get it for that price, you should buy 3080 because everything in between is just too close",radeon_rx_6000
"Anything special about that timeframe? Is that when they drop them online? What cards did you get btw? (sorry for the barrage of questions, just anxious to upgrade to anything really).",radeon_rx_6000
"Honestly I think the best chance OpenCL has is if Intel and AMD team up to push it hard. There are a lot of new supercomputers with RDNA2 GPUs in them and Intel's best chance in the market is to leverage their software prowess (see MKL) so if they worked together to make OpenCL good... that could be big. The problem IME is there is a higher bar of entry for OpenCL, because there aren't first party BLAS implementations, nor libraries like cusolver (so I guess LAPACK) or cudnn. I think ROCm has support for some cublas APIs, but not all, and the documentation for RoCm isn't that good IMO.

I was hoping I could buy a 6900XT but I ended up getting a 3080 because CUDA has everything I need.

It frustrates me to no end that AMD has great price/perf for compute, and has for a while, but they haven't spent enough effort on the software to support their hardware, which is also true in the CPU market.",radeon_rx_6000
Thanks for explaining it. So would memory bandwith be more of a factor? If so I wonder if the infinity cache would outperform the GDDR6X,radeon_rx_6000
"> Rather it just increases the ability of the CPU to utilize the PCIe bandwidth 

Can you provide any proof for that?

Afaik there are two ways to get data from/to VRAM.

The standard DMA I/O involves using the DMA engines of the GPU. Those are microcontrollers that copy the data from RAM into VRAM and vice versa. I am pretty sure you can saturate PCIE bandwidth with them if you wanted.

The modern memory-mapped I/O (with GPUs supporting unified memory) is where the whole BAR thing comes into play, as VRAM is mapped into CPU/process address space so that the CPU can directly access it as if it was RAM. This reduces latency as you don't have to go through the DMA engines to move data.

This mapped I/O is typically limited to ""small"" memory windows. SAM (resizable BAR) is about removing this limitation.",radeon_rx_6000
"DLSS was a result of Nvidia's experience in deep learning and AI, it's in the name. To my knowledge AMD isn't nearly as much a player in that space if at all. But they could surprise us. 

As a side note, Nvidia has been pushing that similar tech elsewhere too. They have an AI upscaler on the Nvidia shield that pretty much kicks the shit out of any other upscaling technique used on TVs.",radeon_rx_6000
"To be fair, I also had doubts of this generation beating Ampere. But they seem to be doing quite well.

I'm excited, but I get your doubts. We'll have to wait and see",radeon_rx_6000
yeah the cheapest 3080 is 819€ - now it only has to be available.......,radeon_rx_6000
"Interesting, I was under the impression they could just leave some of the ram chips off to cut down cost.",radeon_rx_6000
Yea I feel like the 3080 is actually the best card to game at 1440p,radeon_rx_6000
"By ""quite a bit"" you mean 157 fps vs 153 fps average?",radeon_rx_6000
"Downvoted for quoting a literal fact? This board sure is something else, interesting.",radeon_rx_6000
Geez that’s pretty bad,radeon_rx_6000
SFF enthusiasts no doubt undervolt the shit out of those 3000 cards.,radeon_rx_6000
"This is true. I've been saying I don't see much reason to get a 6800XT over a 3080 in these comments, but this is one situation it would definitely make sense. When you factor in the cost of having to get a new PSU for the 3080, the 6800XT becomes much more enticing.",radeon_rx_6000
Depending if Nvidia can get TSMC allocation for their next architecture it might be a moot point. but what really matters is than your psu can handle power spikes and not trigger overcurrent protection.,radeon_rx_6000
[deleted],radeon_rx_6000
"Server is CDNA, not RDNA",radeon_rx_6000
"Not really though. Mobile chips are just going to be down clocked anyways, servers are going to use Quadra or Tesla cards.",radeon_rx_6000
"This.  By the time 10gb become limiting for games you actually care about, we'll be 2 graphics card generations down the road.",radeon_rx_6000
"Rasterization will bottleneck before VRAM. That's the point I think you're making.

DirectStorage will potentially make VRAM bottlenecks a thing of the past in most cases.",radeon_rx_6000
DirectStorage will not reduce VRAM requirements. Spilling to SSD is slower than spilling to system memory.,radeon_rx_6000
">DirectStorage/RTX IO will hopefully make this a non-issue, hopefully in 3 years we will see widespread adoption.

That's not how it's going to play out.  It's not like PC games will have DirectStorage uniquely.  Multiplatform games are gonna start being \*built\* around this sort of paradigm, and will be pushing memory demands a lot harder on a moment-to-moment basis.  It will not alleviate memory demands, it's going to do quite the opposite.",radeon_rx_6000
"Did you hear the part ""in the future"" ?",radeon_rx_6000
"No at 4k pure rasterization the 6800 xt is not better. Not on average in any of the benches I've seen.

Not sure where that conclusion is coming from.",radeon_rx_6000
"Is it AMD supersampling or microsoft?  Because I have zero confidence that AMD will produce something matching DLSS.

DLSS 2.0 took an enormous amount of computing resources to train on Nvidia's super computers and AMD is no where close to having Nvidia's level of expertise or resources in ML.  MS has a chance to match what Nvidia has done.  AMD is a long shot.",radeon_rx_6000
Try undervolting to 1950 MHz @ 931 mV. Clocks are more stable for me so if anything there’s a theoretical performance improvement.,radeon_rx_6000
That's awesome.,radeon_rx_6000
"It's their first generation after all. They focused on beating/matching the Ampere cards in rasterization, especially at lower resolutions.",radeon_rx_6000
Because Minecraft and games like Quake 2 are full raytraced. Meaning they measure the actual raytracing performance without rasterization overhead.,radeon_rx_6000
I mean it's pretty bad... if you look at control it falls between a 3070 and 2080 super.,radeon_rx_6000
"It's one of the more aggressive implementations of RT, which is why I used it, but you're right, I'll change the link into an album.",radeon_rx_6000
Not at 4K. But at 1440p and especially at 1080p it is perfectly usable. 60-80 FPS is perfectly reachable with TOTL hardware.,radeon_rx_6000
Some people dont.  If you spend the majority of your time playing competitive games you probably don't care much about ray tracing right now.  you care about raw frame output.,radeon_rx_6000
"I generally don't either, but it does make sense in some slower paced games. Either way it's not something I'd give up for only a 7% savings.",radeon_rx_6000
"The issue is if you're spending $650 anyway, what's $50 more to get superior ray tracing, dlss, and generally better drivers/software?",radeon_rx_6000
"VRAM capacity is likely only going to be a factor in 4K later on, and the 6800XT is  behind on 4k performance.",radeon_rx_6000
"To be perfectly fair, you probably can't find a 6800XT since about 09:30 EDT this morning either.",radeon_rx_6000
Probably to give a purpose to cards that didnt bin well enough be a 6800 XT.,radeon_rx_6000
Ya I was just talking about near the end where he was talking about neesh use cases.  I was not trying to troll or hope i did not come off as.,radeon_rx_6000
So thats what people meant by paper launch,radeon_rx_6000
Well only like 3 people have internet and to access it you gotta send smoke signals to one of those people and wait for their response,radeon_rx_6000
"I'm not really the target audience for overpriced hardware, but I'd take the $50 if everything else was equal. 

But with the whole set of features NV has, there's something in there for most people to make them go NV. And I think most people havent forgotten the RX5000 drivers yet. The only major advange for AMD is linux, but that's a very small group of people.",radeon_rx_6000
Agreed. I’m not streaming. I’m not video encoding or recording. I’m mostly going to be using it for gaming. I’m coming from a GeForce 960 (on 1060p 60hz) so I doubt I’ll even know what I’m missing with DLSS and RT after moving up to 1440p 144+,radeon_rx_6000
"Imo the fact nvidia isn't struggling makes this lunch even more impressive. A month ago people were blown away by 3080 and today AMD is standing tall next to nvidia while sandbagging the last few gens.

Software offering, yes. There's nothing even to discuss, AMD is behind. Same with RT. Imo they should have went a bit more aggressive on the price, but I don't know the margins on these cards, this cache is huge.",radeon_rx_6000
So AMD would consistently stay a generation behind then.,radeon_rx_6000
">	same settings on PC and console and xbox was roughly similar in rt-performance to rtx 2060 super. It can do low rt settings at 4k 30fps.

This is misleading. Though the ini file settings are the same, but the Series X has far worse image quality to the 2060S. Alex even said in the DF video review that there is no setting or ini tweak he can do to get the RT to look as bad as it does on the Series X. So sure the framerate is the same, but the quality settings at that framerate are definitely not equal.",radeon_rx_6000
"Just like people who bought 2080 Ti before Ampere and DLSS 2.0, right?

AMD can't win in ray tracing on perf anyway if they don't have DLSS-like feature. 

But, just like when we give DLSS time before it's actually good, let's see if some time is enough for AMD's DLSS implementation is actually good when it's released.",radeon_rx_6000
"I mean, it's extremely viable for 4k still. Just no RT*.",radeon_rx_6000
"I think that if you manage to get AMD this year, you only get it for rasterization.

This also bodes incredibly bad for future RT titles. If any game is going to sell for the new consoles, which are RDNA2 based, RT performance will be abysmal on them. Considering RT was a feature for them..",radeon_rx_6000
"Because you want raw power for playing 1440p at 165Hz without any dips? Or for VR?

I mean like wtf is going on here in the comments?

AMD released competitive powerful gpu that price+performance wise slides nicely between 3070 and 3080, with far better temps and power consumption... and redditors here whine about raytrayicing. Did I miss the big shift where people started to care? Because half the comments under RT on/off comparisons submissions were always how theres barely noticeable difference and how its not really worth the fps hit.",radeon_rx_6000
"Steve from Hardware Unboxed said he's had his for at least a week and he already updated the game benchmarks to include Godfall, Valhalla and something else new.  He's going to have a great video of benchmarks.",radeon_rx_6000
"written articles are better for in depth, but i like videos for killing time",radeon_rx_6000
6? Rookie numbers.,radeon_rx_6000
"What's missing is the driver code taking advantage of it.


I'll have to see if maybe my windows partition starts taking advantage of it when ever I get around to updating. There is 0 reason for SAM not work for me, it works under Linux just fine.",radeon_rx_6000
They only confirmed ampere till now.,radeon_rx_6000
"GN is about performance at a normalized noise level, so they turn down the fans, which leads to less noise, but worse temps.

If the user (or reviewer) doesn't care about the noise and keeps it on auto or even increases the fan speed, they will get better temps.",radeon_rx_6000
"Apparently that was an issue in which Nvidia's default settings reduced the color gamut available. From a thread I found, it was detecting everything plugged in via HDMI as a TV instead of a monitor and automatically did that.",radeon_rx_6000
"Nvidia didn't sit on their asses because AMD still was able to compete with Nvidia outside of the high end.

And while being large, was no where near Intel size until relatively recently. Intel is a victim of its own success in a way.",radeon_rx_6000
"No, Ampere did poorly at lower resolutions compared to Turing as well. RDNA2 just scales like other non-Ampere architectures.",radeon_rx_6000
Ampere has an extra FP path in each CU that will be more used in 4k.,radeon_rx_6000
"No. SAM helps only for CPU-GPU communication in terms of CPU being able to directly modify data in VRAM instead of going through some designated buffer. 

Here the point was how fast the GPU can access VRAM.",radeon_rx_6000
Nvidia will be getting SAM as well.,radeon_rx_6000
"And that's the only reason GCN has been able to hold on half as well as it did.

As a gaming uArch, GCN has many, many shortcomings.",radeon_rx_6000
"That's not really what I'm talking about. The prices will settle eventually, lets say 3 month from now. However, when that happens there may not be many 3080 SKUs that cost $700. For example, I believe that every EVGA 3080 was listed for at least $730, but there was a temporary $30 rebate which lowered the price to match the FE, and that has now expired. Their website currently has a similar deal for a 3070. If AIBs produce limited quantifies of their $700 SKUs relative to their overall 3080 production numbers, the actual prices will never come down to the MSRP levels. In a way, Nvidia's MSRP was a misdirection designed to create the impression that product is cheaper than it really is, because most of the reviews will use the $700 figure in all their price-performance analyses. It remains to be seen whether AMD took the same route. If that's not the case, it's entirely possible that in a couple months 6800XT for $650 will be widely available, while 3080s at $700 will be MIA. In which case the actual price difference will be greater than the manufacturer suggested $50.",radeon_rx_6000
"It’s absolutely a no-brainer that no one should go with the 6800 (xt). 
If you don’t want ray tracing and/or 4K go with a lower end card or last gen offering. 
They are great cards but not at that price point and what they were made for which makes them a bit superfluous.",radeon_rx_6000
Which is a scumbag move considering Ryzen 3000 is the same damn chipset.,radeon_rx_6000
So? I have an Nvidia card anyway. This features is not something AMD invented or has a monopoly on. It's a feature of how PCs are build nowadays.,radeon_rx_6000
It's hard to know how prices will creep up at legitimate retailers in coming weeks. You can't really base review content off that. The best they can do is just lay out the performance and tell people to buy whatever is cheaper.,radeon_rx_6000
"How is it too powerful for 1440p? The 3080 seems absolutely perfect for a 1440p, 144Hz monitor.",radeon_rx_6000
"Man, I'm jealous your GPU even makes the charts",radeon_rx_6000
"Maxwell gang, I want to upgrade but I can't find any GPU :(",radeon_rx_6000
"How? It's not like Xbox is running nvidia. Pc version of Valhalla also suffers from xbox bugs like camera issue.

What is interesting is recent releases from cross gen run better on AMD. Valhalla, Dirt 5. I'm not sure, but I think WD legion also ran better on AMD RT withstanding.",radeon_rx_6000
But 16gb of VRAM!! I'm glad these reviews are out and people will buy this though so less competition for the 3080,radeon_rx_6000
Current estimate is working through the backlog sometime in February.,radeon_rx_6000
"Games like AC Odyssey get somewhere around 80fps on 1440p. For people with high refresh displays, there is still quite a bit of headroom for performance.",radeon_rx_6000
Thanks Ü,radeon_rx_6000
">Raytracing is basically the next big thing now that the new consoles have it, all of the new games will market for it.
>

Don't the new consoles use the same RDNA 2 architecture? Sure, they're customized for Sony/Microsoft, but it's pretty reasonable to suspect that the 6800XT is more powerful than the custom console GPUs.

Microsoft has also said that they're helping AMD bring any of the console's benefits to PC.

So while ray tracing is the next big headline (it looks great and can really bring scenes together), if the 6800XT has issues I would expect the consoles to fare worse. I'd guess that AI upscaling will likely have a much bigger impact on the console landscape (I believe that AMD's competitor to DLSS 2.0  will be rolled out to consoles?).",radeon_rx_6000
"DLSS is major. RT I don't care about. When I see videos comparing RT on/off, it is a toss up whether it looks better or not. So far it is a gimmick.",radeon_rx_6000
"With pressure from Nvidia enabling it (or whatever they'll call it) later for both Intel and AMD CPUs, I can see SAM happening for at least Zen 2 too.",radeon_rx_6000
For Zen 2? Plus it's largely within AMD's gift to pressure board manufacturers to patch support in if it's only a matter of software.,radeon_rx_6000
[deleted],radeon_rx_6000
"Yeah,you're right about NVIDIA's R&D, let's see, there have been some leaked patents from Microsoft and Sony showing more efficient way to get Ray-Tracing done on less capable hardware, hopefully it gets here fast(if it does) , as a console gamer,I'm really curious about how this affects the consoles...",radeon_rx_6000
"Where is this explained? I don't think this is correct, AMD built in the RT accelerators into the compute units rather than the Nvidia route of dedicating specific cores..AFAIK",radeon_rx_6000
"Yeah, I know, but I'm curious how Series X has ML if it isn't capable of AI?",radeon_rx_6000
"Language disambiguation:

https://www.themathdoctors.org/three-times-larger-idiom-or-error/",radeon_rx_6000
"It looks like the 6800XT doesn’t perform better in Godfall at 4K despite all the extra VRAM, which was a title that explicitly called out VRAM requirements.",radeon_rx_6000
3080 has significantly faster VRAM though.,radeon_rx_6000
Extra vram capacity is honestly really overrated outside of Creative work lol...,radeon_rx_6000
">Well, some people feel more comfortable with the extra VRAM.

That's because they don't know what they are talking about - the 4k raster results should make it clear to those folks that VRAM Bandwidth is more important than VRAM capacity over a certain threshold.",radeon_rx_6000
Who? Professional cgi folks? That’s not exactly a huge market,radeon_rx_6000
"Nah look at watch dog legion, it's optimized for console, shouldn't be biased.",radeon_rx_6000
"15% in some non-RT titles is huge when the 3080 is only around 6% or so ahead at 4k.

Remember that GA102 is 92mm^2 bigger and has 1.5 billion more transistors. Even current performance is already impressive.",radeon_rx_6000
"Yeah, I feel so bad for these reviewers.",radeon_rx_6000
First it's the toilet paper .... Now it's our gpus and consoles.... What's next????,radeon_rx_6000
It's not far out to predict that AMD's cards will also be hard to get until the end of year.,radeon_rx_6000
"AMD's Vega GPUs were nearly non-existent in Canada, like it did show up and was available but so, so few of them seemed to make it here it felt basically irrelevant. I ended up replacing my R9 290 with a GTX 1080 instead, cause it was, you know, actually available, and went on sale $50 off right before the mining craze price hikes took off.",radeon_rx_6000
"There is a clear double standard at play here.

When Nvidia launched, the blame was placed 100% on their shoulders.

People said they should have predicted the high demand, that they should have built up more stock before launching, that they should have done X Y and Z. 


&nbsp;

Now that AMD is launching, I'm seeing more and more people say what you're saying. 

This is an unprecedented situation, we can't expect AMD to respond well given covid, it's not possible to have enough GPUs for everyone etc.

&nbsp;

We can't have it both ways. AMD can't be given the benefit of the doubt, whilst Nvidia gets raked over the coals, for the exact same issue.",radeon_rx_6000
The games with raytracing will be made for Consoles first. Guess  who is providing RayTracing hardware for consoles ?,radeon_rx_6000
"Ray tracing is only in a few games, and destroys framerates. But Nvidia marketing has convinced everyone its an essential feature to have, even over rasterisation performance which effects every game ever.",radeon_rx_6000
1000000000%. Nvidia was able to make ray tracing a serious decision maker. AMD definitely has a problem tho that they lose on almost all of the additional features and are not the clear favorite without the features.,radeon_rx_6000
"Point 1 doesn’t really stand when Watch dog legions is optimized for consoles and also use ray tracing. The performance drop is just as bad. Citing Dirt 5 is classic cherry picking. So you found 1 title that performed better out of how many?

>	better ray tracing performance in existing titles.

So you really think future titles will have 6800XT perform equally or better than the 3080?",radeon_rx_6000
"> Big Navi is still much better than consoles, so you don't have to worry about that.

I do if I want to play at higher resolutions, frame rates or simply higher settings. A ton of raytracing optimizations is simply what objects are part of the BVH, how many rays per pixel and what objects get raytraced reflections. All those things but the last two especially can be exposed to the user easily via the settings menu / ini, like pretty much all raytracing enabled games have done so far. WD:L shows how this can be easily used for higher than console settings.",radeon_rx_6000
"Yeah I'm not expecting anything with it

Watching the GN review right now, and right as I read your reply, GN Steve said it improved frames on SOTTR by a whopping -0.2 FPS the comedic timing is priceless",radeon_rx_6000
"not trying to be disrespectful, but that was clear the moment the leaks and reviews got rolling (even before the launch).

Future proofing isn't a lie, it's just a small window of opportunity to get a satisfiable price/perf over few years, contrary to the belief that you could future proof with every high end part.

I bought a b350 which could even support 5000 gen with a bios mod, that's quite decent, though if I had bought an x370 one that would have been a terrible financial decision. Similar to your situation, a 2070 is far from a bad choice for future proofing compared to a 2080 ti due to diminishing returns.",radeon_rx_6000
"Also a 2070 isnt really a future proofing, especially given how products higher in the stack couldn't really even ray trace properly.",radeon_rx_6000
"Pretty much this. Please return in a year when more than half of AAA releases support DLSS and RT. And as for the latter the ones that favor Nvidia-centric implementation, not like Dirt 5 case.",radeon_rx_6000
"[I get what you're trying to say, but that's equivalent to saying ""if AMD had the better architecture, Nvidia would be in a lot of trouble.""](https://www.youtube.com/watch?v=A-RfHC91Ewc)

All that matters is the end-user performance.

Which is none, because none of these cards can be bought.",radeon_rx_6000
"AMD also has tremendously more efficient cards than Nvidia this generation, so that's a moot point.",radeon_rx_6000
"Not checked at all. What I did though was run the SOTTR Benchmark. With no or lower undervolting my pc would simply turn off. 
With around 100mv undervolt it doesn't and I didn't notice any difference in fps but that might be the cpu bottleneck.

Ill look into numbers when my system is complete.",radeon_rx_6000
lol you not seen some of the partner  cards power draw then. 1 asus one (3080 draws 500 watts under load.),radeon_rx_6000
"While they aren't what I would likely buy, I am looking forward to the new mid range cards from both NVIDIA and AMD, AMD's been doing really well in that department for a while as the generally better bang for buck (though this varies, and also I am going by Canadian prices). Hopefully that market stays very competitive.",radeon_rx_6000
"> I stopped buying games until my backlog is caught up. Got sick of looking at unplayed games.

My Steam library is currently at ~250 games. I've played maybe 25% of those and finished around 10%. I envy your self control.",radeon_rx_6000
"Than why would you buy a new GPU at all? Its not like some Pascal GPU or lower can't handle those games perfectly. 

IMO it makes little sense to play games years after release but get a new GPU (no matter if midrange or high end) early on.",radeon_rx_6000
"Gotta work with the devs on this one,
Remember minecraft rtx was actually worked on by nvidia engineers on their in house first gen hardware...its a beautiful game of course, but in the future i see devs just working using dx12 ultimate, like there is good documentation and wide hardware support
Just saying either card will make any gamer happy down the line.",radeon_rx_6000
">	My prediction is Nvidia sponsored games will push devs to destroy AMD RT performance deliberately but AMD sponsored games will have RT effects that run pretty well on both cards, 

More like amd sponsored titles will just lock out NVIDIA users like godfall. Don’t have to look bad against the 3080 in the chart if it doesn’t run on a 3080 /taps forehead

Funny how every NVIDIA sponsored title opened it up to any dxr card and yet the AMD title locks it to only their hardware.",radeon_rx_6000
"I agree with your points, especially how RT is under estimated.

I also managed to be lucky enough to get my order for the 6800 XT.  I am aware of the difference and tbh I was looking to get the 3080 and changed my mind last few days due to the leaks. In Sweden it's practically impossible to get a 3080, it also costs 100-200 usd more in general. There are 2 models near the MSRP but ofc those are never in stock.

To me, I considered this factor and it was way more likely that I could get the 6800 this year. for the good 3080 models there are queues for queues. Does it then matter if NV has better RT and DLSS when you can't even buy one. I got the 6800 XT for 8000 sek... compared to the 3080 model I was looking at was 10800. For me the price difference is so huge that it was easy.  Most 3080 models are 20% or more in price.",radeon_rx_6000
"With AMD GPU's being in both next gen consoles, I'm kinda thinking we'll see pretty decent support for AMD's ray tracing in the future since devs will have to target it anyway (well, for cross platform games anyway).",radeon_rx_6000
"It doesnt matter what ""most"" will do as long as both Vendors are sold out.",radeon_rx_6000
"Yeah I get you. My context is usually a 3 generation leap so even these are amazing to me! I think I will go for a 3070 or a 3060ti (from my Rx 480, which is actually a bit more than 3 gens now:0) simply because they are cheaper",radeon_rx_6000
"They haven't necessarily brought these optimizations to PC in general and AMD cards in particular. Like, we don't even know if they ran the game with raytracing on AMD cards at all.",radeon_rx_6000
Dirt 5 seems to be terribly optimized though.,radeon_rx_6000
"It is not slightly better.It is mostly 10-15 fps better with 6800XT matching rtx3090 in many games.Also AMD optimised ray tracing will make a obviously l difference.Any optimizations makes difference.Even then RT is supported in only few games.Like 4-5 games in the top 200 most reviewed games.It took months for Nvdia to bring a playable RT experience to the market.AMD just released their cards, give them a few months.Also super resolution is also in works.",radeon_rx_6000
"I guess it's very highly objective, but to me until around 80-100 fps, frame rate comes before *anything* else when it comes to improving games' looks. I'd go down to medium/low settings if I had to in most games if that helped me reach that threshold. Once above those framerates I'm all for adding more eye candy at the cost of performance.

But it's definitely highly subjective. I'm sure plenty of people prefer maxed out settings at 60 fps over relatively high settings at 100 fps and there's nothing wrong in that opinion either.",radeon_rx_6000
"> AFAIK FidelityFX Superres is made with DirectML

Is there a source for that?",radeon_rx_6000
"Nvidia tensor cores are record breaking AI hardware. Of course DLSS will run on the Tensor cores, this allows for 'heavier' AI based solutions with negligible cost in latency",radeon_rx_6000
[deleted],radeon_rx_6000
[deleted],radeon_rx_6000
Dont know about them,radeon_rx_6000
"That's pretty cynical. For no other reason than giving a review that's more positive than others. Being an outlier does not imply a dataset is wrong or even fishy, just that the conditions that led to the result may be different. In this case, could be game selection, could data tampering. But, you don't have the information to make such an inference.  And frankly, that's unhelpful. If there's data inconsistencies between his and others, maybe report it to him, and get him to investigate.",radeon_rx_6000
">Especially since they had so much time to prepare.

??? 

I don't understand this. What is so much time to prepare? You do understand this hardware is typically developed in 4-5 year cycles in the background? AMD 4-5 years ago was financially in a terrible situation treading water.

Y'all comment about hardware in a vacuum like they just chose to create something inferior, and not that there were tradeoffs made with what they could mass produce competitively given their resources.",radeon_rx_6000
"It's a hardware comparison. Of course HU know how to conduct fully fledged comparisons because they do it all the time.  Just look at the 5000 series comprehensive testing. 

You wouldn't leave out multitask benchmarks on a ryzen cpu would you? It's a poor review.",radeon_rx_6000
"The reason nobody gave a shit before Ampere is because a good number of games that used it were nigh unplayable with RT on unless they also had DLSS *and* you owned a $1200 2080 Ti. We also saw both next gen consoles announced this year, both with raytracing support, which means it’s going to very quickly go from the niche feature it was in the Turing era to being a standard part of graphically intensive games.

It’s similar with 4k. When I was last shopping for a GPU, my target was the highest resolution I could manage at 60 FPS and max settings for at least a few years while maintaining a reasonable-ish price. Again, hardware at the time was not good enough that 4k60 was doable in every game even with a 2080 Ti. IMO it’s still a little premature to jump on 4k, but I see why people are doing it; the 3080 and 6800XT are big improvements and enable good 4k performance in more games than ever, albeit we’re not at the point where 4k60 with raytracing is especially viable yet.

It’s not just Nvidia marketing. The GPU market landscape has actually changed quite a bit lately, and that’s a significant part of why the attitudes on these things are different now.",radeon_rx_6000
"""History always repeats itself"" my ass, lol",radeon_rx_6000
I feel like I'm in some weird universe where AMD fans will come up with whatever excuse they can to justify buying inferior products from their favorite company. If the 3080 and 6800xt were both in stock I don't see why anyone would buy the 6800xt.,radeon_rx_6000
"I unfollowed the moment HUB said Ampere is not a gaming gpu, like WHAT!? It's clearly apparent at the bias towards AMD. I only really watch Gamer's Nexus now.",radeon_rx_6000
"But it's not irrelevant for people that care about immersion and high fidelity. eSports titles, yes I agree that those features aren't relevant but for games like The Witcher 3, Horizon Zero Dawn, SotTR and even Cyberpunk, visual effects matter. They're RPG and story driven games. Combat definitely is an important part of RPGs but people will happily drop from 100 fps to 70 fps for a better visual experience.

Edit: sorry man! Responded to you twice... It was a point I meant for another commenter but for some reason I ended up replying to you instead.",radeon_rx_6000
"Then why is there so much hype around CyberPunk, a story based game that doesn't need high fps to be enjoyable and is built to look breathtaking? If you're also spending this much money on a GPU then why not use the features that are baked in?",radeon_rx_6000
"That is completely false as proven by the fact that consoles target 30 fps more often than not, but push graphics as far as they can. The majority wants good graphics and today that means shiny reflective metal, glass, mirrors and puddles everywhere.",radeon_rx_6000
"Should have clarified it sorry. It just baffles me that they don't support the output out of the gate like Xbox. I know I am in the minority of players, who plays on their desk. I mean they even Demoed UE5 in 1440p.",radeon_rx_6000
Yeah that's the case with Sekiro I think. Where it renders at 1440 or 1800p(don't remember which) and keeps consistent 60 FPS and good frametiming.,radeon_rx_6000
"Still not shocking, the 2080TI leads the 3080 and 3090 in Borderlands 3 at 1080p according to TPU. It’s a trend we saw at Ampere launch; it needs higher resolutions to really stretch its legs.",radeon_rx_6000
"My point is that DirectStorage enables offloading of file decompression to the and doesn’t stack the whole time required to do this via cpu and still have to copy from dram after that. If you can use IO faster without penalty, you need less VRAM for the same result in a game. Being able to remove data and - should it be needed again - reloading it more quickly, rather than holding it in VRAM because the penalty of reloading is too great currently should open that path up to developers. Higher VRAM demand is then driven by higher quality, not by the need to work around loading times, waiting for the cpu and memory management.

In admit that it assumes that game developers bother with this kind of optimization rather than mostly doing the same as now, but with less loading corridors and shorter loading screens. I guess it depends on how bothersome and time-consuming the API will be to use for more granular operations.",radeon_rx_6000
Skyrim mod creators firing up those AI 32K textures soon.,radeon_rx_6000
"> I don't think most people care too much about power consumption

I think they do when it's AMD - they made that very clear whenever they said the Vega 64 was a power hog. Now with the power consumption of the 3080 it suddenly ""doesn't matter"".",radeon_rx_6000
"Eh i would largely agree with you but were starting to hit a threshold where the old single GPU standard, 550w, no longer cuts it.  Even 750w is a tough sell on someone with say a 9900k and 3080.  With power supplies short in supply and high in demand this becomes amplified.    At the very least its worth noting when making objective comparisons.",radeon_rx_6000
"They do when AMD is the one that has higher power consumption, just not when its Nvidia.",radeon_rx_6000
"> especially since the majority of people don't leave their systems running 24/7.

I don't know a single gamer that ever shuts down his PC.",radeon_rx_6000
"Where are you seeing that?  TPU has an average delta of ~90w during gaming, ~70w @ peak load.

https://www.techpowerup.com/review/amd-radeon-rx-6800-xt/31.html",radeon_rx_6000
"No, you can undervolt some cards with good result. Undervolting is always about silicon lottery.",radeon_rx_6000
"Nvidia did everything they could to ensure higher costs. They thought they weren't going to have competition, so they went with Samsung in hopes of squeezing a better deal out of TSMC (this would have been finalized a 12-18 months ago at least). Nvidia's die is almost 15% bigger (536mm^2 Big Navi vs 628mm^2 GA102).

They went with GDDR6x instead of GDDR6. Micron says 6x memory is more expensive to manufacture in general (PAM4 and higher clocks aren't free). GDDR6 being in consoles means it also got a huge economy of scale boost. GDDR6 is $10-12 and 6x is probably closer to $20 from what I can tell. This means AMD paid $160-190 for 16GB of RAM while Nvidia paid close to $200 for 10GB (or $480 for 24GB).

The proof of how bad is really shown in the ""budget"" GA104 (3070). If you take away 130mm^2 for Infinity Cache, AMD's chip is about 406mm^2 with 80 CUs. The GA104 is 392mm^2 with just a hair over half the CU of GA102.",radeon_rx_6000
"Leaked prices put 10nm at $6k per wafer and 7nm at $9.3k per wafer. Samsung claims to be 8nm though they're closer to 10nm from other manufacturers. Their yields are considered bad which in the industry means worse than 0.5 defect per square centimeter. Plug that into a wafer calculator and things look grim. 84 complete dies of which only 8 are defect-free. That's $71 if ever die is usable (a pipe dream given the huge chances of multiple defects per die) and a staggering $750 per die if only defect-free chips are used. If they can use 40% of defective dies, their cost per die is $156.

In contrast, on TSMC N7 (an incredible 0.09 defect per square centimeter at the beginning of this year) gives 96 complete dies with 60 bring defect-free. That's $96 each if all defective dies are usable and $155 if zero are usable. Given the 6800 and 6800xt, I'd guess most defective unit get used. I'd even wager they are even taking complete chips, binning the highest clocking CU, and lasering the rest. At 40% of defective die reuse (it's likely higher for AMD as multiple defects are less likely), that's $125 per chip. If they hit 60%, that's about $115

As you can see, wafer cost isn't everything. It's likely that AMD could drop prices $50 and match nvidia's profits on this factor alone. Their RAM costs are undoubtedly lower. Narrower bus means fewer PCB layers. Lower power means reduced VRM and cooler costs. An 8gb version would drop $80-100 from the price.

A cut down 6800 with 50-55 CU, 96MB cache, and 8GB RAM, could probably sell for $350 with identical profit margins while being about the same speed as the 3070 and using otherwise throw-away dies with less cache. If they did a difference PCB with a 192-bit bus (harvest dies with bad RAM controllers), that plus an even lower TDP might drop prices even more at the same profit.",radeon_rx_6000
1440p 144hz is as equally high end as 4k 60 imo. It's just a different taste.,radeon_rx_6000
"Not really, you can very easily build a midrange pc (high end of midrange but still) around a 3080 on the assumption that the card will last you the next 3-4 years on 1440p. The price jump between what you need for a good 1440p setup and what you need for a good 4k setup, to say nothing of the cost of the monitor itself, is pretty big even assuming the same graphics card.",radeon_rx_6000
"You can't ""optimize"" software for CUDA, it's either written for CUDA or it isn't. Most machine learning training software straight up won't work on amd.",radeon_rx_6000
"Well, then disregard my comment you lucky bastard.",radeon_rx_6000
"\> it's clearly not a titan card

&#x200B;

Yeah, it's cheaper than one.",radeon_rx_6000
Dont worry Im sure if you really need those features Nvidia would be happy to sell them back to you on a quadro card.,radeon_rx_6000
"[https://www.youtube.com/watch?v=FqpfYTi43TE](https://www.youtube.com/watch?v=FqpfYTi43TE)

Lots of posts on the Nvidia sub as well",radeon_rx_6000
"I’ve been wondering the same, and it looks like it’s due to them using a 9900k in order to have comparative results. The cards are getting bottlenecked and I assume AMDs aggressive power savings are kicking in when cpu limited. They also had the 3080 beating the 6800XT at 1080p, whereas basically other other outlets had the 6800XT winning at 1080p.",radeon_rx_6000
but I like Pibb :(,radeon_rx_6000
"Lmao
U say overclocking and then better value?
Those 2 things dont go together
Overclocked card basically means more wattage and like 50-100 bucks more for a performance boost of maaaaaaaaaaybe 5% at max",radeon_rx_6000
"I see your struggle to decide and I currently share that struggle. I don't game at 4k, maybe 1440p in the future, don't stream and probably won't use RTX, so AMD would be an apropriate upgrade, too. 

If supply comes back and prices normalize there are probably gonna be 3070 and 6800 AIB-models that are similar in price, and I hope by then we know if AMD's drivers are worth buying the card. 

I like the competition but the last years the choice was relatively easy about which GPU to get. 
What a priviliged 1st world problem :D.",radeon_rx_6000
"Agreed. If RT in games is what you are after, there is little to no reason to pick AMD over Nvidia.

The reviews on HUB and GN had noted that rasterization performance is relatively decent for RX 6800 XT in comparison with RTX 3080 and RTX 3070. The victory of ""value"" in Nvidia card, specifically RTX 3070, was not that much of a margin in 1080p and 1440p.

CMIIW regarding the overclocking, but I have seen the video that based on the numbers, RX 6800 XT had slightly better overclocking gains compared to Ampere series cards. In GN Steve's video, they have shown that with overclock, RX 6800 XT reaches the top of the charts... Can't quite recall which chart was it. 

While overclocking is certainly limited, it is not as limited as their Nvidia counterparts. I will need to research more upon this... Do not take my statement on ""AMD is more fun to tinker around than Nvidia"" as a fact; in fact, take it with a grain of salt. From overclocking standpoint, I fear that unless either Nvidia and/or AMD wanted to release an ""unlimited"" VBIOS, we would have very little to show how much these cards can be overclocked. I am also looking forward for AIBs models, particularly in Sapphire (for a long time) and ASUS (recently... their ASUS TUF RTX 3080 series had left me on a positive note).

Additionally, AMD had drown out more performance per watt compared to the Nvidia cards... Again, this tells little and require several driver updates to truly see the card's potential. Last time, RX 5700 XT was not as close as RTX 2070 Super during the release; I suspect the same could occur in this RDNA2 release.

One final opinion that I have (and I'm quite sure very minor) is that... only AMD cards can run Hackintosh. Period. Dot. That is my only (stupid) reason that AMD ""wins"" over Nvidia.

In conclusion... selecting between AMD and Nvidia on their newest releases makes me think hard. I think that is a great sign that they are competing...",radeon_rx_6000
"Yeah you're probably right with that. But in the end does is really matter if you pull 30W more or not, or does it really matter if you have a few % less fps? I think we are at a point where amd and nvidia are real competitors for my use case (I don't stream/create content of any sort, and most likely won't use RTX). Still can't decide which card to get but nice to have more options",radeon_rx_6000
For the 3070 the amd card is about 80 bucks more. It has double the vram but that will only matter if the 8gb is maxed on the 3070. For 1440p I don't see a reason to spend more than the 3070.,radeon_rx_6000
Still sounds really niche,radeon_rx_6000
Scorptec,radeon_rx_6000
In achieving what?,radeon_rx_6000
"A quick glance I would guess something like ~17 games have DLSS 2, still a lot more than 2-5 though you're likely to only be interested in 2-5 tbh. But I'm guessing Cyberpunk will definitely be the DLSS seller for a LOT of people.",radeon_rx_6000
"This list includes DLSS 1.0, which is practically useless.

Doesn't include Anthem for some reason though. :)",radeon_rx_6000
"Sorry but if you play game and can't see effect if it is on then it is waste of resources.

Good example of that is Legion. In DF video consoles only use RT on things that matter like reflections in window, meanwhile on PC you get a lot more surfaces and effect is performance tanked to shit and end effect isn't that great. Half of DF video you need to seriously squint eyes to even seee anything.

Fact is that PBR rendering took away the most awesome thing about raytracing, proper material shading while screen space reflections are good enough to cover 90% of end effect.

Ray traced GI also isn't that better than other forms of GI and it murders framerate even more than them.

If PBR wouldn't exist then raytracing would be amazing jump.",radeon_rx_6000
"Because it's subjectively true. Out of the games you would want to play, 2-5 have working DLSS.",radeon_rx_6000
"Can't fault you there, to be honest I doubted AMD until now when the reviews are up.

That being said, AMD is in a good position with a plan in place for next generation already in motion. At the very least that's what Dr. Lisa says. NVidia are no slouches either, so the potential is there.",radeon_rx_6000
"I mean if they were saying that last gen, and here we are, I think it currently is glorious to have these 4-6 high end options between companies.",radeon_rx_6000
">Depending on the game, base resolution, and output resolution, *DLSS can add 1-4ms to render time per frame, which can be a lot.*

This is only true on Turing, on Ampere it is done in parallel, meaning the only thing it limits is the max framerate which would be the inverse of the DLSS frame time. So if it's 1ms, that means it would bottleneck you only after 1000 fps. Even at 3ms (which I've never seen it that high before), it's still 333 fps before it bottlenecks the rest of the gpu.

Also, not sure where you got the 4ms number from. A 2080 Ti (so not even Ampere with the upgraded tensor cores) was seeing around 1.5ms @ 4k.

The rest you said is very true.",radeon_rx_6000
"Besides the first two, that stuff is used by a small niche of gamers.

And as for DLSS, we'll have to see the AMD version coming in a few months before making a final judgement.

The ray tracing performance on the other hand is pretty bad and will stay that way I'm afraid.",radeon_rx_6000
But also being shown off by a bunch of other guys on the internet :*(,radeon_rx_6000
"Its still beats it, its cheaper and more efficient.  Nothing meh about it.",radeon_rx_6000
">i don't think the historical state of things matters to most customers 

It doesn't, but it not mattering to consumers doesn't create R&D or raw performance out of thin air.

It's not a reason to go with the AMD gpu, but it surely is a huge factor when considering if the card is ""meh"" or an actually interesting product than can offer some competitiveness for the market

>they should've been a bit more aggressive with pricing.

With both consoles launching with RDNA2 GPUs they probably don't have enough supply to sell the 6800XT for less than current MSRP",radeon_rx_6000
Not at 1440p seems about same for my use of wanting play high-ultra on 1440 144hz and still not sold on ray tracing.,radeon_rx_6000
"So it's either better than the 3080 at a lower price point or it's meh, got it

>horrid RT performance

Can't expect them to simply match Nvidias second RT gen out of nothing",radeon_rx_6000
"Or because once you factor in raytracing performance, DLSS, and other features that Nvidia has.

It falls behind quickly.",radeon_rx_6000
They also want it to happen at half the price,radeon_rx_6000
Haha jeez this comment chain 🤦‍♂️,radeon_rx_6000
[removed],radeon_rx_6000
"If you cared so much about video encoding, DLSS, RTX Voice and Cuda why would you wait for an AMD card? I cannot possibly wrap my head around this logic...",radeon_rx_6000
"Bad opengl support too, so bad performance with a lot of emulators as well, and some rendering things that use opengl in the viewport",radeon_rx_6000
I don't use any of that.... I game at 1440p and just want high fps on high-ultra,radeon_rx_6000
Are you kidding me ? No cuda ? WTF are you smoking ?,radeon_rx_6000
">can you please explain?

I'll try my best

>7nm vs 8nm?

These numbers refer to the process nodes and they used to be a metric to describe the transistors density (how many transistors they can fill in a said amount of space) of a chip. 

Nowadays, however, they aren't exactly precise or meaningful, as in Samsung 8nm (which is used by Nvidia) isn't directly comparable to TSMC 7nm (used by AMD). IIRC The TSMC node would be denser if you were to go by the numbers alone, but the Samsung node is actually denser

Also the differences between the two cards performance come mainly from the process nodes *and* the architecture of the GPUs, so you can't say one card has a manufacturing advantage based solely on the node density value",radeon_rx_6000
"""Pre-Orders"" for thousands of orders without even knowing when they would get their first shipment",radeon_rx_6000
"By the time the *next* generation of GPUs launch, we won't even have half of the benchsuite itles using RT unless reviewers actively cherry pick them.",radeon_rx_6000
"Going to 192 is probably a huge change in worst-case performance. In an ideal universe, all access would happen in cache with prefetchers keeping everything flowing.

In practice, weird accesses happen. When they do, everything stalls until the loading occurs. When this happens, that reduction in bandwidth suddenly matters a ton. 

The bulk of reads/writes happen within the frame buffer. You still need to get textures, models, etc too though. 128mb gives 1 4k, hdr frame (50-ish mb) with the rest left over for textures or 1 4k, non-hdr frame with 2x MSAA (about 100mb). I really wish some reviews looked into this (maybe Anandtech will).

With those same frame buffers and 96mb cache, there's suddenly less room to prefetch textures, so need for bandwidth goes up. At the same time, fewer CU also means demand goes down somewhat.

Now, if you're playing at 1440p instead, suddenly the non-framebuffer size at 96mb becomes similar to before. This means you'll have to choose larger resolution and lower textures or lower resolution and higher textures to reduce cache thrashing.

128mb/4k/256-bit, 96mb/1440p/192-bit, and 64mb/1080p/128-bit all just seem to go together well.

I seriously doubt that last one week be a dedicated GPU though. The die size seems too perfect for an APU. While AMD might not have known about Apple's M1, I doubt it given how it would affect their current agreements. Likewise they had to know about Intel's Xe.

30 CU is very doable as shown in their collaboration with Intel. The traditional issue has been the CPU and GPU fighting over the DDR4 bandwidth. Infinity Cache alleviates most of that pressure. If they wanted to get smart about it, they'd share the cache between CPU and GPU. Instead of smart access memory, they could literally transfer ownership of the cache instead which would immediately eliminate another huge portion of memory traffic (this would definitely need specific support though). I believe everything could fit inside 350mm^2 without any trouble meaning they could sell it pretty cheaply. I'd expect market value to be $350-400 and another couple hundred if they added dedicated hbm2.",radeon_rx_6000
"We will be comparing a different generation of cards by then!

Edit: I just thought about what my rig was like when CP2077 was announced. I can't even remember it.",radeon_rx_6000
"> ""Yes, I want that too"".

They did that back when Nvidia blew the ceiling off GPU pricing, so I wouldn't be surprised.",radeon_rx_6000
"That's odd and doesn't match with a handful of other breakdowns I've seen today, wonder why kitguru found differently.",radeon_rx_6000
"Minor point.  You seem to say ""similar"" when Nvidia loses, but ""better"" when Nvidia wins.",radeon_rx_6000
"I think this is highly reductive, and disregards things like memory bottlenecks and architectural differences at different resolutions. ""It's better at 4k, thus it's better"" is just not accurate in real world scenarios.",radeon_rx_6000
"This is wrong. The difference in performance drop from 1440p to 4K isn't (at least mostly) from CPU bottlenecks, but from different architectures optimized for different things. Nvidia has far more memory bandwidth, while AMD has infinite cache, for one. They're very different architectures.

You can see it's not about the CPU by comparing to eg. 1080p results, or you can see the 3090 scale fine compared to the 3080 in 1440p, but still it Ampere as a whole scales much better at 4K.",radeon_rx_6000
"Less expensive and same/more performance
And you say its not a better deal?
Cause of ray tracing... what?",radeon_rx_6000
Waiting until stuff is in stock is the only thing that makes sense. Buying a card you don't really want just because it is in stock today is crazy.,radeon_rx_6000
"> All in all, if you have a choice (which is the key right now with everything sold out) and you only play at 1440p and below, and you don't want to use raytracing at all, then the 6800XT makes sense. Outside of any of that, the 3080 is the better choice.

This is a very very specific use case though.",radeon_rx_6000
[deleted],radeon_rx_6000
"yeah but thats 1 ray and doesnt require anything special, its veen around forever.

when you scale up to hundreds of rays and beyond new hw and sw solutions are needed",radeon_rx_6000
"Well shit, aren't you a smart fella?",radeon_rx_6000
"Ask a fortune teller, don't ask here if you want the exact date.",radeon_rx_6000
"Everyone should care about RT it looks way too good when done properly. But the next gen consoles are on RDNA2 . They will use DXR not RTX. And they have more VRAM than current 3070/3080. If anything what comes thanks to the consoles is going to favor AMD's offerings.

If you care about NVIDIA ray tracing right now, though, you should get one of the 3000 series. In fact, if one or more of the RTX/DLSS 2.0 games is on the top of your list, you should absolutely get one of the 3000 gpus. 3070 or 3080 depending on budget is a no brainer. 

Otherwise 6800/6800xt seam to make a hell of a lot more sense to me when looking at the full picture. Especially for sub-optimal setups in 1080p/1440p and average memory. Which is the overwhelming majority of real world users.",radeon_rx_6000
">and given next-gen console's foray into RT, more & more games will come out utilizing it.

Looking at the performance, I wouldn't be too sure about that.",radeon_rx_6000
"I got mine from EVGA's notification system. Signed up early, on the 19th Sept or so before they announced the queue. $809.99 turned into $874.. lol. Shipping and taxes, though.",radeon_rx_6000
"> How much better does ray tracing make these titles look at high resolution, high refresh rates? 1%? 5%?

https://www.youtube.com/watch?v=eiQv32imK2g

I apparently have to post this every time I talk on /r/Hardware about GPUs... If that is a 1 to 5% difference than I don't know what else to say. But even in games like the new Spiderman or Watch Dogs Legion that use RT for reflections the difference is way bigger than 5%. 

Cyberpunk will have raytracing (reflection, shadows and GI) and a good junk of new console games as well, so if you want to have at least console equivalent visuals you really should care about raytracing performance as well. On top of that you could just as well ask if Medium vs Ultra presets are important. Raytracing is nothing else than another visual quality option. 

> As well, you still have to compare the experience of higher refresh rate vs. ray tracing.

Especially with DLSS you can have well above 60 FPS and raytracing, at least if you don't insist on having everything at max. But if you are talking about well above 120/144hz (or even at locked 120 to 144hz) you should rather be worried about available CPU power now that the consoles (who have been largely CPU bound last gen) have fairly decent midrange CPU's on board. There isn't much on the market that allows a 60 fps CPU bottlenecked PS5 / XsX game to run at locked 144hz at the moment, unless said games can scale at more than 8 threads.",radeon_rx_6000
"What's ""all of them""?",radeon_rx_6000
Computerbase shows the 3080 wining in 11 out of 14 games against the 6800XT and completely destroying it in raytracing performance. All w/o even using DLSS in supported games.,radeon_rx_6000
Nope,radeon_rx_6000
"Well, according to the reviews I watched, rasterized gaming was a toss-up, not a clear win for the 6800xt. That plus driver issues makes the 3080 pull far closer to even.

edit: plus, I never said the 3080 was the superior card. I said the 6800xt wasn't exactly good enough to be a go-to, although I'll readily admit it matches the 3080 in rasterized gaming performance.",radeon_rx_6000
"That's a disingenuous comparison and you know it. The two are directly comparable in everything except performance and one costs 300% of the other, instead of one costing 108% of the other but having a multitude of features to justify the price increase. If you're doing literally anything except rasterized gaming or you dislike driver issues, the 3080 eliminates some or all of the p2p difference.",radeon_rx_6000
"It marginally wins at rasterized performance/dollar, while losing very significantly in raytraced performance and lacking dlss (not to mention nvenc, cuda etc. that have value to some users). If we assume that real price gap is gonna be comparable to MSRPs gap,  I'd magine most people would rather choose 3080, but honestly neither is a bad choice. 

But anyway with AMD coming as an underdog, this isn't the victory they badly need. A lot of people will just default to nvidia hardware (and there's legit reasons like having gsync monitor too) unless AMD comes up with a definite victory. I already got mine, but I would've loved to see the market disruption of AMD getting ahead in high end.",radeon_rx_6000
VRAM?,radeon_rx_6000
The console thing is a red herring. PC gamers don’t just get rid of their pc for a console. If AMD does release 20% more performance from drivers then your argument would make more sense but I don’t see that happening. I’d imagine good drivers from th outset was a heavy consideration from AMD for big navi,radeon_rx_6000
"I couldn't find any info on that claim. Microsoft's blog entry only talks about GPU direct access to NVMe drive, there's no mention of resizable bar support. https://devblogs.microsoft.com/directx/directstorage-is-coming-to-pc/",radeon_rx_6000
[deleted],radeon_rx_6000
Nvidia's ray tracing solution is not proprietary.,radeon_rx_6000
Wrong. Spider-Man is a basically a PS4 game with make-up on. We should see some much better implementations in the coming years. The systems where just released.,radeon_rx_6000
"I mean I definitely could live without RT, DLSS, nvenc or (possibly) better drivers of Nvidia when you look at each as an individual matter, but they're definitely meaningful enough to add *some* value to almost all customers. Missing (or worse performance) in any of those isn't a dealbreaker for me, but still I'm happy to have them. I was honestly expecting these radeons to beat my RTX 3070 in rasterization performance/dollar by slightly higher margin than they did.

If you're buying a high end graphics card for gaming, you're gonna be playing graphically demanding games. *Many* of those games will support raytracing, DLSS or both in the next few years. If you look at this year's AAA titles, most already do.  And to counter the ""yeah but most games don't"" - you also don't need anywhere near this level of GPUs for most games anyway. Aside from AAA titles very few games are demanding.

Anyway it seems like people are arguing over relatively minor differences, if anything the performance/dollar of all new GPUs from both camps (excluding 3090) seems to be pretty much the same ballpark. Whatever card you end up choosing there shouldn't be much reason for regret.",radeon_rx_6000
If you want 1080p 144 then there is no reason the buy one of these gpus. Any 300$ previous gen gpu van do that.,radeon_rx_6000
I get the supply and demand ideology but would waiting 2 or 4 months even make a worthwhile difference in pricing?,radeon_rx_6000
"I’d be surprised if new partner 3070s ever sell below $400 while they’re current, especially as constrained on supply as they are.

Also, this isn’t really angled at you in particular, but I see the sentiment a lot so I figure I’ll say it: there is also an opportunity cost to *not* buying; time with a product when it’s near the top of the stack is valuable and for some people the benefit of having a card a few months earlier is worth the premium they pay in both dollars and bugginess. That calculus is different for everyone, but it’s something to consider.",radeon_rx_6000
"Don't know where you are getting 10 - 15 fps @ 4k from. The 6800xt is within a few % at 4k and seems to be the better card for 1440p 144hz based on the reviews I've seen. DLSS isn't free performance, there is a visual impact and added latency. AMD is apparently working on their dlss competitor as well. We are still a generation or two from ray tracing being feasible imo. Next year's cards will make current cards look silly in rt anyway. The 6800 xt looks to be a solid buy. The 3080 is worth the extra 50 if you game at 4k and want ray tracing; but really the performance hit is still too great.",radeon_rx_6000
Yup,radeon_rx_6000
What percentage of PC gamers regularly stream or share content then?,radeon_rx_6000
"We're not debating that, that's a known. What I'm saying is that AMD obviously doesn't feel the need to prioritise it as a feature.",radeon_rx_6000
I mean my work laptop can do that on a 3 year old midrange Intel CPU.,radeon_rx_6000
"LoL , do you even math?

3440 x 1440 = 4953600

3840x2160 = 8294400

4953600 /  8294400 = 59.7%

**I said 3440 has 59% pixels of 3840x2160 = and my calculations aboved showed that it's correct.**

8294400/4953600 = 1.67, which is what you are referring to but that wasn't what I was talking about.

**3440 has 59.7% of the pixels of 4k, and 4k has 67% more pixels than 3440 x 1440. They are both correct and valid statements.**

To make it simpler for you. A 1kg object is 50% the weight of a 2kg object, and the 2kg object is 100% heavier than the 1kg one. See? Both valid statements.

Please learn to fucking read.",radeon_rx_6000
"I will be keeping this card for the nest 5 years or so (I will most likely lower the resolution and graphics settings as the card gets older).

In 3440x1440, wouldn't the vram lack be exacerbated? It is around 30% more pixels, so something that would take 8 on 1440p, would it take 11.5 in UW 1440p? For me: graphical quality > more frames.

I'm not too bothered by the difference between something like 90 and 110fps, if the game looks good. Since I'm planning on getting a VA monitor.",radeon_rx_6000
"How does NVENC affect VR?  I've seen several people say this, I understand how it works better in recording / streaming, just not VR.",radeon_rx_6000
"That's situational though, a lot of people don't care enough about watch dogs legion (another forgettable ubisoft shooter in an ocean of forgettable ubisoft shooters) for it to factor in, and game streaming services are only viable for those with good and very stable internet connections.

For some it's great value. For others it's effectively nothing.",radeon_rx_6000
Every review I've seen has put AMD power usage significantly lower than Nvidia's regardless.,radeon_rx_6000
"Yes, eventually. But then you could make the argument AMD will release driver updates that improve performance. Or maybe the new upscaling tech. So something in the future is not quite as good as something you can use today",radeon_rx_6000
"Comes down to the person ultimately. I care a lot about texture quality! It's the most important visual setting for me.

 But I also suspect I'll be doing a 2 or 3 year upgrade cycle on my PC so according to my own estimate it would be moot for me.",radeon_rx_6000
"i can understand that argument but dont you think its a bit insane that nvidia has actually decreased vram capacity for this generations flagship card? from 11gb to 10gb? really? a step down? nvidia was extremely greedy with this generation, going to samsung instead of tsmc (worked out pretty fucking well, didnt it?), only using 10gigs instead of 16-20 with inefficient gddr6x memory. pascals 1080 ti had 11gb and that was 4,5 years ago. but im not really surprised, nvidia has always cheaped out on vram capacity.  its always been amd pushing it as far as one can remember",radeon_rx_6000
Turing was ridiculously overpriced and persented little price/performance gain over prior gen. Nvidia knew competition was coming and fixed that for Ampere. We're unlikely to see price/performance improvements like that with future generations because we usually aren't going to have it backloaded.,radeon_rx_6000
"And drivers. I went from a R7 270X to a GTX 1650 about a year ago due to necessity and availability. The radeon card was annoying to update its drivers. Nvidia just works. I love AMD -- and maybe I will end up with a 6800XT, but it really looks like a 3080 is better for my needs as a 4k/60 gamer.",radeon_rx_6000
"These are high end cards, though. The “average user” argument doesn’t hold a lot do water for $700 cards imo",radeon_rx_6000
">raytracing or DLSS

I bet if those two features are widely used by now AMD would probably have priced the 6800XT at $600.",radeon_rx_6000
"No doubt, but 2 big kickers:

1) Nvidia has pretty much always been better on the software side, let alone in AI/deep learning

2) Nvidia has dedicated hardware/tensor cores that can run in parallel to the cuda cores to do the upscaling with almost no penalty performance wise

Big Navi not having their solution hardware accelerated is going to infer a penalty when running this as it will have to use the CPU or GPU. 

So even though you'll get more frames with their solution with it on than with it off (which again, will likely look worse too), you won't probably get near as many frames as you would if it was done on dedicated hardware.",radeon_rx_6000
"Does anyone have a comparison with the SAM-stuff turned on yet? If  that gets it to AMD's claimed position (15% better performance 15% more expensive) while still having way better power consumption, then it's a better sell for new builds at least.

Though we also don't know if/when nvidia's implementation would be released.",radeon_rx_6000
"I’m going through benchmarks on mobile, what’s the power draw difference between them?",radeon_rx_6000
"Honestly between the 3080 and the 6800XT I can't see me being happy giving up the raytracing  performance for a 7% savings.

Neither the 3070 nor the 6800 are going to be good for 1440p or 4k raytracing at 60fps, so if 1080p raytracing is important then the 3070 is the right choice, but if you don't care about raytracing then it comes down to is 7% better rasterized performance worth paying 15% more to get the 6800. That's a personal decision IMO. Maybe the games you want to play will let you hit your monitor's refresh rate consistently, thus giving a much better experience than dipping just below it frequently.",radeon_rx_6000
"Depends.the 3070 is excellent value( if you get it at that price). And if you think you want more performance and are thinking to get the 6800 you might as well spend 70$ more for the xt. But then you also have to factor in AIB cards might be more expensive than the 650$.
So in the End it all comes to what you actually need. 


In my use case I will be waiting for a 3080ti to upgrade from a 1080ti as that card punches well above its weight.",radeon_rx_6000
"The MSI Ventus 3X was the ones with most stock from what I saw at least. I got 3 of those + a gigabyte one. Yes they drop around that timeframe, I'd double check on /r/bapcsalescanada to see if that has changed. Also if you have an iphone get the newegg app and set up apple pay with it, you can buy the card in a single click, so if you manage to find one in stock your basically guaranteed to get it.",radeon_rx_6000
">	To my knowledge AMD isn’t nearly as much a player in that space if at all.

That’s too generous for AMD. In AI and ML, the expertise and experience and not even close. Look at how CUDA is used. Look st Tensorflow support. In all the ML applications I have tried so far, I pretty much need a Nvidia card.",radeon_rx_6000
Bought a msi ventus X3 oc today for 829€. If you keep searching you'll find one. I had tabs open for all major retailers and refreshed periodically.,radeon_rx_6000
"They can't do that unfortunately, because that would cut down the bus width from 256 bit to 128 bit. They need 8 modules.",radeon_rx_6000
"True. My 2070 Super is undervolted as far as i can in my Ghost S1, but there's reason to believe the 6800 can be undervolted too, no?",radeon_rx_6000
"And when you read it, it sounds like that's a small niche group of people, but it is probably the largest group  of potential consumers.  

Personally I'm sitting on a 2060Super and don't need to hurry to update. But I'd also be one of the people needing to upgrade the PSU, so yeah whole heartly agree. 6800 is much more interesting to me right now. 

In the end I feel like the first who can actually provide supply faster, will pull ahead this time.",radeon_rx_6000
$600 and $700 is not a trivial price difference,radeon_rx_6000
"Why not, it adds 15/20%+ extra to the overall cost?",radeon_rx_6000
And there will be zero crossover /s,radeon_rx_6000
"You know exactly what will happen - benchmarks will just use ultra settings so 2-3 years down the line people will go ""wow look at this vram bottleneck at 4k on 3080"" in a couple of games and declare that it was trash even though you'll be able to drop textures one notch and get great perf that still looks almost identical.

Nobody cares that all of these cards will need to drop other types of settings within a few years to maintain 4k at good framerates, and already need to use reconstruction to get ready tracing on at that res with good framerates, but are psychologically unable to deal with the idea of maybe dropping vram related settings for some games at 4k",radeon_rx_6000
"Why would it do the opposite? The whole point of Direct Storage is swapping directly from the SSD. If anything, it would stress memory bandwidth. But PCIe and most importantly the SSDs are the issue here, not the VRAM.",radeon_rx_6000
"This is 3.5GB 970 all over again.
Future games that max out the VRAM on a 3080 won't run acceptably well on 6800XT either because the power isn't there.

I had the same kneejerk to the 3080 VRAM numbers, but realistically they put enough VRAM to cover what the card can realistically do.",radeon_rx_6000
"Okay, but in 1 year we will have the new Nvidia GPUs or a refresh.",radeon_rx_6000
" AMD, mainly. 

I assume there is some cooperation since MS and Sony want to use it.",radeon_rx_6000
">Is it AMD supersampling or microsoft? Because I have zero confidence that AMD will produce something matching DLSS.

The aim isn't to produce something that matches DLSS2.0 but rather something to replace it.

They stated on the HotHardware stream earlier today game developers requested something cross API, cross platform and cross vendor. Something that would work on both AMD and Nvidia cards as well as both consoles. So they're all working together (to be clear, Scott and Frank never said Nvidia was, but I highly, highly doubt they were left out of the conversation) to put something together alongside Sony and Microsoft.",radeon_rx_6000
"TPU has them matching to losing against a 2060 non-super, on a card in the 3080 price/performance tier. Digital Foundry’s recent video showed Xbox having texture quality problems when RT was enabled.

This isn’t just gen1 jitters, NVIDIA’s gen1 was substantially better than AMD’s. These are some difficult design compromises due to the combined texture/RT unit. NVIDIA’s stand-alone cores are simply performing much better.",radeon_rx_6000
"To be fair, 28.2 FPS on a 3080 is also nothing anyone could enjoy.",radeon_rx_6000
"we should stop talking about msrp pricing and start talking about actual pricing. right now in germany, if you want a 3080, you gotta be ready to pay 950€. if you want a 6800xt as of right now, you can get a reference one for 830€. thats already a 120€ price difference in itself. the 3080 has been out for 2 months and its still nowhere available. the 6800(xt) just launched. if they have more stock coming, prices go down further and the price gap becomes larger and larger. 

im not paying 950€ for a 3080, not after they told me its worth 700€. but im also not paying 830€ for the 6800xt. itll be a race for price and availibility for me and most people. given the rumors weve seen around ampere, we might just never get decent stock, samsungs 8nm just seems like a failed node. 

the 10gb of the 3080 are a huge turnoff for me personally as well",radeon_rx_6000
"$50 is pretty big on a budget build, instead of putting that money into a GPU you could spend it on a good medium range gaming mouse for example, in the case of my friend who wants to become a PC gamer for the very first time, that’s a critical part of his budget.",radeon_rx_6000
"If you're not gonna use ray tracing or dlss no point in spending the 50. Some people save up for the cheapest version of a card so 50 dollars is a lot. Drivers could be an issue but we'll see, and software is mostly subjective.",radeon_rx_6000
"Yes, in current games, but it will be beneficial in the future. A few years ago I would have said 11GB in the 2080Ti is pointless, and it largely was. But the 3080 has less VRAM than that.",radeon_rx_6000
true but it can be somewhat forgiven at launch. its hard to justify nvidia still having poor availability 2 months after launch.,radeon_rx_6000
"Respectfully:

Niche*",radeon_rx_6000
"Yeah sure the launch is impressive I am totally happy amd is competitive again but people are treating it like the processor side while the circumstances are different, also the amd cards still are kinda behind in productivity",radeon_rx_6000
"People keep talking about software, but I don't yet it. The hell do you want? Fancy upscaling that only works in a few games? Screw that, I bought a 4k monitor for native 4k. Ray tracing? The only game I own that even has that is Fallout 4. Frankly, I don't even care about ray tracing. It's still a new tech with limited games. 

I remember back in the day people called consoles crap due to their dedicated upscaling chips. I guess now that it's on PC y'all like it now or something. Idk. All I know is I first started using a 4k monitor with cross fire 7870s, then a fury X and now a 5700 XT. I can't see a 6800/XT being a downgrade.",radeon_rx_6000
"Amd seems to release new generations more often, so no not really. Amd released rdna2 after ampere, but I expect them to release rdna3 before nvidia's next gen.",radeon_rx_6000
"AMD hasn't released 6900xt yet, how can you say who's ahead?",radeon_rx_6000
"> Just like people who bought 2080 Ti before Ampere and DLSS 2.0, right?

Well at that point there were no equal perf rasterization competition from AMD either so its not really an apples to apples comparison is it?",radeon_rx_6000
"Right, because RTX is Nvidia",radeon_rx_6000
"Agree with all of that.  I think the total package as a whole is a tough sell - it's a little cheaper than the 3080, but also a little slower, and it's a lot slower in RT.  There's no DLSS support and no tensor cores that back up NVIDIA's DLSS performance.  But it is really really efficient, and it has 16GB.

Well, I guess people have a choice.  These are not two identical implementations of the same product from different brands, it depends on whether you care about raster (not that it's mindblowing value for the $$$ but that is the area the card doesn't fall down in), memory capacity, and efficiency, or on the nvidia side you have RT and hardware-accelerated DLSS.  I think perf/$ works out about the same either way ([edit:  yup basically identical perf/$ to 3080](https://www.techpowerup.com/review/amd-radeon-rx-6800-xt/37.html))

I'd really like to have seen a 3080 20GB or 3080 Ti 20GB, I think the memory capacity gives AMD a bit of an opening.",radeon_rx_6000
anandtech or techpowerup?,radeon_rx_6000
"I reckon the latter is more realistic, but I can see why GN does that then.",radeon_rx_6000
That was much more obvious and you can see it was wrong.,radeon_rx_6000
"Yeah, people are missing this. Ampere, from its doubled CUDA cores, scales like old GCN, like the 290x or fury x, where it ""gains"" relative performance as the framerate goes down and the window of time to schedule work across shaders increases. And what's great at driving the framerate down? Increasing the resolution.

AMD producing an arch that is unburdened by this effect is really impressive IMO, considering they were bound by it so severely before.",radeon_rx_6000
Ah ok 👍,radeon_rx_6000
Yes because it’s simply resizable BAR which is part of PCIe spec anyway,radeon_rx_6000
"AMD partner cards will be over MSRP too, and the ones closer to MSRP will be shitty models with bad coolers that you won’t want to own, just like NVIDIA.",radeon_rx_6000
"That makes no sense. There are plenty of reasons someone should go with a 6000 series card. Power efficiency, Freesync, More VRAM, Price, S.A.M, Supporting a company whose products you like. Not everyone wants or needs the same things in a card. Pretending this is some no brainier choice is ridiculous.",radeon_rx_6000
"I'd wager that even Feb may be too optimistic, unless a big influx of stock comes available, especially when you factor in how many folks signed up on the day they announced the queue system.",radeon_rx_6000
DLSS is why its not,radeon_rx_6000
Sam is gonna happening to every cpu and system because it's not a propietary,radeon_rx_6000
"My B450 board already has this option, I've had it turned on since 2019",radeon_rx_6000
[deleted],radeon_rx_6000
"RT accelerators are assigned from texture mapping units, except texture mapping is kinda important. You can just google amd rt and tmus.",radeon_rx_6000
Dedicated integer cores,radeon_rx_6000
"Did you even read the source you linked?

> Technically, you are right, though I'm ambivalent about this 
particular phrase. We have two different phrases:

> 1) ""Three times as large as N"" means ""3 * N.""

> 2) ""Three times larger than N"" means ""4 * N"" - but only if you stop  to think about it, as many people do not.


....

> The reason I'm ambivalent about case (2) is that I can't picture using ""times"" in this way in an incremental sense; I wouldn't say ""1/2 times greater."" It's just not a natural way to say what you're taking it to say, so I naturally tend to assume the speaker really meant to say ""3 times as large.""

Just assume 3 times as large/fast when someone says 3 times larger/faster.

No need to convolute things for the sake of convoluting things. lol.",radeon_rx_6000
And the 6800xt blows for creative work - some of it doesn't even work since AMD lacks tensor cores.  See LTT review.,radeon_rx_6000
Yeah dude. Steve from GN said he was doing 100 hour weeks. Hopefully things get back to normal after the 6900XT launch. Good for consumers but these reviewers must hate all these launches lining up at the same time.,radeon_rx_6000
Houses,radeon_rx_6000
"exercise equipment, bikes, golf clubs, guns and ammo, a lot of industries and hobbies are facing a supply shortage/overwhelming demand.",radeon_rx_6000
"in my case, its more that I'm fedup with reading that bullshit on paper launch in the comments. Its not a paper launch(for both nvidia and amd), just impossible to predict demand, get over it. Not like you're gonna die because you need to wait a few months...",radeon_rx_6000
"Why do you assume you're dealing with Nvidia fans and AMD fans, rather than people who understand economics and people who don't?",radeon_rx_6000
The difference is one company has been an undisputed market leader for close to a decade if not longer while the other one was close to bankruptcy until a few years ago.,radeon_rx_6000
"I had that thought as well but regardless NVIDIA kicks the shit out of them on RT this generation

Just because RDNA2 powered consoles are capable of ray tracing doesn't mean RDNA2 is going to be the best RT option for PC users 

not hating on AMD though, I'm still very impressed they caught up on rasterized performance and still think these cards are a big win for gamers",radeon_rx_6000
"i mean its a super cool feature to have, and with DLSS its actually effective. Control is a blast to play through.

That said otherwise I agree, if you dont care about RT at all, go AMD. 

I got a 3090 solely for NVENC and Ray Tracing though",radeon_rx_6000
"Nvidia started pushing it first, but MS and Sony also joined in promoting it for their new consoles.",radeon_rx_6000
I think he means the fact that people are discussing dlss resolution like they're real res. The 6800xt does perform close to or above 2080ti in RT without using DLSS and yet the guy above is buying reviewers who mixed DLSS with native res.,radeon_rx_6000
"My personal opinion is that, given what we have seen so far, there is a very high chance Nvidia's implementation of ray tracing is going to perform better in future titles too. That said, I cannot deny that almost all current titles with ray tracing have been developed with Nvidia's implementation in mind. In that sense, and given Dirt 5's unusual results, I'm willing to give AMD the benefit of the doubt. This, of course, matters very little to anyone making a purchase decision today. Today, the 3080 is the best option if you want any of these features, no question.

I realize that the wording of what I said in point 1 came off as denying the advantages of the 3080. This was not my intention. I was trying to establish the fact that there is at least evidence of games performing better using AMD's implementation. I should have said ""there is not enough evidence today to suggest the differences in ray tracing performance are going to continue to be this big as games are optimized for AMD's implementation, though that certainly is the case today, and we only have one valid counterexample"".",radeon_rx_6000
"5700xt matches 2070 ""amd has no hope they have a node advantage, when nvidia goes to a smaller node blah blah""

nvidia present a new arch, on a smaller node (and they went 8 because of their own choice) , amd releases still on 7 and matches their top gpu ""amd has no hope they have a node advantage once nvidia shrinks blah blah""",radeon_rx_6000
"Read the context. They're not discussing which card is the better buy. They are discussing whether AMD will keep improving and eventually overtake Nvidia, like they did against Intel. Personally I don't expect Nvidia to be outdone the way Intel has been. I expect AMD be on top after they release rdna3, and for Nvidia to put a lot into the 4000-series now that they have a real competitor.",radeon_rx_6000
"> AMD also has tremendously more efficient cards than Nvidia this generation

According to Computerbase's efficiency charts, they are within a few % of each other. That's significantly less than the node advantage.

It's certainly not ""tremendously more efficient"" (it is, however, tremendously more efficient than any previous AMD GPU).",radeon_rx_6000
"Igor's review has them in lockstep, 6800, 3070, 6800XT, 3080 in order of decreasing efficiency. I wouldn't say it's tremendous.",radeon_rx_6000
Because of the node advantage...,radeon_rx_6000
"> lol you not seen some of the partner cards power draw then.

I was comparing reference to reference, average gaming (not peak).

* Techpowerup has the EVGA FTW3 3080 at 317W in gaming loads
* They have the Asus TUF OC at 305W

The Asus ROG STRIX, not tested by them, has similar stock power limits a the FTW3 model.

No 3080 draws 500W in gaming loads in their out of the box configuration. I'll give you the benefit of the doubt that you were looking at measurements taken at the wall (total system draw + PSU inefficiency).",radeon_rx_6000
"I was impressed by the 5600XT. Driver issues aside, I found it a better alternative to the 2060. Problem was that by the time AMD released it, I had owned my 2060 for nearly a year. 

AMD being competitive with Nvidia in performance within the same launch window is the key difference this generation.",radeon_rx_6000
"> I envy your self control.

It's been a multi-year project. The only games I've ""bought"" were things the family would enjoy (Fall Guys), or trades for GPU pack-ins using Reddit (IE, get a game I don't like, trade for one I want).

My wife ruins it periodically though. She occasionally wants to re-up our World of Warcraft subscription. That lapsed today so I'm going to try to wrap up GTA V before Monday.",radeon_rx_6000
My reasons for buying a GPU are my own. I enjoying having control over my gaming experience.,radeon_rx_6000
Minecraft RTX is DirectX12 Ultimate. There is just a lot of ray tracing going on.,radeon_rx_6000
Godfall won't load on Nvidia hardware? What?,radeon_rx_6000
In your case that's an easy choice imo. A card you can get is better than one you can't.,radeon_rx_6000
"Yes, but unfortunately that's a gamble people would have to take if they're interested in buying right now. 

I have the luxury of being able to wait to see how things shake out, but some people aren't as patient.",radeon_rx_6000
this too,radeon_rx_6000
Bro lol.,radeon_rx_6000
"https://www.overclock3d.net/news/software/amd_has_an_answer_to_dlss_directml_super_resolution/1

https://www.dsogaming.com/news/amd-will-share-more-info-on-radeon-rx-6000-gpus-ray-tracing-and-ai-supersampling-tech/

No direct confirmation from AMD. (yet)",radeon_rx_6000
"> Do we know what the actual overhead of DLSS is (running on Tensor)?

That's an interesting question and I don't think any third party looked into it in too much detail so far. What we have are NV's claims, and what we can infer indirectly. We know how many pixels each DLSS 2.0 mode actually renders, and we know the performance of that vs. the same pixel count + DLSS. And if you look at that data it seems more expensive than you might assume (and I also initially assumed FWIW).

A second more recent data point is with Ampere's capability for asynchronous DLSS. Third parties *did* measure the impact of that in Wolfenstein, and [it looks quite significant](https://www.techspot.com/article/2109-nvidia-rtx-3080-ray-tracing-dlss/). Since that ""just"" reduces the overhead of performing the DLSS inference (and likely not to 0), this is another indication that the overhead is quite significant.",radeon_rx_6000
"What I'm saying is the open source version that becomes industry standard should focus on raster pipelines since it is the common denominator and isn't tied to and current or future gpu make

I honestly think dlss and rtx is just nvidia trying to move goal posts away from raw raster and over to areas where they know and has to play catch up and they can pad the field by buying out the souls of any experts in advance",radeon_rx_6000
"The other games AMD falls behind drastically in. If we throw out the heavily favored games, like Dirt 5 and Control, AMD loses across the board still.",radeon_rx_6000
"Imho they know exactly what the 6000 series weaknesses are, and chose to review around them. The RT game selection is a joke.

If you contrast that with a nearly 2 year rhetoric on how bad Turing is at RT, and how dlss isn't worth including in benchmarks, its strikingly off kilter. 

It's not that the numbers are bad, it's that it seems like particular thought was given on how to get numbers that make 6000 series seem much closer to Ampere than more in depth methodologies would reveal.",radeon_rx_6000
">I don't understand this. What is so much time to prepare?

They had from the launch of the 2000 series, to now to prepare a semi-competent answer. Their answer is just not that. Their ray tracing accelerators are piss poor and barely better than the 2000 series, if that.

>You do understand this hardware is typically developed in 4-5 year cycles in the background? AMD 4-5 years ago was financially in a terrible situation treading water.

AMD is not some indie company. They were not in such dire straights 4-5 years ago that they had absolutely 0 money. Further, it was their own damn fault.

I don't care that AMD flubbed it up 10 years ago, and couldn't get their shit together for 5.",radeon_rx_6000
"I am sorry, did $700 card that still wont get you nowhere near 144hz suddenly playable?

And we are not even at 4k60?

Are you like not aware how long trends actually take? Steam stats and such?",radeon_rx_6000
"The only reason Nvidia priced the 3080 the way they did is because of the 6800XT. You'd be saying the same thing if the 6800XT was a turd, except you'd be paying $1000.",radeon_rx_6000
"Perhaps you have misheard them? It is easy to accuse someone of being biased if the results do not align with your previous beliefs.

But then again, their game selection may favor the games optimized for AMD. Remember that back then that they had been roasting AMD graphics card for subpar performance to their Nvidia alternatives (Vega and VII)?",radeon_rx_6000
"They said Ampere is not a gaming-focused architecture, which can be argued. That's why you see RTX 3000 cards doing (comparatively) better at 4K than 1080p and 1440p. They can leverage their large amount of cores better at higher resolutions, otherwise parts of the GPU are idling. This resembles somewhat AMD's problems with the Vega architecture. Nobody is saying Ampere is bad at gaming.",radeon_rx_6000
"Well to be fair, Cyberpunk isn't out yet and we have no ideas how it actually performs.",radeon_rx_6000
Very few people are willing to have their FPS halved just for some -presently- very minor graphical improvements. Improvements who for the most part could be baked in rasterization,radeon_rx_6000
"Mostly story based?
Dude did u see gameplay of the action?
People always want high fps",radeon_rx_6000
"Ehmm the customers dont decide what the console will target the companies do

Most people dont know what fps is and its more difficult to market than resolution and raytracing",radeon_rx_6000
"I think if we checked a list of the most popular game titles by hours played right now, RT titles that really draw you in for long periods, there isn't a big list. 


Most people end up playing old games. All my favorites games are still DX11 and a few Dx12. The new games are nice but not what my system typically runs, and honestly, I think most people are like that.

Will RT matter in another generation? Sure, when it's standard. But until it's standard, it doesn't deserve to be treated as such in it's reviews. 

I think there is way too much emphasis put on it imo.",radeon_rx_6000
You're still better off with a 4K monitor if the game has variable rendering resolution,radeon_rx_6000
Good for them then) I don't texture mod Skyrim.,radeon_rx_6000
"That’s definitely a concern. If you need a 100+ dollar power supply to supplement one card over the other, you may need to do some reconsideration.",radeon_rx_6000
Because in the past AMD cards were hot and loud on top of taking more juice. Just look at the Fury X vs the 980 TI.,radeon_rx_6000
"Yeah, seeing how the narrative has turned around since Pascal days just shows the extreme bias people have",radeon_rx_6000
"I don't know a single gamer that leaves their PC on over night.

Anecdotes are fun.",radeon_rx_6000
I shut mine down every night :/ guess im not a GaMeR,radeon_rx_6000
"But most people at least put it to sleep when they're not using it, which uses almost no power",radeon_rx_6000
"In reviews that actually measured the card power consumption. GN, HWUB, etc.",radeon_rx_6000
"I would not call machine learning a productivity. It is own category.

ROCm is a thing tho, and works on RDNA2 apparently already. It also allows quick porting from CUDA.

A lot of productivity software doesn't use CUDA either.",radeon_rx_6000
"I guess that's how you're supposed to say it.

I don't really understand the productivity side of the benchmark aside from some software just suffer tremendously with AMD GPU (as you said, it just doesn't work).

Thanks for the correction.",radeon_rx_6000
Without the same driver support. So it is not a Titan replacement.,radeon_rx_6000
"Now, this feels insane to me, but I might actually stretch out of my AMD fandom and wallet sanity to a 3080ti/3090 because it looks like Big Navi is going to scale worse above 4k, and I run 3x4k screens, so that actually really matters.

I've been looking for 8k benches as a niche thing some reviewer may have done but no luck. Have to wait until 6900XT comes out, anway.",radeon_rx_6000
"In USA or perhaps EU, I believe that is largely true. 

I live in the SEA region; only time will tell whether the cards would be priced as is without price hike in some regions. There are some instances that either Nvidia or AMD cards are more expensive. 

I will need to watch reviews on how much does VRAM matters. Radeon VII had gigantic amount of VRAM and they are highly sought after for productivity-related tasks (CMIIW). While the gaming performance for RX 6800 XT leaves a lot to be desired in comparison to RTX 3070/3080, I would like to see its performance outside of gaming scenario... Just for the sake of getting more data. 

I have a 1080p monitor... I think it makes a very little sense for me to upgrade for now. Thanks for pointing that out. I need to do more research before pulling any trigger and to point out whether AMD had worse or better value; at the present moment, I am leaning on AMD had a little bit worse value, especially considering their lackluster RT performance.",radeon_rx_6000
"It depends on how often you upgrade your GPU I guess. Historically, cards with lower VRAM than their counterparts ‘aged’ worse. This is no guarantee of the future of course, but the extra VRAM really allows the 6800 to excel at rasterisation.

My main concern would be whether AMD can avoid their bad reputation with poor driver support...",radeon_rx_6000
Not everyone runs barebones winblows pcs,radeon_rx_6000
Watch the video,radeon_rx_6000
"That's funny, I have played legion with RTX and it looks awesome.

You can't explain away the deficiencies of the AMD offering.",radeon_rx_6000
"I haven't played Legion. But I have played Control, and the RT implementation in that has a very significant impact on overall visual quality.",radeon_rx_6000
Have you played Control or Metro with RT maxed? It’s a night and day difference.,radeon_rx_6000
"Why do AMD fans constantly have to make shit up? There are over 2 dozen games that support DLSS 2.0 and there will be dozens more added in 2021, with nearly every big title supporting it. Pretending “DLSS doesn’t matter” is just sticking your head in the sand, exactly like AMD fans pretended RT was a gimmick until AMD announced it would support it with Big Navi.",radeon_rx_6000
"I misjudged a review. Here's a 2080TI, which I confirmed by comparing performance from another one of OC3D's articles.

[https://www.overclock3d.net/reviews/software/dlss\_2\_0\_with\_control\_-\_nvidia\_s\_new\_killer\_feature/4](https://www.overclock3d.net/reviews/software/dlss_2_0_with_control_-_nvidia_s_new_killer_feature/4)

[https://www.overclock3d.net/reviews/software/control\_pc\_performance\_review\_optimisation\_guide/5](https://www.overclock3d.net/reviews/software/control_pc_performance_review_optimisation_guide/5)

1440p to DLSS 1440p (4k) went from (84.3FPS) 11.86ms to (66.9FPS) 14.95ms for RT off, a little over 3ms. But it looks like RT and DLSS could already run concurrently on Turing. There's not much difference when RT is on.",radeon_rx_6000
"RTX Voice, now called RTX Broadcasting is like gold during covid times.

forgot to add: RTX Reflex",radeon_rx_6000
And saying she will come right over to screw my brains out and then ghosting(no stock) me for months,radeon_rx_6000
Hahaha nice one!,radeon_rx_6000
">	still not sold on ray tracing.

I don’t know how anyone can still be on the fence seeing almost every next gen console game launch with it out of the box.",radeon_rx_6000
"I mean yes, you have to price lower and be better if you’re missing features like good RT perf, DLSS, NVEnc, etc. it doesn’t matter that it’s first gen RT, if anything that should be reflected in the pricing more.",radeon_rx_6000
"Well yes, even Nvidia first attempt at DLSS and raytracing is also isn't that good with Turing. So I already expected AMD's implementation at first attempts will be mediocre at best.

Anyone expecting more is probably have a disconnect with reality.",radeon_rx_6000
I think most people are just happy to see Intel get a swift kick in the ass since they've been relatively stagnant for a few years now.,radeon_rx_6000
"I care about performance and features for price. For the right price or raster performance, it might be OK to give up on some extra features, but this ain't it. Big dud.",radeon_rx_6000
And what about ray tracing? I’ll remind you several exclusives on PS5 have it already at launch so it’s going to be widespread this generation.,radeon_rx_6000
"This interests me, and I know that previous AMD GPUs were pretty bad at openGL, but has this been actually benchmarked on the 6000 series or is it just an assumption based on last gen cards?",radeon_rx_6000
There is a lot of software that is Cuda optimized. This is absolutely a factor.,radeon_rx_6000
!remindme 22 days,radeon_rx_6000
"Computerbase also has the 3080 ahead in 1440p rasterization:  
https://www.computerbase.de/2020-11/amd-radeon-rx-6800-xt-test/3/#abschnitt_benchmarks_in_2560__1440_sowie_1920__1080

So does PCGH:  
https://www.pcgameshardware.de/Radeon-RX-6800-XT-Grafikkarte-276951/Tests/Benchmark-Release-Review-vs-RTX-3080-1361423/3/",radeon_rx_6000
"Because the margin is very small to declare which card is a clear winner when Nvidia ""loses"" and the margin is fucking huge when Nvidia ""wins"".

I am not sure everyone is looking at the same reviews. AMD loses out to Nvidia in Raytracing so badly it is not even a competition. Even if I don't want 4k, RTX 3070 still objectively smashes the competition in performace per dollar.",radeon_rx_6000
"I just stated that I would like to have a red team computer. I just stated that NVIDIA provided terrible VRAM

I say similar when results are in the 5% margin but better when it's way over 10%.

But nice as hominem you got going on there. The fact that's your only argument kinda shows the lack of there of.

In fact if you went through my profile the last post would show you me criticising NVIDIA.",radeon_rx_6000
That has been my experience in 20 years of PC gaming. What architectural differences would that highly affect performance at lower resolutions but wouldn't affect future games that are more demanding for GPU power anyway? The days of being fillrate limited are long behind us. The only thing that comes to mind is the lower VRAM bandwidth of Big Navi compared to Ampere but GPU's have been largely memory bandwidth bound for years now and I fully expect newer titles (and especially with raytracing enabled to get at least console visuals) to become more bandwidth hungry.,radeon_rx_6000
I still stick to my original opinion. Benchmarking GPUs at higher resolutions has always in the last 20 years shown which GPU delivers the best performance at normal resolution in future more demanding titles.,radeon_rx_6000
Yeah that's basically what I'm getting at.,radeon_rx_6000
"You're only partly right, and I had to [look it up](https://www.hardwaretimes.com/the-unreal-5-demo-on-the-ps5-used-software-ray-tracing-similar-to-reshades-ray-tracing-shader-ray-traced-gi/) again.

>Lumen uses ray tracing to solve indirect lighting, but not triangle ray tracing. Lumen traces rays  against a scene representation consisting of signed distance fields,  voxels, and height fields. As a result, it requires no special ray  tracing hardware.Lumen uses a combination of different techniques  to efficiently trace rays. Screen-space traces handle tiny details,  mesh signed distance field traces handle medium-scale light transfer and  voxel traces handle large scale light transfer.  
>  
>The important revelation here is that while Lumen GI does use ray-tracing for indirect lighting, it’s not the same as NVIDIA’s RTX implementation which relies on ray-triangle intersection and BVH. It leverages three different methods to trace the rays and obtain the required information.  
>  
>While in NVIDIA’s implementation, the objects in a scene undergo  conversion into BVH objects using the RTCores, in Unreal’s Lumen GI, the  scene is represented using voxels, SDFs (signed distance fields) and  height fields. These aren’t as intensive as BVH and therefore don’t  require hardware support.  
>  
>Overall, it seems like **the Unreal 5 powered Lumen GI that ran on the PS5  uses a very complex lighting method which can’t be completely  classified as ray-tracing** (debatable). Still, the results are impressive  and that’s what matters.

So not really the same ray tracing Nvidia and AMD are talking about when they talk about their RT cores (or whatever they're called) capabilities. All done in software. Likely not good for detailed reflections, but in how many games do you see or need those?",radeon_rx_6000
Its not.,radeon_rx_6000
I get that. My point was ray tracing is more than prettier reflections.,radeon_rx_6000
Good points.,radeon_rx_6000
"Your console argument pretty much works against you since they'll both use RDNA2-like experiences. So if the games will look good ray traced on consoles at reasonable frame rates, they will on RDNA2 cards with drivers in a few months.

If they don't look good on consoles, well, then... raytracing will be an even smaller niche for a few years.

Regarding the metro video you've sent, this is a bigger difference than I expected, but I don't see these scenes as dramatically better than the rest. The lighting looks different, yes, but not necessarily better. The textures are hit-and-miss. Some of the outdoor scenes are unequivocally better.

And if this is the best looking game with ""full feature ray tracing"" And I'm limited to average ~70 FPS @ 1440p, that's kinda bunk--I'd much rather have 100 Hz 3440x1440 and wait another 3 years for ray tracing to mature.",radeon_rx_6000
HWU's review pretty consistently show's the 6800XT outperforming the 3080 FE at 1440P.,radeon_rx_6000
"I saw 5+% averaged across games in favor of the 6800xt pretty consistently but I personally don't care in the case of such small margins. I was merely calling out you seemingly pretending price and perf aren't important and over-emphasizing some nebulous ""everything else""",radeon_rx_6000
"Ray tracing on pc is a meme tho

By the time it actually becomes worthwhile we'll be a few generations ahead",radeon_rx_6000
"And what are you going to do with that unused VRAM over a 3080, if it doesn't show up in any benchmarks? It's just there being unused",radeon_rx_6000
"I'm just going by what happened with the 5700xt...

And I wouldn't lump in all ""pc gamers "" together as some magical beings 

Me and my friends are pc gamers and we're getting consoles cause it's just a better bang for buck / experience and exclusives for the foreseeable future 

Again.. why pay $1000 for one part when you can pay $500 for a whole system",radeon_rx_6000
"I can swear I read something about  CPU getting full access to GPU memory, but alas I can't find it. Mostly what I can find is IO optimization and allowing streaming of compressed assets to GPU and letting GPU uncompressed them. Maybe I'm confusing it with something.",radeon_rx_6000
Think you missed the point,radeon_rx_6000
It is. It's a block box and you have to pay to use it. Whereas the dxr dx22 raytracing is not.,radeon_rx_6000
If you're the type of player who likes to play single player story driven AAA titles(like me) I would very much expect RT and DLSS to be relevant a lot in the years to come. It already kinda is.,radeon_rx_6000
"I would agree completely, and yet I see idiots all over buying 3080's for 1080p gaming. It's insane.",radeon_rx_6000
This is reddit land of the broke teenager / college student. they don't understand the concept of people spending more money to have nicer things earlier because they built their computers on a $700 budget.,radeon_rx_6000
"Yes their is a 15-10 fps difference in 4k titles mutliple titles. This is where i'm getting it from. [https://www.youtube.com/watch?v=jLVGL7aAYgY](https://www.youtube.com/watch?v=jLVGL7aAYgY) tomb raider stock vs stock thats how big the difference is @ 4k. Also as someone who routinely switches between dlss and not their is no visual impact that i can discern. When you're playing @4k with rtx on you need dlss its a big difference then even having the performance to go @ 60 fps vs not having any at all.   


Granted yeah you can say the same about amd specific titles. But even then i'd rather have the  nvidia card based on just driver updates in general. It's not a massive win and it does perform worse at 1080p. But i still say it's the better value.",radeon_rx_6000
It's super common for friends to share clips with each other privately in discord. Almost all the clips you see on subreddits for any game are shadowplay. This feature is a deal breaker for a lot of people.,radeon_rx_6000
A $120 chromebook does it just fine.,radeon_rx_6000
"No, I read just fine, it's your math that's a failure. It's not half way if you actually use your brain. I know it's hard but I know you can do it.

You also said:

>Also roughly 50% more pixels than 1440p.

Which is false. Again, you should ""learn to fucking read"" your own post. Seems like you really lack reading comprehension.",radeon_rx_6000
"I would say nvidia long term since the card has tensor cores which will greatly help as things like DLSS mature even more.  The AMD cards just don't have anything there.  I can see the ram concerns, but the big eaters of ram are textures which lowering graphics greatly reduces usage.

Save some extra money and go 3080 :)",radeon_rx_6000
I believe one of Oculus' wireless HMDs utilizes video stream encoding and NVEnc over AMD's encoder makes a huge difference,radeon_rx_6000
"6800XT pulls 8% less power according to computerbase. Significant? Maybe, though it's also 6% slower. Certainly not what you'd call a generational leap in efficiency at any rate. But TPU has it drawing 31% less which is a big outlier. And other outlets like GN, HUB etc line up with CB, not TPU.",radeon_rx_6000
"Not really, since this is just implementing something that already exists whereas you're talking about an unknown quality improvement or a new method of doing upscaling without tensor cores.",radeon_rx_6000
I’d counter - again - that competition is going to raise the bar more quickly than it did during AMD’s recent period of irrelevance,radeon_rx_6000
">Does anyone have a comparison with the SAM-stuff turned on yet? If that gets it to AMD's claimed position (15% better performance 15% more expensive)

Well considering the performance gains were all over the place and it's highly dependent on the game, there's pretty much no way you'll see an average gain of 15% across most games. I'm guessing 5-10% (from what we saw) the majority of the time.",radeon_rx_6000
"Looking at GN, SAM may get you a bit more performance but AMD has yet to white/black list games that benefit from it or not.",radeon_rx_6000
"Average draw in gaming is significantly lower. 3070 draws 33% more. Peak gaming is closer, but 3070 still draws ~13% more",radeon_rx_6000
"TPU has RDNA2 wildly ahead on power (164/210W for 6800/XT), but it's not borne out in other reviews so I suspect /u/wizzardTPU has an error somewhere. Others are reporting much closer to the stated TBP, so efficiency about on par with nvidia.",radeon_rx_6000
220w 3070 250w 6800,radeon_rx_6000
"I care more about the DLSS than the RTX tbh

I'm fine with spending 700 on the 3080 but just like every other card the price is waaay higher over here that MSRP",radeon_rx_6000
"My only gripe with the 3070 is the vram. My end goal is 1440p gaming and the 8gb vram is already maxing out in Watch Dogs Legion, at 1440p not even 4k",radeon_rx_6000
Thanks for the info!,radeon_rx_6000
"Makes sense, thanks for the explanation.",radeon_rx_6000
Well of course; I was just alluding to /u/avboden's comment suggesting that people ignore power consumption and go for Nvidia.,radeon_rx_6000
Where did you read that?,radeon_rx_6000
">Nobody cares that all of these cards will need to drop other types of settings within a few years

In my experience, the crowd spending $700+ on GPU's dont tend to be very happy with having to make regular compromises with settings.",radeon_rx_6000
"This is the point. The last 99th percentile of graphical fidelity requires 2gb of VRAM. Turn down the cloud settings in RDR2 and it drops 1 whole gb of VRAM. 

So yes, 16gb > 10gb VRAM but it's not a big deal for the next couple years",radeon_rx_6000
Let's say a game comes out this year targeting consoles. There's no direct storage right now on PC. What can the PC port do? Just load it all into VRAM,radeon_rx_6000
">This is 3.5GB 970 all over again. Future games that max out the VRAM on a 3080 won't run acceptably well on 6800XT either because the power isn't there.

So the 970 analogy is quite apt.  Because there were actually plenty of games that 970 owners(like myself...) had to compromise on with settings thanks to the lower VRAM, that would otherwise run ok.  Granted, people overplayed the difference between having 3.5 and 4GB, but still, it's not like we've only just recently started getting games that can demand more than 4GB of VRAM.  

10GB will be enough for a little while, but I really think people are underestimating the rise in demands we're gonna get once proper next-gen titles start coming around.",radeon_rx_6000
"wait the 970 was hampered by that down the line. if you are only keeping your GPU for a year or two then sure, but if your someone who wants to keep GPU for a long time the VRAM will matter.",radeon_rx_6000
"Yes, and there was a lawsuit about it and Nvidia lost.",radeon_rx_6000
"I'm talking about the 3080... and people who bought it or considered, for 4K.",radeon_rx_6000
"I imagine it's the other way around.  There's no reason that AMD need to come up with their own AI supersampling model themselves.  Especially when we know Microsoft are working on it.  And I'd bet good money Sony will use their own solutions as well.  

AMD simply provides the hardware.",radeon_rx_6000
"That's without DLSS and unlike Radeons, 3080 has DLSS. They just disabled it for this test.",radeon_rx_6000
Who is using a 3080 or 6800XT in a budget build though? $50 is comparatively less significant in the price range of builds that would use these 2 cards.,radeon_rx_6000
"You realize that with nre consoles having RT most games will have it in the near future? And dlss is getting slowly more popular in games? Also since you want 4K the ,3080 does outperform the amd cards?",radeon_rx_6000
"Ah, fair enough. I'd been out of pc parts for a decade and just got back into it with the recent Nvidia gen, so I'll admit I was just working on a guess there.",radeon_rx_6000
"No, I mean same first gen product.",radeon_rx_6000
Another victim claimed by Papa Huang's marketing team.,radeon_rx_6000
"If you factor in DLSS and compatible games, perf/$ goes up quite a bit..",radeon_rx_6000
"Absolutely, the majority of buyers will never touch Afterburner, Wattman or software like that.

People who watch GN are probably more likely to tune their cards or care about this stuff, still in the minority I think.",radeon_rx_6000
"It definitely does not ""scale like old GCN"". GCN hit a massive wall scaling up and from my understand it had to do a lot with how the memory was set up, the max amount of shaders per shader engine, and the inherent problem that there were fundamental latency issues after a certain point. The biggest thing with Ampere is it doubled it's FP32 output (since Turing added a simultaneous Int32 pipeline, Nvidia just made that extra pipeline be switchable to FP32 as well) which obviously is going to help a lot as resolution goes up since you do significantly more FP32 with more pixels.

Saying it scales like old GCN is super disingenuous and shows you really don't understand how either arch works.",radeon_rx_6000
"A 3080 FE's cooler is perfectly adequate, I'm sure 6800XT's will be as well. At least for Nvidia, those higher priced AIB SKUs seems to be particularly bad value this generation because Ampere does not overclock well.

For instance, a 3080 FTW3 at $810 is only at most 5% faster than an FE. That's 5% performance for a 16% price premium. Sure, with an FTW3 you also pay for binning, but since there isn't much overclocking potential to begin with, you won't get your money's worth.",radeon_rx_6000
"I was semi-mistaken. I had been basing that estimate on [the EVGA tracking spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vRintBllSIYz0TBOTLSbxYW8trXO_V3GUsgOdBEksclTQyBMnXGbrW1j1z_44waeTjLa3t0KEHSjknP/pubhtml#), which shows the 3080 FTW Ultra as working through around late February...but that's only one of the cards. Other cards are projected into April or even June.",radeon_rx_6000
"I'm sure it will, especially since the underlying tech has been part of the PCIe spec for a while now.

But Nvidia likely won't call it SAM. Kinda like how motherboard manufacturers call BIOS flashing through USB different names.",radeon_rx_6000
"Oh, so you meant that AMD didn't design RT hardware dedicated to RT, got it now, thanks!",radeon_rx_6000
Thanks!,radeon_rx_6000
People need jobs the buy houses.,radeon_rx_6000
"Moved house earlier this autumn, lucky me)",radeon_rx_6000
"I agree that neither of these launches are paper launches by the classic definition. And I think Patience is the most important thing to have with this whole pandemic and quarantine going on.

But I'm annoyed by a lot of double standards and hypocritical discussions surrounding AMD and Nvidia. And I don't like seeing the narrative change suddenly just because a different company is the one in the spotlight. 

If Nvidia has stock issues, then AMD has stock issues. If this is a situation that AMD can't be expected to predict demand, then the same goes for Nvidia.",radeon_rx_6000
Occams Razor,radeon_rx_6000
Please go on and explain why that's relevant,radeon_rx_6000
"And guess what ? Raytracing is not going to be major this generation. Some shadows here, some Dynamic GI (maybe as it is very heavy) here or reflections here.

&#x200B;

But all of them combined ? No.",radeon_rx_6000
"> The 6800xt does perform close to or above 2080ti in RT without using DLSS

Not really. In mixed Raster/RT that's true. In pure RT (e.g. Minecraft) the 2080ti is still much faster even without taking into account DLSS.",radeon_rx_6000
"I actually think you raised an interesting point, so I apologize for my cherry picking remark. I am still inclined to believe that Nvidia has the upper hand based on the hardware especially for BVH, but I think I didn't fully consider that console games which are more optimize for DXR might reduce the gap.

Thanks for raising the point :)",radeon_rx_6000
"I see what you're saying and I agree with your overall point regarding Nvidia's supremacy (unless they decide to continue segmenting more towards compute/datacenter at the cost of gaming performance).  I could have supplemented my post by saying there is no promise that node advantage will disappear in the future.

I could have also discussed the difference between arch on the same node with the 5000 vs 6000 series and the fact that AMD's newfound money from Zen probably won't pay off in graphics for another couple of years.

I wouldn't be surprised if Nvidia eventually gave AMD the pure rasterization crown and tried to hold onto its gaming customers with drivers and features, while continuing to fine-tune its architecture to dominate the much more profitable datacenter sector.",radeon_rx_6000
Sadly the node advantage didn't help them that much with 5700XT tho.,radeon_rx_6000
Nope.look at radeon 5700.improving 7nm doesn't help that much.,radeon_rx_6000
"Who cares ? If the card is more efficient then it's more efficient, regardless of where that comes from.. And since they're both moving to 5nm, with AMD having proven they can significantly improve the efficiency of their architecture on the same node, I don't see the issue. They were on the same node a year ago and it was embarassing. Surely the architecture improved.

Gamers won't skip on buying a card because the efficiency advantage is earned through a die shrink.",radeon_rx_6000
"> My reasons for buying a GPU are my own. I enjoying having control over my gaming experience.


Than your argument doesn't make any sense though unless there is a huge crowed of people that only play years old games on new GPUs...

IMO that is a disclaimer that should be in your original post because it reads like you are arguing that hardly any game is using DLSS when in truth you just limiting yourself to old titles released before DLSS was even a thing.",radeon_rx_6000
"I know it is an implementation of dxr...but it came out months before the standard was made available. If i remember correctly from the DF video, nvidia did a lot of the ground work but i will look more into the development",radeon_rx_6000
It won't run RT on NV cards.,radeon_rx_6000
"Absolutely. I made a deal with myself that I'd buy whichever I could get my hands on first - an RTX 3080, RX 6800XT, RX 6900XT, or a PS5 - and that would be my gaming platform until the supply situation stops being complete insanity.

Lucky for me I got a PS5, so I'm just watching the benchmarks and info until it's time to rebuild my gaming PC... probably springtime next year, I'm guessing.

I certainly could have been bit by making a gamble on the new Radeon cards if I'd found their stock first!",radeon_rx_6000
"What? Chances are, they didn't have the cards or stable drivers, especially considering how AMD was keeping things under wraps.",radeon_rx_6000
"Oh! Totally missed that M$ has a DLSS-like thing, thanks for sharing.

They're not Nvidia but, when it comes to AI, I'd expect far more of them than AMD.

Now we have to hope that AMD's got something up their sleeve in their hardware and that their DirectML implementation doesn't suck. Otherwise it's looking grim this generation.",radeon_rx_6000
[deleted],radeon_rx_6000
[deleted],radeon_rx_6000
"Yeah it's not just the benchmarks they show, which are fine but they're interpretation of them.  It seems though in Australia that Nvidia's prices are much higher than AMD's so I can understand a little bit but I just don't understand how they came to the conclusion that the 6800 is the all-around best value.",radeon_rx_6000
"Part of it could be that the main selling point of the 20xx series over the 10xx was RT and DLSS, since the value proposition compared to the 10xx series was relatively bad. Comparatively, I never got the impression that the 6000 series was promoting RT performance. They were focused on rasterization and were like ""we support raytracing now too.""",radeon_rx_6000
"How many years has it been since the 2000 series release? 

Let's try doing some basic math yes?",radeon_rx_6000
"I don’t quite understand what you’re saying. This isn’t about the majority of people being able to play at 4k60, it’s just that *a video card exists* which will let you play most games maxed at native 4k60, the asterisk here being raytracing which still commands a substantial performance impact. I’m not saying 4k60 is a particularly good value or that people are suddenly going to upgrade in droves because that kind of performance is available for $700 instead of an inferior offering at $1200.

The minimum I consider playable on PC is average 45-50 FPS, because below that tends to give me motion sickness pretty quickly. Some people are probably okay with 30 and I find 30 to be playable when the display is smaller or farther away from me. So, no, while 144Hz is nice, and *for me* I’d probably prioritize that over higher resolution, that’s not the threshold for “playability” i’m suggesting.",radeon_rx_6000
"That's the thing. With DLSS and improved RT cores on Ampere cards it's no longer a straight halving of the frame rate. Like I mentioned, for story based games you don't even need 100 fps to thoroughly enjoy it. Most people playing those games would happily drop to 70 fps if it meant a more immersive experience which is exactly what RT does.

I'm not arguing that fps doesn't matter. For certain games there's no question of choosing fps over RT. But for games like SotTR, The Witcher 3, Horizon Zero Dawn and Cyberpunk, people would want higher fidelity and a more immersive experience.

As for the HUB review, Steve himself has admitted that RT isn't as important to him because he prefers to play eSports titles rather than story driven titles or RPGs. I think it's fine to want high fps and not care about RT but to outright say that it's not relevant is being ignorant of what a significant number of people might actually care about.",radeon_rx_6000
">Very few people are willing to have their FPS halved

People have been halving their FPS for decades to see the latest and greatest eye-candy. I remember going from a full 50-60 fps in Unreal to maybe 10 on a nice cool day in Doom 3.",radeon_rx_6000
cyberpunk is an rpg which are story based. it also has action scenes. that’s like saying the last of us or grand theft auto arent story based,radeon_rx_6000
"That's definitely true but those games that have the long hours are either eSports titles or multiplayer games that have massive replayability. Whereas RT games tend to be those titles where you finish the campaign and then move on to the next. Makes no sense to implement RT for a game like CS GO but absolutely makes sense for a game like The Witcher 3. CS would by far have the higher hour count but you'd have more games like TW3 played alongside it.

Regarding availability, I think we'd see that RT will be available for those games more readily in the current gen games that are coming out alongside the new consoles. IMO it's become widespread enough that we should see more attention on it in reviews. Most of them are but the way it was dismissed in the HUB review was a little frustrating to watch.",radeon_rx_6000
At least my comment is clearly an anecdote. /u/CactusFruits states the opposite as a fact without anything to back it up. Yet people seem have an issue with an anecdote?,radeon_rx_6000
"??  The following is c/p'd verbatim from TPUs website.  Even if they didnt just measure the card power consumption, which they do, 90w is way to big a discrepancy to just chalk up to random error.

>Improving power efficiency of the GPU architecture has been the key to success for current-generation GPUs. It is also the foundation for low noise levels because any power consumed will turn into heat that has to be moved away from the GPU by its thermal solution. Lower heat output also helps improve cost because smaller, cheaper thermal solutions can be used.

>For this test, we measure power consumption of only the graphics card via the PCI-Express power connector(s) and PCI-Express bus slot. A Keithley Integra 2700 digital multimeter with 6.5-digit resolution is used for all measurements. Again, these values only reflect the card's power consumption as measured at its DC inputs, not that of the whole system.

>We use Metro: Last Light as a standard test for typical 3D gaming usage because it offers the following: very high power draw; high repeatability; is supported on all cards; drivers are actively tested and optimized for it; supports all multi-GPU configurations; test runs in a relatively short time and renders a non-static scene with variable complexity.

>Our results are based on the following tests:

    Idle: Windows 10 sitting at the desktop (1920x1080) with all windows closed and drivers installed. The card is left to warm up in idle mode until power draw is stable.
    Multi-monitor: Two monitors are connected to the tested card, and both use different display timings. Windows 10 is sitting at the desktop (1920x1080 and 1280x1024) with all windows closed and drivers installed. The card is left to warm up in idle mode until power draw is stable. When using two identical monitors with the same timings and resolution, power consumption will be lower. Our test represents the usage model of many productivity users who have one big screen and a small monitor on the side.
    Media Playback: We use VLC Media Player to watch a 4K 30 FPS video that's encoded with H.264 AVC at 64 Mbps bitrate, making it similar enough to many streaming services as well, without adding a dependency on internet bandwidth. This codec should have GPU-accelerated decoding on every modern GPU, so it tests not only GPU power management, but also efficiency of the video decoding hardware.
    Average (Gaming): Metro: Last Light at 1920x1080 because it is representative of a typical gaming power draw. We report the average of all readings (12 per second) while the benchmark is rendering (no title/loading screen). In order to heat up the card, the benchmark is run once first without measuring its power consumption.
    Peak (Gaming): Same test as Average, but we report the highest single reading during the test.
    Sustained (Furmark): We use Furmark's Stability Test at 1600x900, 0xAA. This results in very high no-game power-consumption that can typically only be reached with stress-testing applications. We report the highest single reading after a short startup period. Initial bursts during startup are not included as they are too short to be relevant.

>Power consumption results of other cards on this page are measurements of the respective reference design.

Please provide a source that shows a 25w difference; id be glad to see it.",radeon_rx_6000
"ML workloads not being productivity, sure.

ROCm is absolutely trash though, and I’m saying that as someone who hates Nvidia.",radeon_rx_6000
What driver support? Of what i see past Titans had exact same drivers as rest of GeForce line-up. Outside  of that one time you could flash bios on OG Titan to make it think it's a Quadro.,radeon_rx_6000
For 1080p these card are almost worthless outside of the most extreme situations. Now for workload tasks you have to look at entirely different benchmarks and it will depend on the type of work. For gaming I would wait for the mid range cards to see what is best.,radeon_rx_6000
"Well since you see it buy RTX and go play at half the framerate.
Meanwhile i will pick gpu for 50$ less and play at higher framerates.

What a wonderful world when you can make such choices. Everyone is happy",radeon_rx_6000
Sure if you watch video on YT back to back. When you don't i doubt you would even notice it.,radeon_rx_6000
"I have played both - and no, it isn't night and day. Maybe with the exception of transparent reflections in Control.",radeon_rx_6000
"You're the one making shit up. No, not ""nearly every big title"" is supporting DLSS 2.0. Out of the two dozen games on the list, some are DLSS 1.0, others are very small, very niche titles. There are dozens and dozens of these titles, so a handful of them having DLSS doesn't mean much, especially when they aren't very demanding. If you somehow ended up with a 4K monitor and a 2060 and will only play the games on the list - I guess it's helpful. But I'm looking at the list and there are 5 games I'm interested in. 

On top of that, many people still argue that raytracing is a gimmick, even after AMD added it. Because, in some games, it really is. I don't want mirror puddles. I did enjoy transparent reflections in Control. But developers are still figuring things out, and it might take a few years and even more processing power to make something impressive with raytracing.",radeon_rx_6000
"You can't get the DLSS frame time numbers that the tensor cores add by just looking at the inverse of the frame rate. You have to actually measure the frame time it takes of the tensor cores to figure that number out, no regular frame rate or frame time comparison will work since you're still doing upscaling on the gpu since it still outputs native (hence why the UI is still native in all DLSS games). You can definitely get an idea, but everything else is showing me that it's about half the frame time you're extrapolating.

Also, I'm fairly certain Turing cannot do DLSS in parallel with the cuda cores and Ampere *can*.

Check out this thread about DLSS 2.0 and check the links they provided in the comments.

https://www.reddit.com/r/nvidia/comments/g5ic6o/dlss_20_fps_limit/",radeon_rx_6000
Maybe the heavy FPS dips have something to do with it.,radeon_rx_6000
"I'm not you who replied to, but I'm in the same boat as them per their last comment. Personally, I haven't played a single game that has had a real-time ray tracing option, nor do I think there's that much of a visual difference in most titles for me to even care that much about it. Seems like it just eats performance for no real tangible gains in fidelity. I think *Minecraft* had the biggest difference to me and I have no interest in picking that back up ever again.",radeon_rx_6000
"A lot of it looks questionable. Icy puddles in DMCV, for example. In other games, it looks very subtle - Call of Duty: Cold War, for example.",radeon_rx_6000
"I’m sold on tech, but not sure performance is worth it yet. Not enough games support it and is early tech still. Maybe in my next gpu upgrade in 2 years.",radeon_rx_6000
"True, but why should a consumer care about it being a first implementation though? A better implementation exists with Nvidia, that is all that matters at the end of the day when comparing the products and their features.",radeon_rx_6000
"And even after Ryzen, they sadly still pretty much stagnant... Unlike Nvidia, where they pumps out more exclusive features and perf, which makes AMD struggling to catch up.",radeon_rx_6000
"But i'm seeing great rasterization here, almost double vram at a slightly lower price, but there's a point where your expectations need to be controlled, AMD catching up with Nvidia in rasterization is already a huge deal because no one expected it, now to match it in rasterization plus everything else? That's completely unrealistic and waiting for it to happen in a single architecture launch was a waste of time imo. Not talking about you specifically but some people are acting like these cards are a complete flock and that's kinda harsh.",radeon_rx_6000
"Well even if raytracing performance is not great, it is at least at Turing level for what i saw isn't it? And the Raytracing quality used in consoles is not as good as the one seen on PC so i wouldn't compare that much. All in all, the raytracing capabilities on the 6800/XT should be enough to get by at 1440p, but yeah, at 4K AMD needs a DLSS competitor or there's no chance they can compete at that resolution.",radeon_rx_6000
"Its not a hardware issue but a software one so unless AMD specifies that they redid and improved their opengl software suite in a driver update you can expect poor perfomance. The only bench weve gotten so far is this basemark score

https://www.kitguru.net/components/graphic-cards/joao-silva/amd-radeon-rx-6800-basemark-scores-emerge/

Where the 6800 scores moderately below the gtx 1070",radeon_rx_6000
"CUDA is Nvidia Tech! You shouldn't say ""no cuda""",radeon_rx_6000
Ok cool.  I never said anything that contradicts your points here.,radeon_rx_6000
"I'm not making an argument for anything.  In fact I'm planning on getting an Nvidia card.  Not that matters.  Just thought it was you showing bias.

Why would I go through your profile?

I hope I didn't ruin your day.",radeon_rx_6000
"Quoting an interview it does use raytracing for solving indirect lightning so I guess the guy you're replying to is kinda correct technically.

> ""Lumen uses ray tracing to solve indirect lighting, but not triangle ray tracing,"" explains Daniel Wright, technical director of graphics at Epic. ""Lumen traces rays against a scene representation consisting of signed distance fields, voxels and height fields. As a result, it requires no special ray tracing hardware.""

> To achieve fully dynamic real-time GI, Lumen has a specific hierarchy. ""Lumen uses a combination of different techniques to efficiently trace rays,"" continues Wright. ""Screen-space traces handle tiny details, mesh signed distance field traces handle medium-scale light transfer and voxel traces handle large scale light transfer.""",radeon_rx_6000
"> Your console argument pretty much works against you since they'll both use RDNA2-like experiences. So if the games will look good ray traced on consoles at reasonable frame rates, they will on RDNA2 cards with drivers in a few months.

Most last gen console games had no reasonable frame rates from a PC gamers perspective. For example RDR2 had (and still has) great visuals but is limited to 30 fps on console. So you need at least two times the performance on PC to get the same visuals at acceptable 60 fps. 

> Regarding the metro video you've sent, this is a bigger difference than I expected, but I don't see these scenes as dramatically better than the rest. The lighting looks different, yes, but not necessarily better. The textures are hit-and-miss. Some of the outdoor scenes are unequivocally better.

I disagree with that personally but just to add to this, that game was still designed with none RT GI in mind mostly. Future games and especially those that use RT on consoles as well will have art designed for the lighting and reflections RT can provide. 

> And if this is the best looking game with ""full feature ray tracing"" And I'm limited to average ~70 FPS @ 1440p, that's kinda bunk--I'd much rather have 100 Hz 3440x1440 and wait another 3 years for ray tracing to mature.

Metro Exodus with RT on High runs at 4K at 60 fps on a 2080ti if you use DLSS. That are around 40% more pixel than your 3440x1440 resolution. And that just on a 2080ti (3070 performance). You should easily be able to do that 100hz or more on a 3080. BTW even without DLSS a 2080ti can do 73fps at 2560x1440.

https://www.gamestar.de/artikel/metro-exodus-raytracing-dlss-test-tolle-optik-wenig-fps,3340649,seite2.html

*graphic on top, ""Hoch"" means ""High""*

Same as with the other answer, performance delta of using RT will be (at least slightly) lower once it becomes the tech the game was designed for.",radeon_rx_6000
"I mean, ""everything else"" is pretty much any rendering use, RT, driver support, ray tracing, DLSS, ShadowPlay, and a bunch of other stuff. Some amount of that will be significant to most people.

And besides that, I never said p2p weren't important; I said the gap wasn't nearly big enough to call the 6800xt the go-to gaming card,  especially when you consider ""everything else.""",radeon_rx_6000
"Idk I'm running with it on in Control and Watch Dogs currently (with DLSS) and definitely plan on enjoying it in Cyberpunk as well on my 3090. I think it has quite a lot of value, especially combined with DLSS.",radeon_rx_6000
"For example, I currently own ONE game which supports RT, and I bought that *yesterday*. There's a big difference between being 50% faster in RT if literally every game you play using it and being 50% faster in RT in like 12% of games you play.",radeon_rx_6000
"Yes but you'll *need* it somewhere in the next 7 years.. 

This horse has been beaten to death. More VRAM is just a marketing ploy.",radeon_rx_6000
"It's used in plenty of games that I play, so it's worth it for me at least, and some people are definitely in the same boat.",radeon_rx_6000
I know what happened with the 5700xt. They would’ve sold a lot more if it had released with good drivers. AMD gpus have built a well known rep for poor drivers. That’s why I said good drivers on release was probably a priority for Big Navi.,radeon_rx_6000
"No worries, it can happen to the best of us.",radeon_rx_6000
RTX is DXR. RTX is just what Nvidia calls it for branding and marketing.,radeon_rx_6000
"""it's your math that's a failure. It's not half way""

Prove it. I laid out my calculations. Show me then you dumb fuck. I am very eager to hear your response. :)

""Also roughly 50% more pixels than 1440p.""

I admit I made a mistake with the 1440p statement, I will gladly own up to it.

But stating that 3440 x1440p has 59% of the pixels of 4K is not wrong. The fact that you can't admit to it, means either you have too much pride or you are too dumb to understand. I feel sorry for you really. Grow the fuck up.

If you are really interested in learning math, reading my 1kg example which has been distilled to make it easier for you.",radeon_rx_6000
"Yeah. The rtx perf is abysmal on the 6800. I really want to play Minecraft rtx, so the 6800 is iffy. 

If I cut some excess of my build (b550 mobo and 32 gigs ram) then I can fit a 700 quid card within my stretched budget.

Do you think the 3080's price will lower to close to msrp (700 quid) in the coming months?",radeon_rx_6000
">6800XT pulls 8% less power according to computerbase. Significant? Maybe, though it's also 6% slower.

I think you're cherry picking pretty hard to come to that conclusion. The 6800XT comes out on top in a lot of 1080/1440p games at that lower cost and at that lower power usage. [Hardware Unboxed](https://www.youtube.com/watch?v=ZtxrrrkkTjc&ab_channel=HardwareUnboxed&t=12m10s) for example ended up with an 18 game average where the 6800XT edged out the 3080 in FPS at both 1080p and 1440p.

When you look at the data in aggregate, what you see is the 6800XT being neck and neck with the 3080, either losing or winning by a few percentage points, which is not something you'll ever notice in a real world scenario. In other words, in real life you are getting effectively identical overall performance for a lower cost and at less power usage. That's why everyone's saying the 6800XT is better for 1080/1440p.",radeon_rx_6000
"The result is the same, only SAM is available for you today",radeon_rx_6000
I think this also depends to some extent on how much Intel will manage to not suck with their first gen hardware (though historically that's not looking too likely).,radeon_rx_6000
"Really interesting, looks like AMD is gonna make it competitive in some resolutions,",radeon_rx_6000
Check the comments of the non xt review for what's going on. It has turned into a philosophical question now-„what is the right way to measure power consumption?“,radeon_rx_6000
Okay good - I hadn't come across any discussion of the power limits/behavior of the 6800's in my review skimming so I wasn't sure if someone discovered that they just don't take undervolting well.,radeon_rx_6000
"I agree. People also forget that vram bottleneck means a lot of stuttering, not just less fps ;)",radeon_rx_6000
">970 owners(like myself...) had to compromise on with settings thanks to the lower VRAM, that would otherwise run ok. 

This is a patently false statement. Saying ""like myself"" doesn't let you change facts. Did you forget the 970 literally launched along side a 4GB card with more compute?

> On GTX 980, Shadows of Mordor drops about 24% on GTX 980 and 25% on GTX 970, **a 1% difference**. On Battlefield 4, the drop is 47% on GTX 980 and 50% on GTX 970, **a 3% difference**. On CoD: AW, the drop is 41% on GTX 980 and 44% on GTX 970, **a 3% difference**. _**As you can see, there is very little change in the performance of the GTX 970 relative to GTX 980 on these games when it is using the 0.5GB segment**_.”

There was a **ton** of noise about stuttering that was falsely claimed to be because of 3.5 GBs... lol yeah it turns out when you max out your low mid-range card it stutters and turning down settings helps.

The smoking gun is that NVIDIA actually released a driver that would essentially force the games to avoid that slower memory since contrary to what most of you realize, the game doesn't need everything that's in VRAM.

That's why the theoretical problem **wasn't** 3.5 vs 4GB in terms of space, it was literally the *speed* of those last 500 MBs. Even after the driver fix people would wrongly attribute any stuttering on high settings to ""those darn 500 MBs"", as someone who's work on shipped titles, sorry that's not how that works.",radeon_rx_6000
The point is those slower 500MB aren't why it was hampered. The actual computational demand of games was high enough that it couldn't keep up.,radeon_rx_6000
"That's non-sequtur. If you don't understand a statement, you don't have to force yourself to reply with a random one-liner just because it's reddit...

---

The lawsuit was about the representation of the 3.5 GB (and NVIDIA _settled_)

That never changed the fact the 970 was bottlenecked on compute well before the last 0.5 GB was accessed...

Everyone to make synthetic benchmarks show just how slow that .0.5GB was for clicks, but no one pointed out that even at 3.5 GBs of usage, most of the games at that time would already be tapping out a 970, the fact the last 0.5GB wasn't on the main pipeline wasn't going to change that.",radeon_rx_6000
"Because Nvidia mislead people, not because the performance was majorly affected.",radeon_rx_6000
"That was because of the slower memory, not because the cards failed to perform.",radeon_rx_6000
"I just don't get this point. There is currently one major game that uses more than 10gb VRAM at 4k. MSFS 

And that's with the highest settings. The 6800 xt is underperforming the 3080 at the same settings. They don't even need the 16gb VRAM for it... Rasterization is behind or the memory bandwidth bottleneck.",radeon_rx_6000
More RAM doesn't magically make performance better though. 6800XT performing worse than 3080 at 4K on MSFS would prove that point. So more VRAM is moot and useless if it can't handle the game at high framerates.,radeon_rx_6000
Amd would have trouble patenting it then,radeon_rx_6000
"He's right.

They stated on the HotHardware stream earlier today game developers requested something cross API, cross platform and cross vendor. Something that would work on both AMD and Nvidia cards as well as both consoles, just so they wouldn't have to implement multiple APIs. So they're all working together (to be clear, Scott and Frank never said Nvidia was, but I highly, highly doubt they were left out of the conversation) to put something together alongside Sony and Microsoft.",radeon_rx_6000
"Ah. In the context of a $650 GPU, I get your point. But then again, you’re still talking about the difference in a $1200 build versus putting that $50 in a better case, more fans, etc. Depending on what you plan on doing with it. Plenty of folks might still have a strictly set budget.",radeon_rx_6000
"The PS4 had a 8 core cpu. How many games on PC use 8 cores? 

Dlss is still just upscaling. There's no reason to even want it. 

The 980ti or whatever was better than the Fury X in some games. The Fury X still lasted up till a few months ago.

Few people will be upgrading for a few months. I'm interested to see if drivers help fix the memory bandwidth issue.",radeon_rx_6000
"Your description of ""scaling"" is exactly what I was implying lmao.",radeon_rx_6000
"Lol, yeah, it's absolutely nutty. I wish they had limited it to one 3000 series card per account, instead of allowing people to buy one of each. That would have likely helped speed things along.",radeon_rx_6000
"Because AMD is the underdog therefore they get a free pass.

/s

I am always amused by people here who engage in tribalism and see companies as their friends.",radeon_rx_6000
"Sure. Nvidia has significantly more resources to better forecast and manage the high demand when compared to AMD. Even today Nvidia is a significantly larger organization in terms of headcount, assets, and cash.",radeon_rx_6000
"You don’t have to implement all of raytracing’s features to make an impact in visual fidelity.

Look at Watchdog Legions.

Considering that there will be a lot more raytracing titles from future console ports, I think it matters a lot.",radeon_rx_6000
"Pure RT titles are not something we'll see in any real sort of numbers since the market is ridiculously tiny so I'm not sure that's what any reasonable person would call RT performance on a card. Mixed use-case games are all we'll see in even moderate numbers during this gen, pure RT we've got two games with one in Beta still.

As an aside, do you know if minecraft RTX support mods?",radeon_rx_6000
">In pure RT (e.g. Minecraft)

Poor example. The 6800XT performs roughly on par with what Digital Foundry stated the Series X ran it at many months ago despite the extremely stark differences in hardware.",radeon_rx_6000
"> IMO that is a disclaimer that should be in your original post 

It was. Re-read the first two lines of my first post. Here, I'll provide them for you.

> Just my personal feedback on the matter. As always, YMMV.

> I care about performance first and power efficiency second, but it's weighted heavily.

I was talking about myself. The entire post was about how I personally use GPUs. Never once did I talk about mainstream use case.",radeon_rx_6000
"But it was made in partnership with Microsoft, who develop the DirectX API and spec.",radeon_rx_6000
Why's that,radeon_rx_6000
"I can't really add anything to it, since I'm also going off the same limited information provided.",radeon_rx_6000
"> I wouldn't count on this. Nvidia has some immense technical wizardry going on in-house to produce these DLSS models. They won't be open-sourcing that, and any attempt at reproducing it open source are both unlikely to match in quality and are many years away. 

Exactly they are not doing for the benefit of everyone

> Come on, man. Nvidia developed in-house the two most game-changing pieces of graphics tech of the decade and you're going to pretend like it's some hostile move? DLSS is a fucking mind-blowing invention that almost no one saw coming, and RT has hit us now at playable framerates in consumer-grade hardware a good 5-10 years before anyone thought it possible. Not everything is the move of a villain.

Cool so they will share it with AMD? so that things are actually benefiting everyone and not just people using their cards?

So why hasn't Nvidia gotten rid of the evil VM limited on their drivers if they are so benevolent?",radeon_rx_6000
"What the point of comparing hardware like this review?  

Raytracing is now part of the parcel for all 6000 series buyers.  You can't just pretend it's not there when you've gone and died on a RT hill for 2 years.   Plus we get Sam testing day 1, but no dlss.",radeon_rx_6000
">How many years has it been since the 2000 series release?

>Let's try doing some basic math yes?

AMD's Ryzen processors released in 2017, in March. It is currently November 2020. It has been nearly 4 years since AMD made their great comeback in the computing space. More importantly, The RX 300 series ***was*** competitive, even if not top to bottom of the stack. That was the graphics release 5 years ago, and the last truly competitive cards from AMD since this launch.

Let's not dish out apologia for a megacorp.",radeon_rx_6000
">I don’t quite understand what you’re saying.

What I am saying is that people dont play at 4k60Hz.

It can be discussed how the new cards brought it, but no. People are not playing in any significant number, even looking solely at high spending gaming segment and they are not going to start to in near future.

People are not trading high refresh to go 4k. Visit /r/monitors the 1440p IPS monitors are all everyone wants, even on huge budgets.

Similarly it goes for RT. Titles are not there that plenty, hit is still substantial.

So it should be pretty obvious why I find the sudden torrent of comments complaining about this kinda off putting. Like people are going out of their way to shit on something.

>The minimum...

Err and? We were in your playable spectrum for years. People dont want 4k or RT at the expense of smoothness and responsiveness.

Imagine you would read the same comments you posted but during 1000 or 2000 series release. How would you explain that while we have jumps we are not there? Similarly 3000 will be laughable in 2-3 years and claims how we are finally ready for the switch will come. Probably that time true. if it really be 120 fps with the goodies.",radeon_rx_6000
"> if it meant a more immersive experience which is exactly what RT does

No, it does not. Not now at least; the effect -excluding minecraft- is minor. When the effect is breathtaking, then we shall revisit it. I hate the concept of DLSS with a passion and hate even more that it is more or less necessary for a playable experience. 
All in all, I respect your opinion.",radeon_rx_6000
"No its not an rpg its an action rpg
And if its storybased or not, its packed with action
And even if not, people(most pc gamers) want high refreshrate
Which is why most ppl here recommebd a 1440p 144hz monitor",radeon_rx_6000
"I'm also speaking anecdotally - perhaps I should have made that clear in my comment. I'm just basing it off of the assumption that most people don't leave their systems idling 24/7. I should also clarify that when I mean ""off"", I don't mean shut down, rather suspending the computer in a sleep state (where peak GPU power consumption should become largely irrelevant). But gain, this is purely anecdotal, as I'm sure there are people that do leave their computers on for extended periods of time. At that rate, power consumption is of course a greater concern.",radeon_rx_6000
That furmark number in techpowerup is ~50W higher than what GN got and well over what it should be according to spec. I tend to trust the numbers GN gets because he measures the actual input power with external tools instead of software.,radeon_rx_6000
"I wouldn't call it trash. But i wouldn't recommend it to anybody at this moment, unless they run stuff on some HPC clusters and must use AMD. I opened like 20 bugs to ROCm in past year or more, and it is bloody slow progress with fixing them. Half is finally fixed.",radeon_rx_6000
"https://youtu.be/YjcxrfEVhc8?t=604

The 3090 is gimped in certain professional applications because of drivers.",radeon_rx_6000
"Ok, save $50.  It's not faster with RTX off either though.  Smart people will pay the extra $50 to get raytracing, DLSS, NVENC for better streaming/recording/VR, CUDA cores for anything resembling productivity, G-sync, etc.

Nvidia is simply the more mature product.  But hold up that small increase in CSGO frames that you will never see.",radeon_rx_6000
">upscaling on the gpu since it still outputs native (hence why the UI is still native in all DLSS games)

From Nvidia's own site the direct output of DLSS 2.0's convolution neural network is the upscaled image though. [https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/nvidia-dlss-2-0/nvidia-dlss-2-0-architecture.png](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/news/nvidia-dlss-2-0/nvidia-dlss-2-0-architecture.png)

[https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/](https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/)

And in any case, I don't see how DLSS on vs off is creating unrelated work. Regardless of the exact cost of the DLSS 2.0 subroutine on tensor cores the associated overhead of running DLSS puts the total time of the solution at up to 3.1ms (rounded) on a 2080TI. I'm sure Ampere does handle it better though.",radeon_rx_6000
">but why should a consumer care about it being a first implementation though?

Nope, nope at all. What my point is, lower your expectation.

I don't think being able to compete against your competition after many many years is a ""meh""... If I need to choose between 3080 or 6800XT, I'll gladly choose 3080 too (regardless of the stock).",radeon_rx_6000
"They do, unfortunately. And this is coming from a 10700k owner. I'm getting some FOMO from the new Ryzen chips. Though, I'm quite happy with AMD's performance this gen. Sure, they're not destroying NVIDIA, but they're putting up a damn good fight and I'm excited for the future. I might end up with a 6900XT, who knows.",radeon_rx_6000
"The ultimate question is: is this competitive?

The answer is ""yes, but with a lot of asterisks.""

To me personally, it makes no difference how huge of a jump this is from AMD's previous offering. What matters is, what is the best use for my money right now. To me, it seems that Nvidia is the better purchase at this moment.

Maybe in 2021 things will be different.",radeon_rx_6000
It’s at [34 FPS in Control 1440p](https://tpucdn.com/review/amd-radeon-rx-6800-xt/images/control-rt-2560-1440.png) so for next gen games that fully utilize RT performance js going to be in the toilet.,radeon_rx_6000
Thanks a lot for the info!,radeon_rx_6000
"Considering 1070 satisfied me there throughout the whole time I have had it, that’s... not an achievement.",radeon_rx_6000
"If I can by a card that performs better from nVidia, this is on AMD to solve. The cards released today have worse performance in a lot of aplications, where nVidia cards have better performance because of CUDA optimizations. Saying ""no CUDA"" is perfectly valid if one works with applications that have CUDA optimizations.",radeon_rx_6000
Except that it matters as very few programs use OpenCL as it was shit for developers to use for years. So no CUDA is a detriment even though it supports an equivalent.,radeon_rx_6000
"I'm just justifying to everyone that reads the comment tree that there's no bias. It is what it is.

You seem to be very concerned on the impact you might have had on me, chill dude. All is well.",radeon_rx_6000
"So it's not using DXR or RTX or other hardware based raytracing, which is what we care about. SDF raytracing for shadows has been done in UE4 as well (kite demo), but we don't go around saying UE4 uses raytracing. All volumetric effects in modern games also use ""ray tracing"" in the technical sense, but nobody claims it as such.",radeon_rx_6000
Value ? You paid $1000 for 1 pc component.   Some guy that paid $500 for his ps5 will enjoy cyberpunk same if not better...,radeon_rx_6000
I hate that argument also. Almost everyone buying $700 GPUs will have upgraded at least once or twice by the time that VRAM may become an issue. Especially since 8K will come slower than 4K as even on TVs people are starting to not care about more resolution.,radeon_rx_6000
"I guess, but none of the reviewers managed to find ONE game where the VRAM is a problem.",radeon_rx_6000
"No, it isn't. DXR is the DX12 Ray tracing API. RTX is a DXR compatible Nvidia specific implementation of Ray Tracing that is both software **and hardware based**. Games developed for RTX will run like shit on hardware that wasn't designed for it (See Watch Dogs Legion running like utter shit on the ps5 with RTX). Supposedly, RTX hardware is DXR compatible and should not suffer from the same hit when running DXR games.

This difference is important because as both of the recently released consoles have RDNA2 hardware or RTX non-compatible hardware, the push for DXR is going to be  whole lot greater than for RTX and, therefore, fewer and fewer titles will be RTX only and cripple AMD GPUs, PS5 and Xboxes.",radeon_rx_6000
"I never said that the 59% is wrong, I said that it's not halfway between 1440p and 4k. Maybe you should really go and take some basic english lessons. You're the absolute dumb fuck here, you can't even do basic math or english. The worst part is, you don't even possess the self-awareness to realise it.",radeon_rx_6000
"I'm literally just taking that from the same review. We can wait for 3d centre to do their meta review on 1440p if you want something broad across all publications, because not everything else follows the same pattern as HUB (TPU, for instance, is pretty similar to computerbase).",radeon_rx_6000
Nah,radeon_rx_6000
Actually the stuff I’ve seen isn’t looking too shabby,radeon_rx_6000
"Based on the graphs you've made there it's probably not ""extrapolate metro: LL at 1080p to everything else"", as it seems CPU bottlenecks and/or small triangles are causing big differences between resolutions. Is it extra effort to collect power usage across all benchmarks? Beyond no longer having history to compare to. Or is it all plugged into PCAT/your own risers the whole time and data collection automated?

If it's the same effort, you could give the average power across all titles for each resolution? E.g. 6800 XT averages 210W at 1080p, 260W at 1440p, 295W at 4k. And then use that accordingly for the perf/W at each resolution.

P.S. hopefully this is constructive - I am a huge fan of your reviews, and they were the first place I looked for this set",radeon_rx_6000
"~~So you mean when my 970 straight up instantly gave up in titles with crowded areas (like X Rebirth, which, while being a buggy mess back then, used nowhere near enough VRAM to max 970 out) it was due to the GPU itself not being fast enough?!~~

~~Boy, I could have used that knowledge 5 years ago~~ 😅

EDIT: might have misremembered and meant GTX 460 I've had before that :') Getting old...",radeon_rx_6000
"This is that thing where Redditors find the one tiny part of a comment they can kinda almost understand? 

And rush to comment to prove they understood _something_. (Not the whole comment, but _something_)

---

At this point the conversation derails to try to get them understand the whole rest of the comment actually matters too.",radeon_rx_6000
"There is a reason to want dlss? It looks super fuckin good and it boosts the performance when you need the extra kick,  also there's a difference between physical differences in hardware and developers implementing something like rsy tracing your comparison is way off",radeon_rx_6000
"It's not what you're implying if you're comparing it to GCN lol, so no, you definitely weren't.

GCN can't scale up the amount of streaming processors/ROPs, shaders, shader engines, ect. It hits a massive wall.

Ampere doesn't ""scale"" as well to lower resolutions because this gen has significantly more FP32 pipelines (pretty much double), so you won't see a 1:1 performance relative to the resolution as you go down in res since those FP32 pipelines aren't needed as much.

Pretty simple.",radeon_rx_6000
"They're just your friendly neighborhood multi billion dollar corporation, though! You should be nice to them.",radeon_rx_6000
"But this doesn't express your very unique personal approach of only playing games that are already a few years old with your new Ampere or Big Navi card. And specifically it doesn't clarify your statement about you only having one game with (in that case buggy) DLSS support, which as I already mentioned makes people think that you reiterating the supposed lack of DLSS supporting games when in truth you shouldn't have any expectation to seeing the games you play support any Turing or later graphic features.

That also means that raytracing is even way less of a concern than it is to any of the more common ""I don't care that much about raytracing because of the performance reduction"" crowed. In fact you don't have to care about any of the new GPU features that DX12 Ultimate brings with it like Mesh Shaders or advanced VRS. You also don't have to care how games like Cyberpunk or anything that releases in the next two years run. 

Its a hell of an unique situation that you should be mentioning, just like someone with a special needs dexterity limitation should mention his situation when comparing gamepads.",radeon_rx_6000
"Great question.  Ask AMD and the devs:

https://www.dsogaming.com/news/godfall-supports-ray-tracing-only-on-amd-gpus-nvidia-support-coming-in-the-future/",radeon_rx_6000
[deleted],radeon_rx_6000
">Cool so they will share it with AMD? so that things are actually benefiting everyone and not just people using their cards?

Why should they do that? Is AMD sharing their achitectural innovations in the CPU space with Intel? Are you high?",radeon_rx_6000
"Mind the cringe, but we shall agree to disagree. I just think there are enough people that care about it to warrant more attention than it has gotten in the HUB review. Anyway, there are other reviews that do take it into account, fortunately.",radeon_rx_6000
right but i’d say most people want high FPS *at max graphics.* everyone i know tries to balance the max fidelity they can achieve with 80-100+ FPS,radeon_rx_6000
"I didn't have any problems with your comment tbh. Just wanted to chime in with my personal experience, yet got downvoted -5 in like 5 minutes for some reason.

Side note: It would be pretty cool if steam hardware survey included something regarding average uptime.",radeon_rx_6000
"The numbers used in the TPU article match almost exactly what GN has posted for their 3080 review.  

https://www.gamersnexus.net/hwreviews/3618-nvidia-rtx-3080-founders-edition-review-benchmarks",radeon_rx_6000
"Just like Titans of past.

&#x200B;

And yeah, i doubt you're going to run Siemens NX and the like on Titan of any sort. Which is the kind of applications gimped on GeForce cards. Generally if you can afford former, Quadro is just a pocket change.",radeon_rx_6000
"You're referencing 9 month old articles before Ampere was even remotely announced and multiple sources (if you looked at the link I sent you) show you that the overhead is much less.

Directly from Nvidia: https://i.imgur.com/G7QprUQ.png

Like you said though, Ampere is even better. Regardless, my whole point was that DLSS is not going to be a max framerate bottleneck simply because it's such a little impact.",radeon_rx_6000
If you feel fomo over 5-10% at 1080p do you. I bought a 10700k because it was the best value at the time.,radeon_rx_6000
"I mean, that has been the case with AMD vs Nvidia since i don't know, the Radeon 7000 gen? Nvidia is not Intel and they didn't commit the same mistakes, they've been improving and improving and innovating with their offerings, even if not much their prices. 

To me the jump from AMD's previous offerings makes a difference because it puts us closer to being a fully even ""match"" than we thought they could with a single architecture launch. For example, i have a 5700XT, i can't do raytracing but it matches a 2070S currently in rasterization. Would i like to have Raytracing and DLSS? Of course i would, but i paid for my gpu like 33% less than what i would have paid for a 2070S where i live, does Raytracing and DLSS justify, at this point in time, a 33% extra money to me? Absolutely not. But if you have the money, and really care about Raytracing and DLSS at their current level of support, or work with multimedia then definitely go with Nvidia, because they're undoubtedly the better offer.",radeon_rx_6000
"Yeah that's bad, and Control is basically the modern game with the best and most complete RTX implementation so far, no? That's why AMD desperately needs a DLSS competitor or their cards are gonna have a bad time as time goes on. DLSS is a godsend.",radeon_rx_6000
"Remedy has had 0 chance to optimize for RDNA2's RayTracing though, it's much closer in RT performance in Watch Dogs: Legion and Dirt 5. 2 titles where the devs had any chance to optimize for RDNA 2.",radeon_rx_6000
 I'm not sure im understanding your comment to be honest,radeon_rx_6000
"at least USE PROPER WORDS .when you say no cuda , this means AMD should have Cuda , that's wrong.",radeon_rx_6000
"Maybe. But we are talking about the 3080 vs 6800XT, both similarly priced products. If one offers much better RT, then yes, value.",radeon_rx_6000
"What are you even talking about? The PS5 is not remotely close to PC in performance. In watchdogs it's about equal to a 2060s according to DF. 

We all know consoles are cheaper upfront cost because they get you over a barrel on games and subscriptions, this isn't new news but to say you'll enjoy the game on a console more versus a  3080 or 3090 equipped PC hooked up to a CX is just plain dumb.

Watchdogs legion runs at fucking 30 FPS, that'd be considered unplayable or at least not enjoyable by PC standards.",radeon_rx_6000
"I'm not saying it's especially widespread. But you said there isn't a single reason to buy the 6800 XT for $50 less, and I gave you one. There are obviously reasons to buy either card (if you can actually buy them)",radeon_rx_6000
"RTX is not even an api, uts just the branding in nvidia cards. As for DXR it won't be available on ps5, as it is a proprietary Microsoft api.",radeon_rx_6000
"> I never said that the 59% is wrong 

That's exactly what you were implying. So you are basically just moving the goal post now. Good job.",radeon_rx_6000
"Dlss looks off from what I've seen, and native resolution will always look better. 

Yes there's hardware differences. Just like PhysX or TrueAudio. Thing is those never took off.",radeon_rx_6000
"I think I know better than you about the intent behind my own words lmao. You haven't even asked what I meant, how would you know?",radeon_rx_6000
"> But this doesn't express your very unique personal approach

It does.

If you have questions, feel free to ask. But don't make assumptions just because you misunderstood something (after initially not reading it and claiming that I didn't say it).",radeon_rx_6000
I don't think it's a condition AMD set because Dirt 5 runs on both just fine.,radeon_rx_6000
"> Why would a company share its trade secret and crowning achievement with its competitors? I'm all for consumer-friendly business practices and fair competition but you are suggesting a world in which businesses will have no incentive to invest and innovate because everything they come up with will be given away for free to their competitors. It's naive. 

If it is a trade secret it should never become a standard since it can't be used by anyone but them. What about users of AMD hardware should they just get fucked over by everyone sucking off Nvidia tit and not just use standard share things like rastering pipelines? Honestly I hope RT cores, DLSS, RTX go the way of PhysX

I'm suggesting a world, where things like the public domain are actually protected and not ruined by the likes of the house of mouse who bastardized copyright",radeon_rx_6000
AMD did license amd64 to intel,radeon_rx_6000
"No worries. I would definitely be interested in those kinds of stats as well, along with reasons as to why lengthy uptime is needed. One that immediately comes to mind is running a NAS or a Plex server of sorts, though I think that that can be handled much more efficiently with different hardware.",radeon_rx_6000
That last number is overclocked. Stock is 322-324.,radeon_rx_6000
"Not like Titans in the past. It's literally the 2nd sentence of what I just showed you from NVIDIA. 

The 3090 does not have the same features as the Titans and that was intended by them. It's literally a direct quote from NVIDA. How the hell do you sit here and argue against the the company who literally made them?",radeon_rx_6000
"It's more about supporting a company who's actually trying to innovate. As I said, Intel has been stagnant for way too long and it's coming back to bite them. AMD's success is a good thing as it'll push for more competition which only benefits us in the end.",radeon_rx_6000
"Both cards offered today by AMD are quite a bit more expensive than a 5700XT class card. 5700XT was (maybe still is?) the best choice in its price class. In my opinion, 6800/6800XT are not.",radeon_rx_6000
"DXR is vendor independent. WD: L and Dirt 5 don’t have as complex effects as Control. You can see that the more complex the RT effects are, the worse AMD’s performance is. The worst case scenario is Minecraft, fully path traced, where there’s a 2x gap between 3080 and 6800XT.",radeon_rx_6000
"AMD continues to underperform there, what else can I say? But it’s not unplayable either.",radeon_rx_6000
"My words were proper. An AMD card does not have CUDA. It not having CUDA is a point against it, because of real world performance considerations.",radeon_rx_6000
"Watchdogs is the benchmark of an optimized game now ?

You literally scraped the bottom of the barrel to try and make your point 

Look at spiderman and show me anything on pc that can do that level of ray tracing",radeon_rx_6000
"Nope. I even said in the next line that 3440 is 34% more than 2560. Seems like your reading comprehension is even worse than I initially thought. There's no moving of goal posts, only that you're refusing to admit your math is wrong. I have already pointed out exactly what is wrong with your math, but you simply can't admit it. Good job.",radeon_rx_6000
Have you actually looked at dlss 2.0? Like you speak like someone who never saw a picture from it,radeon_rx_6000
"Bro even if you took out all the technical bullshit, GCN doesn't scale up well and Ampere doesn't scale down well... which still doesn't fit anything with what you're saying and especially not ""scales like old GCN"".

You don't need to admit your wrong but stop with the stupid argument lol you're definitely full of shit",radeon_rx_6000
"My boss doesn't set conditions for me, but I still know who signs the checks.",radeon_rx_6000
Because they are both dependend on each other in the x86 architecture field. Same as Intel licensing x86 to AMD.,radeon_rx_6000
"Yea, but what helped the 5700XT to win that position was Nvidia's complete bonkers prices for Turing, i wouldn't be surprised if the RTX 3060 or 6600XT (or whatever that might be called) displaced the 5700XT from its ""throne"" with a better price and DLSS and RT in the case of Nvidia.",radeon_rx_6000
"Just because DXR is vendor independent doesn't mean that devs don't have to do different optimizations for different architectures.

Minecraft RTX was literally made by Nvidia too. The gap in RT performance is probably looking the worst it ever will at this moment.",radeon_rx_6000
Oh definitely yeah. I dont think thats ever gonna change. The opengl perfomance is not amds priority. But its definitely not unplayable either for the most part. The raw perfomance is usable at 1070 levels but there is rampant stuttering and utilization issues depending on whats happening in opengl stuff that makes the experience much worse than the 1070 and less stable,radeon_rx_6000
"Assassin's creed isn't any better. Haven't even looked at SM I don't do exclusives.

You're fucking delusional if you think the console is close to even a 3080 never mind the 3090, the new consoles don't have any damn games to compare yet that are also on PC beside WL/AC. I have to scrape the bottom of the barrel because the barrel is almost empty.

If I use older games you'll reply with ""WaSnT dESiGnED fOR tHE hArDWaRe."" We'll see how badly embarrassed the consoles get in CP2077.",radeon_rx_6000
I've seen of on my buddy's 1070.,radeon_rx_6000
"If you can't see the parallels between two disparate GPU architectures both ""gaining"" performance relative to other GPUs as resolution increases, I don't know what to tell you.",radeon_rx_6000
"Intel licensed x86 to AMD prior to AMD64 being a thing though. Which seemed to be a benefit to intel in the long run.

It could be said that nvidia licensing their RT cores would in the long run help nvidia but they likely don't want to end up in a situation of a free cross license with amd.",radeon_rx_6000
Absolutely agree.,radeon_rx_6000
"So coincidentally every game with the best looking RT effects has Nvidia bias but every game with minimal RT effects has optimization? The cognitive dissonance is real.

The gap in RT performance would have to close 30-50% which is not in range of what a driver update can manage. 10% maybe but then 6800 XT would still be hugely behind.",radeon_rx_6000
"Now, the usage issues are a problem 🤔",radeon_rx_6000
Show me a pc game on a 3090 that looks better than spiderman. I'll wait,radeon_rx_6000
WTF? Dlss is literally only for RTX cards,radeon_rx_6000
"AFAIK Intel licensed x86 because IBM forced them to, not out of the kindness of their hearts. I don't see Nvidia licensing RT cores anytime soon.",radeon_rx_6000
"I'm not talking about driver updates. I'm talking about the game developers actually testing their RT implementation for RDNA2 and doing some degree of optimization for that architecture.

The architectures are quite different. It's no wonder that AMD is far behind when the developers has had 0 chance to even test their implementation on RDNA2.

How is this cognitive dissonance, stop being a dick.",radeon_rx_6000
"History shows that it worked to the betterment of the x86 platform since intel_64 was lets be honest hot garbage.

And something similar done with RT cores would likely make them more powerful and make them more useful in the long run vs being a stop gap to cover up the limitations of Nvidia raster cores",radeon_rx_6000
"I mean you only have to look at consoles with 1/9th resolution reflections at 30fps to realize RDNA2 RT performance is poor. And I would say a first party Sony title (Spider-Man) is as optimized as you can get. A 6800XT has a little under double a PS5’s performance so you’re talking crippled RT effects at under 60fps.

Once AMD moves to a dedicated RT unit instead of sharing TMUs with RT capability they’ll get better performance.",radeon_rx_6000
"Consoles doesn't have 128mb cache which should make a big difference in RT. Also no launch title is ever as optimized as it gets, it's much too early to make a definite conclusion about how RDNA2's RT performance is going to hold up.",radeon_rx_6000
